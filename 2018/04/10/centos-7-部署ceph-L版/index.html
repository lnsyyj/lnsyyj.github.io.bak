<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="google-site-verification" content="XuZNSkAZZ_hygmzGh5A1FylQH9BedoyFXmlYAgj5fd4">

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-1720709954166786",
    enable_page_level_ads: true
  });
</script>








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="CEPH,">










<meta name="description" content="ceph L版已经发布很久了，官方说默认使用BlueStore作为OSD的存储后端，在Cephalocon APAC 2018上也是讨论的焦点之一。 提到BlueStore，不得不说一说Ceph的STORAGE DEVICES。 STORAGE DEVICESCeph守护进程将数据存储在磁盘上：123456Ceph OSDs ( Object Storage Daemons )    Client">
<meta name="keywords" content="CEPH">
<meta property="og:type" content="article">
<meta property="og:title" content="Centos 7 部署Ceph L版">
<meta property="og:url" content="http://yoursite.com/2018/04/10/centos-7-部署ceph-L版/index.html">
<meta property="og:site_name" content="Jiang&#39;s Blog">
<meta property="og:description" content="ceph L版已经发布很久了，官方说默认使用BlueStore作为OSD的存储后端，在Cephalocon APAC 2018上也是讨论的焦点之一。 提到BlueStore，不得不说一说Ceph的STORAGE DEVICES。 STORAGE DEVICESCeph守护进程将数据存储在磁盘上：123456Ceph OSDs ( Object Storage Daemons )    Client">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://docs.ceph.com/docs/master/_images/ditaa-b5a320fc160057a1a7da010b4215489fa66de242.png">
<meta property="og:image" content="http://docs.ceph.com/docs/master/_images/ditaa-50d12451eb76c5c72c4574b08f0320b39a42e5f1.png">
<meta property="og:updated_time" content="2020-03-22T08:16:36.043Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Centos 7 部署Ceph L版">
<meta name="twitter:description" content="ceph L版已经发布很久了，官方说默认使用BlueStore作为OSD的存储后端，在Cephalocon APAC 2018上也是讨论的焦点之一。 提到BlueStore，不得不说一说Ceph的STORAGE DEVICES。 STORAGE DEVICESCeph守护进程将数据存储在磁盘上：123456Ceph OSDs ( Object Storage Daemons )    Client">
<meta name="twitter:image" content="http://docs.ceph.com/docs/master/_images/ditaa-b5a320fc160057a1a7da010b4215489fa66de242.png">


<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/04/10/centos-7-部署ceph-L版/">





  <title>Centos 7 部署Ceph L版 | Jiang's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jiang's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-ads">
          <a href="/ads.txt" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            ads
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/10/centos-7-部署ceph-L版/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiang Yu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Centos 7 部署Ceph L版</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-10T22:21:44+08:00">
                2018-04-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1720709954166786" data-ad-slot="7859271626"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
        <p>ceph L版已经发布很久了，官方说默认使用BlueStore作为OSD的存储后端，在Cephalocon APAC 2018上也是讨论的焦点之一。</p>
<p>提到BlueStore，不得不说一说Ceph的STORAGE DEVICES。</p>
<h2 id="STORAGE-DEVICES"><a href="#STORAGE-DEVICES" class="headerlink" title="STORAGE DEVICES"></a>STORAGE DEVICES</h2><p>Ceph守护进程将数据存储在磁盘上：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Ceph OSDs ( Object Storage Daemons )</span><br><span class="line">    Client端的大多数数据写入Ceph后被存储的地方，一般而言，每个OSD都由单一存储设备支持，如传统硬盘（HDD）或固态硬盘（SSD）。</span><br><span class="line">    OSD还可以由多种设备组合，如存储数据的HDD和存储某些元数据的SSD（或SSD的分区）。</span><br><span class="line">    群集中OSD的数量通常取决于你要存储的数据量，还需要考虑每个存储设备的容量以及冗余级别和存储类型（replication或erasure coding）。</span><br><span class="line">Ceph Monitor</span><br><span class="line">    管理关键群集状态，如cluster membership和authentication信息。对于较小的集群，需要几千兆字节（几个GB），然而对于较大的集群，monitor的数据库可以达到几十甚至几百千兆（几十个GB甚至几百个GB）。</span><br></pre></td></tr></table></figure></p>
<h2 id="OSD-BACKENDS"><a href="#OSD-BACKENDS" class="headerlink" title="OSD BACKENDS"></a>OSD BACKENDS</h2><p>OSD可以通过两种方式管理存储的数据。从Luminous 12.2.z发行版开始，新的默认（推荐）后端是 BlueStore。在Luminous之前，默认（也是唯一的选择）是 FileStore。</p>
<h3 id="BLUESTORE"><a href="#BLUESTORE" class="headerlink" title="BLUESTORE"></a>BLUESTORE</h3><p>BlueStore是专门用于Ceph OSD管理磁盘上的数据的专用存储后端。在过去十年间，受到了FileStore管理OSD经验的启发.<br>BlueStore的主要功能包括：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">直接管理存储设备 ( Direct management of storage devices )</span><br><span class="line">    BlueStore使用原始块设备或分区。这避免了任何可能限制性能或增加复杂性的抽象层（如像XFS这样的本地文件系统）。</span><br><span class="line"></span><br><span class="line">使用RocksDB进行元数据管理 ( Metadata management with RocksDB )</span><br><span class="line">    为了管理内部元数据，我们嵌入了RocksDB的key/value数据库。例如在磁盘上，从object names到block locations的映射。</span><br><span class="line"></span><br><span class="line">完整的数据和元数据校验 ( Full data and metadata checksumming )</span><br><span class="line">    默认情况下，写入BlueStore的所有数据和元数据都受到一个或多个校验和的保护。没有数据或元数据在未经过验证的情况下，就从磁盘读取或返回给用户。</span><br><span class="line"></span><br><span class="line">内置压缩 ( Inline compression )</span><br><span class="line">    写入的数据在写入磁盘之前可以选择压缩。</span><br><span class="line"></span><br><span class="line">多设备元数据分层 ( Multi-device metadata tiering )</span><br><span class="line">    BlueStore允许将其内部journal（预写日志，write-ahead log）写入单独的高速设备（如SSD，NVMe或NVDIMM）以提高性能。</span><br><span class="line">    如果有大量更快速的存储可用，则内部元数据也可以存储在更快的设备上。</span><br><span class="line"></span><br><span class="line">高效的写时复制 ( Efficient copy-on-write )</span><br><span class="line">    RBD和CephFS快照依赖于copy-on-write clone机制，也在BlueStore中得到了有效的实现。这将为常规快照和erasure coded池提供高效的IO（依靠clone实现高效的two-phase commits）</span><br></pre></td></tr></table></figure></p>
<p><a href="http://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/" target="_blank" rel="noopener">http://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/</a><br><a href="http://docs.ceph.com/docs/master/rados/operations/bluestore-migration/" target="_blank" rel="noopener">http://docs.ceph.com/docs/master/rados/operations/bluestore-migration/</a></p>
<h3 id="FILESTORE"><a href="#FILESTORE" class="headerlink" title="FILESTORE"></a>FILESTORE</h3><p>FileStore是在Ceph中存储objects的传统方法。它依赖于标准文件系统（通常是XFS）和某个元数据的key/value数据库（传统上是LevelDB，现在是RocksDB）结合使用。<br>FileStore经过良好测试并广泛用于生产，但由于其整体设计和对传统文件系统存储object数据的依赖性，因此存在许多性能缺陷。<br>尽管FileStore通常能够在大多数与POSIX兼容的文件系统（包括btrfs和ext4）上运行，但我们只建议使用XFS。<br>btrfs和ext4都有已知的bug和缺陷，使用它们可能会导致数据丢失。默认情况下，所有的Ceph提供的工具都将使用XFS。</p>
<p><a href="http://docs.ceph.com/docs/master/rados/configuration/filestore-config-ref/" target="_blank" rel="noopener">http://docs.ceph.com/docs/master/rados/configuration/filestore-config-ref/</a></p>
<p>在ceph L版代码结构改动比较大，增加了CEPH-MGR向外部监测和管理系统提供额外的监测接口，今天就用虚拟机搭建实验环境玩一玩。</p>
<h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ~]# yum install -y redhat-lsb</span><br><span class="line">[root@cephL ~]# lsb_release -a</span><br><span class="line">LSB Version:	:core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch</span><br><span class="line">Distributor ID:	CentOS</span><br><span class="line">Description:	CentOS Linux release 7.4.1708 (Core)</span><br><span class="line">Release:	7.4.1708</span><br><span class="line">Codename:	Core</span><br><span class="line"></span><br><span class="line">[root@cephL ~]# lsblk</span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0   40G  0 disk</span><br><span class="line">├─sda1            8:1    0    1G  0 part /boot</span><br><span class="line">└─sda2            8:2    0   39G  0 part</span><br><span class="line">  ├─centos-root 253:0    0   36G  0 lvm  /</span><br><span class="line">  └─centos-swap 253:1    0    3G  0 lvm  [SWAP]</span><br><span class="line">sdb               8:16   0   30G  0 disk</span><br><span class="line">sdc               8:32   0   30G  0 disk</span><br><span class="line">sr0              11:0    1 1024M  0 rom</span><br></pre></td></tr></table></figure>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="安装pip和ceph-deploy"><a href="#安装pip和ceph-deploy" class="headerlink" title="安装pip和ceph-deploy"></a>安装pip和ceph-deploy</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ~]# curl "https://bootstrap.pypa.io/get-pip.py" -o "get-pip.py"</span><br><span class="line">[root@cephL ~]# python get-pip.py</span><br><span class="line">[root@cephL ~]# python -m pip install -U pip</span><br><span class="line">[root@cephL ~]# pip install --upgrade setuptools</span><br><span class="line">[root@cephL ~]# pip install ceph-deploy</span><br><span class="line">[root@cephL ~]# ceph-deploy --version</span><br><span class="line">2.0.0</span><br></pre></td></tr></table></figure>
<h3 id="安装ceph软件包"><a href="#安装ceph软件包" class="headerlink" title="安装ceph软件包"></a>安装ceph软件包</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ~]# mkdir ceph-deploy &amp;&amp; cd ceph-deploy</span><br><span class="line">[root@cephL ceph-deploy]# ceph-deploy install cephL --release luminous</span><br></pre></td></tr></table></figure>
<h3 id="开始部署一个新的集群，然后为它写一个CLUSTER-conf和keyring"><a href="#开始部署一个新的集群，然后为它写一个CLUSTER-conf和keyring" class="headerlink" title="开始部署一个新的集群，然后为它写一个CLUSTER.conf和keyring"></a>开始部署一个新的集群，然后为它写一个CLUSTER.conf和keyring</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ceph-deploy]# ceph-deploy new --public-network 192.168.56.101/24  --cluster-network 192.168.56.101/24 cephL</span><br></pre></td></tr></table></figure>
<h3 id="部署MON"><a href="#部署MON" class="headerlink" title="部署MON"></a>部署MON</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ceph-deploy]# ceph-deploy mon create-initial</span><br><span class="line">[root@cephL ceph-deploy]# ceph-deploy mon create cephL</span><br><span class="line">ceph        1110       1  0 12:57 ?        00:00:01 /usr/bin/ceph-mon -f --cluster ceph --id cephL --setuser ceph --setgroup ceph</span><br></pre></td></tr></table></figure>
<h3 id="部署OSD"><a href="#部署OSD" class="headerlink" title="部署OSD"></a>部署OSD</h3><p>bluestore方法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 在创建osd时，L版默认是bluestore</span><br><span class="line">[root@cephL ceph-deploy]# ceph-deploy osd create --data /dev/sdb cephL</span><br><span class="line">ceph        1514       1  0 12:57 ?        00:00:01 /usr/bin/ceph-osd -f --cluster ceph --id 0 --setuser ceph --setgroup ceph</span><br><span class="line">[root@cephL ceph-deploy]# ceph-deploy osd create --data /dev/sdc cephL</span><br><span class="line">ceph        1518       1  0 12:57 ?        00:00:01 /usr/bin/ceph-osd -f --cluster ceph --id 1 --setuser ceph --setgroup ceph</span><br></pre></td></tr></table></figure>
<p>遇到问题</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ceph-deploy]# ceph -s</span><br><span class="line">2018-04-10 12:00:19.660298 7fd1fe0ae700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory</span><br><span class="line">2018-04-10 12:00:19.660310 7fd1fe0ae700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication</span><br><span class="line">2018-04-10 12:00:19.660312 7fd1fe0ae700  0 librados: client.admin initialization error (2) No such file or directory</span><br><span class="line">[errno 2] error connecting to the cluster</span><br><span class="line"></span><br><span class="line">[root@cephL ceph-deploy]# chmod +r *</span><br><span class="line">[root@cephL ceph-deploy]# cp ceph.client.admin.keyring /etc/ceph/</span><br><span class="line">[root@cephL ceph-deploy]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     765752b7-1f77-4d0d-bc18-936b8ad409fd</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            no active mgr</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum cephL</span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 2 osds: 2 up, 2 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 bytes</span><br><span class="line">    usage:   0 kB used, 0 kB / 0 kB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>
<p>filestore方法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 如果是filestore则需要对data device和journal device先做GPT partition</span><br><span class="line">--data DATA           The OSD data logical volume (vg/lv) or absolute path to device</span><br><span class="line">--journal JOURNAL     Logical Volume (vg/lv) or path to GPT partition</span><br><span class="line"></span><br><span class="line">[root@cephL ceph-deploy]# fdisk /dev/sdb</span><br><span class="line">WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion.</span><br><span class="line">欢迎使用 fdisk (util-linux 2.23.2)。</span><br><span class="line">更改将停留在内存中，直到您决定将更改写入磁盘。</span><br><span class="line">使用写入命令前请三思。</span><br><span class="line">命令(输入 m 获取帮助)：g</span><br><span class="line">Building a new GPT disklabel (GUID: 80097CEF-475B-4161-ACC7-7164F6A39DD2)</span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">分区号 (1-128，默认 1)：</span><br><span class="line">第一个扇区 (2048-62914526，默认 2048)：</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (2048-62914526，默认 62914526)：</span><br><span class="line">已创建分区 1</span><br><span class="line">命令(输入 m 获取帮助)：w</span><br><span class="line">The partition table has been altered!</span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">正在同步磁盘。</span><br><span class="line"></span><br><span class="line">[root@cephL ceph-deploy]# fdisk /dev/sdc</span><br><span class="line">WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion.</span><br><span class="line">欢迎使用 fdisk (util-linux 2.23.2)。</span><br><span class="line">更改将停留在内存中，直到您决定将更改写入磁盘。</span><br><span class="line">使用写入命令前请三思。</span><br><span class="line">命令(输入 m 获取帮助)：g</span><br><span class="line">Building a new GPT disklabel (GUID: 21DFA98C-5BCF-40E7-A120-3DEDEA6600ED)</span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">分区号 (1-128，默认 1)：</span><br><span class="line">第一个扇区 (2048-62914526，默认 2048)：</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (2048-62914526，默认 62914526)：</span><br><span class="line">已创建分区 1</span><br><span class="line">命令(输入 m 获取帮助)：w</span><br><span class="line">The partition table has been altered!</span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">正在同步磁盘。</span><br><span class="line"></span><br><span class="line">[root@cephL ceph-deploy]# lsblk</span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0   40G  0 disk </span><br><span class="line">├─sda1            8:1    0    1G  0 part /boot</span><br><span class="line">└─sda2            8:2    0   39G  0 part </span><br><span class="line">  ├─centos-root 253:0    0   36G  0 lvm  /</span><br><span class="line">  └─centos-swap 253:1    0    3G  0 lvm  [SWAP]</span><br><span class="line">sdb               8:16   0   30G  0 disk </span><br><span class="line">└─sdb1            8:17   0   30G  0 part </span><br><span class="line">sdc               8:32   0   30G  0 disk </span><br><span class="line">└─sdc1            8:33   0   30G  0 part </span><br><span class="line">sr0              11:0    1 1024M  0 rom  </span><br><span class="line"></span><br><span class="line">[root@cephL ceph-deploy]# ceph-deploy osd create --filestore --fs-type xfs --data /dev/sdb1 --journal /dev/sdc1 cephL</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.0): /usr/bin/ceph-deploy osd create --filestore --fs-type xfs --data /dev/sdb1 --journal /dev/sdc1 cephL</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  bluestore                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x22c7320&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  fs_type                       : xfs</span><br><span class="line">[ceph_deploy.cli][INFO  ]  block_wal                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  journal                       : /dev/sdc1</span><br><span class="line">[ceph_deploy.cli][INFO  ]  subcommand                    : create</span><br><span class="line">[ceph_deploy.cli][INFO  ]  host                          : cephL</span><br><span class="line">[ceph_deploy.cli][INFO  ]  filestore                     : True</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function osd at 0x225ae60&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  zap_disk                      : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  data                          : /dev/sdb1</span><br><span class="line">[ceph_deploy.cli][INFO  ]  block_db                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  dmcrypt                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  debug                         : False</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Creating OSD on cluster ceph with data device /dev/sdb1</span><br><span class="line">[cephL][DEBUG ] connected to host: cephL </span><br><span class="line">[cephL][DEBUG ] detect platform information from remote host</span><br><span class="line">[cephL][DEBUG ] detect machine type</span><br><span class="line">[cephL][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.4.1708 Core</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Deploying osd to cephL</span><br><span class="line">[cephL][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[cephL][DEBUG ] find the location of an executable</span><br><span class="line">[cephL][INFO  ] Running command: /usr/sbin/ceph-volume --cluster ceph lvm create --filestore --data /dev/sdb1 --journal /dev/sdc1</span><br><span class="line">[cephL][DEBUG ] Running command: /bin/ceph-authtool --gen-print-key</span><br><span class="line">[cephL][DEBUG ] Running command: /bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new 8b7be4a6-b563-434e-b030-132880a10d31</span><br><span class="line">[cephL][DEBUG ] Running command: vgcreate --force --yes ceph-8e2515c1-6170-4299-b82c-a5a47681f946 /dev/sdb1</span><br><span class="line">[cephL][DEBUG ]  stdout: Physical volume "/dev/sdb1" successfully created.</span><br><span class="line">[cephL][DEBUG ]  stdout: Volume group "ceph-8e2515c1-6170-4299-b82c-a5a47681f946" successfully created</span><br><span class="line">[cephL][DEBUG ] Running command: lvcreate --yes -l 100%FREE -n osd-data-8b7be4a6-b563-434e-b030-132880a10d31 ceph-8e2515c1-6170-4299-b82c-a5a47681f946</span><br><span class="line">[cephL][DEBUG ]  stdout: Logical volume "osd-data-8b7be4a6-b563-434e-b030-132880a10d31" created.</span><br><span class="line">[cephL][DEBUG ] Running command: /bin/ceph-authtool --gen-print-key</span><br><span class="line">[cephL][DEBUG ] Running command: mkfs -t xfs -f -i size=2048 /dev/ceph-8e2515c1-6170-4299-b82c-a5a47681f946/osd-data-8b7be4a6-b563-434e-b030-132880a10d31</span><br><span class="line">[cephL][DEBUG ]  stdout: meta-data=/dev/ceph-8e2515c1-6170-4299-b82c-a5a47681f946/osd-data-8b7be4a6-b563-434e-b030-132880a10d31 isize=2048   agcount=4, agsize=1965824 blks</span><br><span class="line">[cephL][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">[cephL][DEBUG ]          =                       crc=1        finobt=0, sparse=0</span><br><span class="line">[cephL][DEBUG ] data     =                       bsize=4096   blocks=7863296, imaxpct=25</span><br><span class="line">[cephL][DEBUG ]          =                       sunit=0      swidth=0 blks</span><br><span class="line">[cephL][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1</span><br><span class="line">[cephL][DEBUG ] log      =internal log           bsize=4096   blocks=3839, version=2</span><br><span class="line">[cephL][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1</span><br><span class="line">[cephL][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">[cephL][DEBUG ] Running command: mount -t xfs -o rw,noatime,inode64 /dev/ceph-8e2515c1-6170-4299-b82c-a5a47681f946/osd-data-8b7be4a6-b563-434e-b030-132880a10d31 /var/lib/ceph/osd/ceph-0</span><br><span class="line">[cephL][DEBUG ] Running command: chown -R ceph:ceph /dev/sdc1</span><br><span class="line">[cephL][DEBUG ] Running command: ln -s /dev/sdc1 /var/lib/ceph/osd/ceph-0/journal</span><br><span class="line">[cephL][DEBUG ] Running command: ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/osd/ceph-0/activate.monmap</span><br><span class="line">[cephL][DEBUG ]  stderr: got monmap epoch 1</span><br><span class="line">[cephL][DEBUG ] Running command: chown -R ceph:ceph /dev/sdc1</span><br><span class="line">[cephL][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-0/</span><br><span class="line">[cephL][DEBUG ] Running command: ceph-osd --cluster ceph --osd-objectstore filestore --mkfs -i 0 --monmap /var/lib/ceph/osd/ceph-0/activate.monmap --osd-data /var/lib/ceph/osd/ceph-0/ --osd-journal /var/lib/ceph/osd/ceph-0/journal --osd-uuid 8b7be4a6-b563-434e-b030-132880a10d31 --setuser ceph --setgroup ceph</span><br><span class="line">[cephL][DEBUG ]  stderr: 2018-05-07 23:01:34.834993 7f315e466d00 -1 journal check: ondisk fsid 00000000-0000-0000-0000-000000000000 doesn't match expected 8b7be4a6-b563-434e-b030-132880a10d31, invalid (someone else's?) journal</span><br><span class="line">[cephL][DEBUG ]  stderr: 2018-05-07 23:01:34.865621 7f315e466d00 -1 journal do_read_entry(4096): bad header magic</span><br><span class="line">[cephL][DEBUG ] 2018-05-07 23:01:34.865667 7f315e466d00 -1 journal do_read_entry(4096): bad header magic</span><br><span class="line">[cephL][DEBUG ] 2018-05-07 23:01:34.865988 7f315e466d00 -1 read_settings error reading settings: (2) No such file or directory</span><br><span class="line">[cephL][DEBUG ]  stderr: 2018-05-07 23:01:34.916284 7f315e466d00 -1 created object store /var/lib/ceph/osd/ceph-0/ for osd.0 fsid 39f3b85e-ee3c-4d8d-93c2-7f7c8aa47121</span><br><span class="line">[cephL][DEBUG ] Running command: ceph-authtool /var/lib/ceph/osd/ceph-0/keyring --create-keyring --name osd.0 --add-key AQBDavBa0IPpIBAAlQxlaWxNrnTX/uaOMdZEQw==</span><br><span class="line">[cephL][DEBUG ]  stdout: creating /var/lib/ceph/osd/ceph-0/keyring</span><br><span class="line">[cephL][DEBUG ] added entity osd.0 auth auth(auid = 18446744073709551615 key=AQBDavBa0IPpIBAAlQxlaWxNrnTX/uaOMdZEQw== with 0 caps)</span><br><span class="line">[cephL][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-0/keyring</span><br><span class="line">[cephL][DEBUG ] --&gt; ceph-volume lvm prepare successful for: /dev/sdb1</span><br><span class="line">[cephL][DEBUG ] Running command: ln -snf /dev/sdc1 /var/lib/ceph/osd/ceph-0/journal</span><br><span class="line">[cephL][DEBUG ] Running command: chown -R ceph:ceph /dev/sdc1</span><br><span class="line">[cephL][DEBUG ] Running command: systemctl enable ceph-volume@lvm-0-8b7be4a6-b563-434e-b030-132880a10d31</span><br><span class="line">[cephL][DEBUG ]  stderr: Created symlink from /etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-0-8b7be4a6-b563-434e-b030-132880a10d31.service to /usr/lib/systemd/system/ceph-volume@.service.</span><br><span class="line">[cephL][DEBUG ] Running command: systemctl start ceph-osd@0</span><br><span class="line">[cephL][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: 0</span><br><span class="line">[cephL][DEBUG ] --&gt; ceph-volume lvm create successful for: /dev/sdb1</span><br><span class="line">[cephL][INFO  ] checking OSD status...</span><br><span class="line">[cephL][DEBUG ] find the location of an executable</span><br><span class="line">[cephL][INFO  ] Running command: /bin/ceph --cluster=ceph osd stat --format=json</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Host cephL is now ready for osd use.</span><br></pre></td></tr></table></figure>
<h3 id="移除OSD"><a href="#移除OSD" class="headerlink" title="移除OSD"></a>移除OSD</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 使OSD进入out状态</span><br><span class="line">[root@cephL ceph-deploy]# ceph osd out 0</span><br><span class="line">marked out osd.0.</span><br><span class="line"><span class="meta">#</span> 观察数据迁移</span><br><span class="line">[root@cephL ceph-deploy]# ceph -w</span><br><span class="line"><span class="meta">#</span> 停止对应的OSD进程</span><br><span class="line">[root@cephL ceph-deploy]# sudo systemctl stop ceph-osd@0</span><br><span class="line"><span class="meta">#</span> 清除数据</span><br><span class="line">[root@cephL ceph-deploy]# ceph osd purge 0 --yes-i-really-mean-it</span><br><span class="line">purged osd.0</span><br><span class="line"><span class="meta">#</span> 在ceph.conf中移除osd配置</span><br><span class="line">[root@cephL ceph-deploy]# vi /etc/ceph/ceph.conf</span><br></pre></td></tr></table></figure>
<h3 id="部署CEPH-MGR"><a href="#部署CEPH-MGR" class="headerlink" title="部署CEPH-MGR"></a>部署CEPH-MGR</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">install netstat tool</span><br><span class="line">[root@cephL ~]# yum -y install net-tools</span><br><span class="line"></span><br><span class="line">[root@cephL ceph-deploy]# ceph-deploy mgr create cephL:cephLMGR</span><br><span class="line">ceph        1111       1  0 12:57 ?        00:00:08 /usr/bin/ceph-mgr -f --cluster ceph --id cephLMGR --setuser ceph --setgroup ceph</span><br><span class="line">[root@cephL ceph-deploy]# ceph mgr module enable dashboard</span><br><span class="line"></span><br><span class="line">open 7000 port</span><br><span class="line">[root@cephL ceph-deploy]# sudo firewall-cmd --zone=public --add-port=7000/tcp --permanent</span><br><span class="line">[root@cephL ceph-deploy]# sudo firewall-cmd --reload</span><br></pre></td></tr></table></figure>
<p>相关命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ceph-deploy]# ceph mgr module ls</span><br><span class="line">[root@cephL ceph-deploy]# ceph mgr services</span><br><span class="line">[root@cephL ceph-deploy]# ceph tell mgr help</span><br></pre></td></tr></table></figure>
<h3 id="部署MDS并创建CEPH-FS"><a href="#部署MDS并创建CEPH-FS" class="headerlink" title="部署MDS并创建CEPH FS"></a>部署MDS并创建CEPH FS</h3><p><img src="http://docs.ceph.com/docs/master/_images/ditaa-b5a320fc160057a1a7da010b4215489fa66de242.png" alt></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ceph-deploy]# ceph-deploy mds create cephL</span><br><span class="line">ceph        2150       1  0 13:00 ?        00:00:00 /usr/bin/ceph-mds -f --cluster ceph --id cephL --setuser ceph --setgroup ceph</span><br></pre></td></tr></table></figure>
<p>Ceph文件系统至少需要两个RADOS pool，一个用于存储数据，一个用于存储元数据。</p>
<p>配置这些pool时，可以考虑：</p>
<p>​     对元数据pool使用更多的replication数量，因为该pool中的任何数据丢失都可能导致整个文件系统无法访问。</p>
<p>​     为元数据pool使用SSD等较低延迟的存储设备，因为这将直接影响客户端上文件系统操作的延迟。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create cephfs_data &lt;pg_num&gt;</span><br><span class="line">ceph osd pool create cephfs_metadata &lt;pg_num&gt;</span><br><span class="line">例如：</span><br><span class="line">[root@cephL ceph-deploy]# ceph osd pool create cephfs_data 32</span><br><span class="line">[root@cephL ceph-deploy]# ceph osd pool create cephfs_metadata 32</span><br></pre></td></tr></table></figure>
<p>更改pool的副本数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &#123;poolname&#125; size &#123;num-replicas&#125;</span><br><span class="line">例如：</span><br><span class="line">[root@cephL ceph-deploy]# ceph osd pool set cephfs_data size 1</span><br><span class="line">[root@cephL ceph-deploy]# ceph osd pool set cephfs_data size 1</span><br></pre></td></tr></table></figure>
<p>一旦创建了pool，就可以使用fs new命令启用文件系统：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph fs new &lt;fs_name&gt; &lt;metadata&gt; &lt;data&gt;</span><br><span class="line">例如：</span><br><span class="line">ceph fs new cephFS cephfs_metadata cephfs_data</span><br></pre></td></tr></table></figure>
<p>一旦创建了文件系统，您的MDS将能够进入active状态。例如，在single MDS system中：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ceph-deploy]# ceph mds stat</span><br><span class="line">cephFS-1/1/1 up  &#123;0=cephL=up:active&#125;</span><br></pre></td></tr></table></figure>
<p>一旦创建了文件系统并且MDS处于active状态，你就可以挂载文件系统了。如果您创建了多个文件系统，在挂载文件系统时，选择使用哪一个。</p>
<p>如果创建了多个文件系统，并且client在挂载时没有指定挂载哪个文件系统，你可以使用ceph fs set-default命令来设置client默认看到的文件系统。</p>
<p>挂载CEPH FS ( File System ) 有两种方式：</p>
<p><strong>KERNEL DRIVER</strong></p>
<p>要挂载Ceph文件系统，您可以在知道monitor主机IP地址的情况下使用mount命令，或使用mount.ceph utility将monitor主机名解析为IP地址。例如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /mnt/mycephfs</span><br><span class="line">sudo mount -t ceph 192.168.0.1:6789:/ /mnt/mycephfs</span><br><span class="line">例如：</span><br><span class="line">[root@cephL ceph-deploy]# sudo mount -t ceph 192.168.56.101:6789:/ /mnt/mycephfs</span><br><span class="line">mount error 22 = Invalid argument</span><br><span class="line">Ceph 10.x (Jewel)版本开始，如果使用kernel方式（无论是krbd还是cephFS）官方推荐至少使用4.x的kernel。</span><br><span class="line">如果无法升级linux kernel，那么映射rbd请使用librbd方式，cephFS请使用fuse方式。</span><br></pre></td></tr></table></figure>
<p>如果挂载Ceph文件系统时开启了cephx authentication，您必须指定user和secret。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t ceph 192.168.0.1:6789:/ /mnt/mycephfs -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</span><br></pre></td></tr></table></figure>
<p>上述用法在Bash history中留下了secret。更安全的方法是从文件中读取secret。 例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t ceph 192.168.0.1:6789:/ /mnt/mycephfs -o name=admin,secretfile=/etc/ceph/admin.secret</span><br></pre></td></tr></table></figure>
<p>如果您有多个文件系统，请使用mds_namespace选项指定要挂载的文件系统，例如-o mds_namespace=myfs</p>
<p>要卸载Ceph文件系统，可以使用umount命令。 例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo umount /mnt/mycephfs</span><br><span class="line">提示：在执行此命令之前，请确保您不在挂载的目录中。</span><br></pre></td></tr></table></figure>
<p><strong>FUSE</strong></p>
<p>在用户空间（FUSE）中挂载Ceph文件系统之前，请确保客户端主机具有Ceph配置文件的副本以及Ceph元数据服务器的CAPS keyring。</p>
<p>在您的客户端主机上，将Ceph配置文件从monitor主机复制到/etc/ceph目录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /etc/ceph</span><br><span class="line">sudo scp &#123;user&#125;@&#123;server-machine&#125;:/etc/ceph/ceph.conf /etc/ceph/ceph.conf</span><br></pre></td></tr></table></figure>
<p>在您的客户端主机上，将monitor主机的Ceph keyring复制到/etc/ceph目录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo scp &#123;user&#125;@&#123;server-machine&#125;:/etc/ceph/ceph.keyring /etc/ceph/ceph.keyring</span><br></pre></td></tr></table></figure>
<p>确保Ceph配置文件和keyring在您的客户端机器上设置了适当的权限（例如，chmod 644）。</p>
<p>要将Ceph文件系统挂在为FUSE，可以使用ceph-fuse命令。 例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /home/usernname/cephfs</span><br><span class="line">sudo ceph-fuse -m 192.168.0.1:6789 /home/username/cephfs</span><br></pre></td></tr></table></figure>
<p>如果您拥有多个文件系统，请使用 –client_mds_namespace 命令行参数指定要挂载哪一个文件系统，或者向ceph.conf中添加client_mds_namespace设置。</p>
<p>要自动挂载ceph-fuse，您可以在system fstab中添加一个条目。此外还可以使用ceph-fuse@.service和ceph-fuse.target systemd units。通常这些unit文件为ceph-fuse描述默认的dependencies和推荐的execution context。例如使用ceph-fuse挂载到/mnt：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl start ceph-fuse@/mnt.service</span><br></pre></td></tr></table></figure>
<p>持久化挂载点可通过以下方式进行设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl enable ceph-fuse@/mnt.service</span><br></pre></td></tr></table></figure>
<h3 id="部署RGW"><a href="#部署RGW" class="headerlink" title="部署RGW"></a>部署RGW</h3><p><img src="http://docs.ceph.com/docs/master/_images/ditaa-50d12451eb76c5c72c4574b08f0320b39a42e5f1.png" alt></p>
<p>Ceph Object Gateway原来叫RADOS Gateway，它是构建在librados之上的对象存储接口，为应用程序提供了一个RESTful gateway，用户可以通过HTTP协议访问Ceph存储集群。</p>
<p>Ceph Object Storage支持两个接口：</p>
<ul>
<li><p>S3-compatible：与Amazon S3 RESTful API中一些子集兼容的接口，提供对象存储功能。</p>
</li>
<li><p>Swift-compatible：与OpenStack Swift API中一些子集兼容的接口，提供对象存储功能。</p>
</li>
</ul>
<p>Ceph Object Storage使用Ceph Object Gateway daemon (radosgw)，它是一个HTTP server，用于与Ceph存储集群进行交互。由于它提供了与OpenStack Swift和Amazon S3兼容的接口，因此Ceph Object Gateway具有自己的用户管理。Ceph Object Gateway可以将数据存储在与Ceph Filesystem和Ceph Block Device相同的Ceph存储集群中。但是我相信在生产环境中不会这么做，如果数据量大的话会影响Ceph Filesystem和Ceph Block Device的性能，个人一般会独立出一个Ceph Object Gateway集群。S3和Swift API共享一个通用的namespace，因此您可以使用一个API编写数据并使用另一个API检索它。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Note：Ceph Object Storage 不使用 Ceph Metadata Server</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 必须部署MGR，才能部署RGW</span><br><span class="line"></span><br><span class="line">[root@cephL ceph-deploy]# ceph-deploy rgw create cephL:RGW</span><br><span class="line">root        2799       1  0 13:13 ?        00:00:00 /usr/bin/radosgw -f --cluster ceph --name client.rgw.RGW --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 重启RGW</span><br><span class="line">[root@cephL ~]# systemctl restart ceph-radosgw@rgw.cephL.service</span><br><span class="line">[root@cephL ~]# systemctl restart ceph-radosgw@rgw</span><br><span class="line"></span><br><span class="line">问题一，这难道是ceph-deploy 2.0.0的坑？</span><br><span class="line">[root@cephL ~]# tailf /var/log/ceph/ceph-client.rgw.log</span><br><span class="line">2018-05-11 22:30:31.999421 7f537c31fe00  0 ceph version 12.2.4 (52085d5249a80c5f5121a76d6288429f35e4e77b) luminous (stable), process (unknown), pid 3450</span><br><span class="line">2018-05-11 22:30:32.021546 7f537c31fe00 -1 auth: unable to find a keyring on /var/lib/ceph/radosgw/ceph-rgw/keyring: (2) No such file or directory</span><br><span class="line">2018-05-11 22:30:32.021561 7f537c31fe00 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication</span><br><span class="line">2018-05-11 22:30:32.021563 7f537c31fe00  0 librados: client.rgw initialization error (2) No such file or directory</span><br><span class="line">2018-05-11 22:30:32.022900 7f537c31fe00 -1 Couldn't init storage provider (RADOS)</span><br><span class="line"></span><br><span class="line">[root@cephL radosgw]# pwd</span><br><span class="line">/var/lib/ceph/radosgw</span><br><span class="line">[root@cephL radosgw]# ls</span><br><span class="line">ceph-rgw.RGW</span><br><span class="line">[root@cephL radosgw]# mv ceph-rgw.RGW  ceph-rgw</span><br></pre></td></tr></table></figure>
<h3 id="配置变动"><a href="#配置变动" class="headerlink" title="配置变动"></a>配置变动</h3><p>在L版中，删除pool的操作做了强制限制。需要在/etc/ceph/ceph.conf中加入相关参数才允许删除pool。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 允许删除pool，需要添加</span><br><span class="line">mon allow pool delete = true</span><br></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>感谢支持，打赏￥0.5</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Jiang Yu 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Jiang Yu 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">

      
      


      
        <div class="post-tags">
          
            <a href="/tags/CEPH/" rel="tag"># CEPH</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/07/Go命令/" rel="next" title="Go命令">
                <i class="fa fa-chevron-left"></i> Go命令
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/04/12/RocksDB介绍/" rel="prev" title="RocksDB介绍">
                RocksDB介绍 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Jiang Yu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">110</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">45</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#STORAGE-DEVICES"><span class="nav-number">1.</span> <span class="nav-text">STORAGE DEVICES</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#OSD-BACKENDS"><span class="nav-number">2.</span> <span class="nav-text">OSD BACKENDS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BLUESTORE"><span class="nav-number">2.1.</span> <span class="nav-text">BLUESTORE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FILESTORE"><span class="nav-number">2.2.</span> <span class="nav-text">FILESTORE</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#环境信息"><span class="nav-number">3.</span> <span class="nav-text">环境信息</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装"><span class="nav-number">4.</span> <span class="nav-text">安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安装pip和ceph-deploy"><span class="nav-number">4.1.</span> <span class="nav-text">安装pip和ceph-deploy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装ceph软件包"><span class="nav-number">4.2.</span> <span class="nav-text">安装ceph软件包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#开始部署一个新的集群，然后为它写一个CLUSTER-conf和keyring"><span class="nav-number">4.3.</span> <span class="nav-text">开始部署一个新的集群，然后为它写一个CLUSTER.conf和keyring</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署MON"><span class="nav-number">4.4.</span> <span class="nav-text">部署MON</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署OSD"><span class="nav-number">4.5.</span> <span class="nav-text">部署OSD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#移除OSD"><span class="nav-number">4.6.</span> <span class="nav-text">移除OSD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署CEPH-MGR"><span class="nav-number">4.7.</span> <span class="nav-text">部署CEPH-MGR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署MDS并创建CEPH-FS"><span class="nav-number">4.8.</span> <span class="nav-text">部署MDS并创建CEPH FS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署RGW"><span class="nav-number">4.9.</span> <span class="nav-text">部署RGW</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置变动"><span class="nav-number">4.10.</span> <span class="nav-text">配置变动</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiang Yu</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>

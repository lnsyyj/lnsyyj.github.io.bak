<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ceph 官方librbd接口demo]]></title>
    <url>%2F2020%2F08%2F01%2Fceph-%E5%AE%98%E6%96%B9librbd%E6%8E%A5%E5%8F%A3demo%2F</url>
    <content type="text"><![CDATA[以下代码来自ceph master分支，examples/librbd/hello_world.cc目录，该目录中有操作librbd接口的demo。 实验使用ceph-14.2.10 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-// vim: ts=8 sw=2 smarttab/* * Ceph - scalable distributed file system * * This is free software; you can redistribute it and/or * modify it under the terms of the GNU Lesser General Public * License version 2.1, as published by the Free Software * Foundation. See file COPYING. */// ceph 14.2.10需要安装libradospp-dev，否则无法找到rados/librados.hpp// 安装librados-dev 和librbd-dev package，便可include rados/librados.hpp与rbd/librbd.hpp头文件#include &lt;rados/librados.hpp&gt;#include &lt;rbd/librbd.hpp&gt;#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;sstream&gt;int main(int argc, const char **argv) &#123; int ret = 0; // 我们将在下面使用这些变量 const char *pool_name = &quot;hello_world_pool&quot;; std::string hello(&quot;hello world!&quot;); std::string object_name(&quot;hello_object&quot;); librados::IoCtx io_ctx; // 首先，我们创建一个Rados对象并对其进行初始化 librados::Rados rados; &#123; ret = rados.init(&quot;admin&quot;); // 只需使用 client.admin keyring if (ret &lt; 0) &#123; // 让我们处理可能出现的任何错误 std::cerr &lt;&lt; &quot;couldn&apos;t initialize rados! error &quot; &lt;&lt; ret &lt;&lt; std::endl; ret = EXIT_FAILURE; goto out; &#125; else &#123; std::cout &lt;&lt; &quot;we just set up a rados cluster object&quot; &lt;&lt; std::endl; &#125; &#125; /* * 现在我们需要获取rados对象的config info. * 它可以为我们解析argv来找到ID，monitors等，因此我们就使用它。 */ &#123; ret = rados.conf_parse_argv(argc, argv); if (ret &lt; 0) &#123; // 这不太可能发生，但是我们需要努力成为一个好公民。 std::cerr &lt;&lt; &quot;failed to parse config options! error &quot; &lt;&lt; ret &lt;&lt; std::endl; ret = EXIT_FAILURE; goto out; &#125; else &#123; std::cout &lt;&lt; &quot;we just parsed our config options&quot; &lt;&lt; std::endl; // 如果用户指定了config file，我们也想使用它，而conf_parse_argv不会为我们这样做。 for (int i = 0; i &lt; argc; ++i) &#123; if ((strcmp(argv[i], &quot;-c&quot;) == 0) || (strcmp(argv[i], &quot;--conf&quot;) == 0)) &#123; ret = rados.conf_read_file(argv[i + 1]); if (ret &lt; 0) &#123; // 如果config file格式错误，这可能会失败 std::cerr &lt;&lt; &quot;failed to parse config file &quot; &lt;&lt; argv[i + 1] &lt;&lt; &quot;! error&quot; &lt;&lt; ret &lt;&lt; std::endl; ret = EXIT_FAILURE; goto out; &#125; break; &#125; &#125; &#125; &#125; /* * 接下来，我们connect到cluster */ &#123; ret = rados.connect(); if (ret &lt; 0) &#123; std::cerr &lt;&lt; &quot;couldn&apos;t connect to cluster! error &quot; &lt;&lt; ret &lt;&lt; std::endl; ret = EXIT_FAILURE; goto out; &#125; else &#123; std::cout &lt;&lt; &quot;we just connected to the rados cluster&quot; &lt;&lt; std::endl; &#125; &#125; /* * 让我们创建自己的pool * 请注意，此命令将创建默认PG数的pool（PG数由monitors指定），这可能不适合生产使用，但可以进行测试。 */ &#123; ret = rados.pool_create(pool_name); if (ret &lt; 0) &#123; std::cerr &lt;&lt; &quot;couldn&apos;t create pool! error &quot; &lt;&lt; ret &lt;&lt; std::endl; return EXIT_FAILURE; &#125; else &#123; std::cout &lt;&lt; &quot;we just created a new pool named &quot; &lt;&lt; pool_name &lt;&lt; std::endl; &#125; &#125; /* * 创建一个IoCtx，用于对pool进行IO */ &#123; ret = rados.ioctx_create(pool_name, io_ctx); if (ret &lt; 0) &#123; std::cerr &lt;&lt; &quot;couldn&apos;t set up ioctx! error &quot; &lt;&lt; ret &lt;&lt; std::endl; ret = EXIT_FAILURE; goto out; &#125; else &#123; std::cout &lt;&lt; &quot;we just created an ioctx for our pool&quot; &lt;&lt; std::endl; &#125; &#125; /* * 创建一个rbd image并向其写入数据 */ &#123; std::string name = &quot;librbd_test&quot;; uint64_t size = 2 &lt;&lt; 20; int order = 0; librbd::RBD rbd; librbd::Image image; ret = rbd.create(io_ctx, name.c_str(), size, &amp;order); if (ret &lt; 0) &#123; std::cerr &lt;&lt; &quot;couldn&apos;t create an rbd image! error &quot; &lt;&lt; ret &lt;&lt; std::endl; ret = EXIT_FAILURE; goto out; &#125; else &#123; std::cout &lt;&lt; &quot;we just created an rbd image&quot; &lt;&lt; std::endl; &#125; ret = rbd.open(io_ctx, image, name.c_str(), NULL); if (ret &lt; 0) &#123; std::cerr &lt;&lt; &quot;couldn&apos;t open the rbd image! error &quot; &lt;&lt; ret &lt;&lt; std::endl; ret = EXIT_FAILURE; goto out; &#125; else &#123; std::cout &lt;&lt; &quot;we just opened the rbd image&quot; &lt;&lt; std::endl; &#125; int TEST_IO_SIZE = 512; char test_data[TEST_IO_SIZE + 1]; int i; for (i = 0; i &lt; TEST_IO_SIZE; ++i) &#123; test_data[i] = (char) (rand() % (126 - 33) + 33); &#125; test_data[TEST_IO_SIZE] = &apos;\0&apos;; size_t len = strlen(test_data); ceph::bufferlist bl; bl.append(test_data, len); ret = image.write(0, len, bl); if (ret &lt; 0) &#123; std::cerr &lt;&lt; &quot;couldn&apos;t write to the rbd image! error &quot; &lt;&lt; ret &lt;&lt; std::endl; ret = EXIT_FAILURE; goto out; &#125; else &#123; std::cout &lt;&lt; &quot;we just wrote data to our rbd image &quot; &lt;&lt; std::endl; &#125; /* * 读取image并将其与我们编写的数据进行比较 */ ceph::bufferlist bl_r; int read; read = image.read(0, TEST_IO_SIZE, bl_r); if (read &lt; 0) &#123; std::cerr &lt;&lt; &quot;we couldn&apos;t read data from the image! error&quot; &lt;&lt; std::endl; ret = EXIT_FAILURE; goto out; &#125; std::string bl_res(bl_r.c_str(), read); int res = memcmp(bl_res.c_str(), test_data, TEST_IO_SIZE); if (res != 0) &#123; std::cerr &lt;&lt; &quot;what we read didn&apos;t match expected! error&quot; &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; &quot;we read our data on the image successfully&quot; &lt;&lt; std::endl; &#125; image.close(); /* * 现在删除image */ ret = rbd.remove(io_ctx, name.c_str()); if (ret &lt; 0) &#123; std::cerr &lt;&lt; &quot;failed to delete rbd image! error &quot; &lt;&lt; ret &lt;&lt; std::endl; ret = EXIT_FAILURE; goto out; &#125; else &#123; std::cout &lt;&lt; &quot;we just deleted our rbd image &quot; &lt;&lt; std::endl; &#125; &#125; ret = EXIT_SUCCESS; out: /* * 现在我们已经完成了，因此让我们删除pool，然后正常关闭connection。 */ int delete_ret = rados.pool_delete(pool_name); if (delete_ret &lt; 0) &#123; // be careful not to std::cerr &lt;&lt; &quot;We failed to delete our test pool!&quot; &lt;&lt; std::endl; ret = EXIT_FAILURE; &#125; rados.shutdown(); return ret;&#125; examples/librbd/Makefile文件 1234567891011121314151617181920212223242526CXX?=g++CXX_FLAGS?=-std=c++11 -Wno-unused-parameter -Wall -Wextra -Werror -gCXX_LIBS?=-lboost_system -lrbd -lradosCXX_INC?=$(LOCAL_LIBRADOS_INC)CXX_CC=$(CXX) $(CXX_FLAGS) $(CXX_INC) $(LOCAL_LIBRADOS)# Relative path to the Ceph source:CEPH_SRC_HOME?=../../srcCEPH_BLD_HOME?=../../buildLOCAL_LIBRADOS?=-L$(CEPH_BLD_HOME)/lib/ -Wl,-rpath,$(CEPH_BLD_HOME)/libLOCAL_LIBRADOS_INC?=-I$(CEPH_SRC_HOME)/includeall: hello_world_cpp # Build against the system librados instead of the one in the build tree:all-system: LOCAL_LIBRADOS=all-system: LOCAL_LIBRADOS_INC=all-system: allhello_world_cpp: hello_world.cc $(CXX_CC) -o hello_world_cpp hello_world.cc $(CXX_LIBS)clean: rm -f hello_world_cpp 安装调试包1234567apt-get install -y ceph-base-dbg ceph-common-dbg ceph-fuse-dbg ceph-mds-dbg ceph-mgr-dbg ceph-mon-dbg ceph-osd-dbg libcephfs2-dbg librados2-dbg libradosstriper1-dbg librbd1-dbg librgw2-dbg python-cephfs-dbg python-rados-dbg python-rbd-dbg python-rgw-dbg python3-cephfs-dbg python3-rados-dbg python3-rbd-dbg python3-rgw-dbg radosgw-dbg rbd-fuse-dbg rbd-mirror-dbg rbd-nbd-dbgubuntu包的命名规则，引致 https://blog.csdn.net/anlian523/article/details/90733234通常情况下-dev后缀（develope）：包含了库的接口（.h文件即头文件），这个为了当你开发一个程序时想要链接到这个包时。-dbg后缀（debug）：包含调试符号，通常仅供开发人员使用该软件或调试软件的人员使用。-utils后缀（utility）：通常提供一些额外的命令行工具。 它可能会将用户暴露给内部功能或仅提供CLI。 编译并使用gdb调试 12345678910111213141516171819202122make -dgdb ./hello_world_cpp(gdb) b main(gdb) r -c /etc/ceph/ceph.conf ### 66 break;(gdb) p argv[0]$2 = 0x7fffffffe77b &quot;/root/codes/hello_world_cpp&quot;(gdb) p argv[1]$3 = 0x7fffffffe797 &quot;-c&quot;(gdb) p argv[2]$4 = 0x7fffffffe79a &quot;/etc/ceph/ceph.conf&quot;### 119 uint64_t size = 2 &lt;&lt; 20;(gdb) p size$7 = 2097152### gdb跳出循环方法 until NUM### gdb跳出函数方法 finish(gdb) until 149main (argc=3, argv=0x7fffffffe528) at hello_world.cc:149]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph pg 状态]]></title>
    <url>%2F2020%2F03%2F27%2Fceph-pg-%E7%8A%B6%E6%80%81%2F</url>
    <content type="text"><![CDATA[ceph 14.2.7 1234567891011121314151617181920212223242526272829303132333435/* * pg states */#define PG_STATE_CREATING (1ULL &lt;&lt; 0) // creating#define PG_STATE_ACTIVE (1ULL &lt;&lt; 1) // i am active. (primary: replicas too)#define PG_STATE_CLEAN (1ULL &lt;&lt; 2) // peers are complete, clean of stray replicas.#define PG_STATE_DOWN (1ULL &lt;&lt; 4) // a needed replica is down, PG offline#define PG_STATE_RECOVERY_UNFOUND (1ULL &lt;&lt; 5) // recovery stopped due to unfound#define PG_STATE_BACKFILL_UNFOUND (1ULL &lt;&lt; 6) // backfill stopped due to unfound#define PG_STATE_PREMERGE (1ULL &lt;&lt; 7) // i am prepare to merging#define PG_STATE_SCRUBBING (1ULL &lt;&lt; 8) // scrubbing//#define PG_STATE_SCRUBQ (1ULL &lt;&lt; 9) // queued for scrub#define PG_STATE_DEGRADED (1ULL &lt;&lt; 10) // pg contains objects with reduced redundancy#define PG_STATE_INCONSISTENT (1ULL &lt;&lt; 11) // pg replicas are inconsistent (but shouldn&apos;t be)#define PG_STATE_PEERING (1ULL &lt;&lt; 12) // pg is (re)peering#define PG_STATE_REPAIR (1ULL &lt;&lt; 13) // pg should repair on next scrub#define PG_STATE_RECOVERING (1ULL &lt;&lt; 14) // pg is recovering/migrating objects#define PG_STATE_BACKFILL_WAIT (1ULL &lt;&lt; 15) // [active] reserving backfill#define PG_STATE_INCOMPLETE (1ULL &lt;&lt; 16) // incomplete content, peering failed.#define PG_STATE_STALE (1ULL &lt;&lt; 17) // our state for this pg is stale, unknown.#define PG_STATE_REMAPPED (1ULL &lt;&lt; 18) // pg is explicitly remapped to different OSDs than CRUSH#define PG_STATE_DEEP_SCRUB (1ULL &lt;&lt; 19) // deep scrub: check CRC32 on files#define PG_STATE_BACKFILLING (1ULL &lt;&lt; 20) // [active] backfilling pg content#define PG_STATE_BACKFILL_TOOFULL (1ULL &lt;&lt; 21) // backfill can&apos;t proceed: too full#define PG_STATE_RECOVERY_WAIT (1ULL &lt;&lt; 22) // waiting for recovery reservations#define PG_STATE_UNDERSIZED (1ULL &lt;&lt; 23) // pg acting &lt; pool size#define PG_STATE_ACTIVATING (1ULL &lt;&lt; 24) // pg is peered but not yet active#define PG_STATE_PEERED (1ULL &lt;&lt; 25) // peered, cannot go active, can recover#define PG_STATE_SNAPTRIM (1ULL &lt;&lt; 26) // trimming snaps#define PG_STATE_SNAPTRIM_WAIT (1ULL &lt;&lt; 27) // queued to trim snaps#define PG_STATE_RECOVERY_TOOFULL (1ULL &lt;&lt; 28) // recovery can&apos;t proceed: too full#define PG_STATE_SNAPTRIM_ERROR (1ULL &lt;&lt; 29) // error stopped trimming snaps#define PG_STATE_FORCED_RECOVERY (1ULL &lt;&lt; 30) // force recovery of this pg before any other#define PG_STATE_FORCED_BACKFILL (1ULL &lt;&lt; 31) // force backfill of this pg before any other#define PG_STATE_FAILED_REPAIR (1ULL &lt;&lt; 32) // A repair failed to fix all errors]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph运维笔记]]></title>
    <url>%2F2020%2F03%2F11%2Fceph%E8%BF%90%E7%BB%B4%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[OSD换盘操作1、确保删除OSD时，集群没有接近near full 2、检查删除OSD时，是否有数据丢失， ceph osd ok-to-stop {osd-num} 12345678无数据丢失情况root@ceph:~# ceph osd ok-to-stop 2OSD(s) 2 are ok to stop without reducing availability or risking data, provided there are no other concurrent failures or interventions.70 PGs are likely to be degraded (but remain available) as a result.有数据丢失情况root@ceph:~# ceph osd ok-to-stop 2Error EBUSY: 18 PGs are already too degraded, would become too degraded or might become unavailable 3、标记OSD out， ceph osd out {osd-num} 1234567891011root@ceph:~# ceph osd out 3marked out osd.3. root@ceph:~# ceph osd treeID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.08997 root default -3 0.08997 host ceph 0 ssd 0.02249 osd.0 up 1.00000 1.00000 1 ssd 0.02249 osd.1 up 1.00000 1.00000 2 ssd 0.02249 osd.2 up 1.00000 1.00000 3 ssd 0.02249 osd.3 down 0 1.00000 4、停止OSD进程， systemctl stop ceph-osd@{osd-num} 123456root@ceph:~# systemctl stop ceph-osd@3root@ceph:~# ps -ef | grep ceph-osdceph 3334 1 0 10:19 ? 00:00:13 /usr/bin/ceph-osd -f --cluster ceph --id 2 --setuser ceph --setgroup cephceph 3336 1 0 10:19 ? 00:00:10 /usr/bin/ceph-osd -f --cluster ceph --id 0 --setuser ceph --setgroup cephceph 3337 1 0 10:19 ? 00:00:10 /usr/bin/ceph-osd -f --cluster ceph --id 1 --setuser ceph --setgroup ceph 5、从crush map中删除OSD， ceph osd purge {osd-num} –yes-i-really-mean-it 12345678910root@ceph:~# ceph osd purge 3 --yes-i-really-mean-itpurged osd.3root@ceph:~# ceph osd treeID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.06747 root default -3 0.06747 host ceph 0 ssd 0.02249 osd.0 up 1.00000 1.00000 1 ssd 0.02249 osd.1 up 1.00000 1.00000 2 ssd 0.02249 osd.2 up 1.00000 1.00000 6、从ceph.conf中删除OSD 7、卸载OSD挂载点， umount /var/lib/ceph/osd/ceph-{osd-num} 1umount /var/lib/ceph/osd/ceph-3 8、删除/var/lib/ceph/osd/ceph-{osd-num}/目录 1rm -rf /var/lib/ceph/osd/ceph-3/ 9、换盘后，重新添加OSD]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[慕课网-Go语言语法进阶指南]]></title>
    <url>%2F2020%2F03%2F01%2F%E6%85%95%E8%AF%BE%E7%BD%91-Go%E8%AF%AD%E8%A8%80%E8%AF%AD%E6%B3%95%E8%BF%9B%E9%98%B6%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[课程地址：https://www.imooc.com/video/21088 Golang语言趋势图：https://www.tiobe.com/tiobe-index/go/ 内建方法在任何的包、任何的文件都可以引用这样的方法，无需导入包就可以使用。 常用的内建方法make1234567891011121314151617181920212223242526272829303132333435make：创建slice、map、chan （这三种类型必须使用make创建，这三种类型必须要Golang帮我们初始化好，避免在使用中出现问题）make返回类型引用（slice类似于数组，数组在golang中是不可变化长度的，slice是可变长度的。map是一个key、value数据结构。chan是管道。）func main() &#123; makeSlice() makeMap() makeChan()&#125;// makeSlice 创建切片func makeSlice() &#123; mSlice := make([]string, 3) mSlice[0] = &quot;dog&quot; mSlice[1] = &quot;cat&quot; mSlice[2] = &quot;tiger&quot; fmt.Println(mSlice)&#125;// makeMap 创建mapfunc makeMap() &#123; mMap := make(map[int]string) mMap[10] = &quot;dog&quot; mMap[100] = &quot;cat&quot; fmt.Println(mMap)&#125;// makeChan 创建没有缓存的chanfunc makeChan() &#123; mChan := make(chan int) close(mChan)&#125;程序执行结果：[dog cat tiger]map[10:dog 100:cat] new1234567891011121314151617内存置零返回传入类型的指针地址func main() &#123; NewMap()&#125;func NewMap() &#123; mNewMap := new(map[int]string) mMakeMap := make(map[int]string) fmt.Println(reflect.TypeOf(mNewMap)) fmt.Println(reflect.TypeOf(mMakeMap))&#125;程序执行结果：*map[int]stringmap[int]string append &amp; delete &amp; copy123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113slice -&gt; append &amp; copymap -&gt; delete// 向切片中添加元素并打印出切片的长度和容量func main() &#123; appendElementForSlice()&#125;func appendElementForSlice() &#123; mIDSlice := make([]string, 2) mIDSlice[0] = &quot;id-1&quot; mIDSlice[1] = &quot;id-2&quot; fmt.Println(&quot;len=&quot;,len(mIDSlice)) fmt.Println(&quot;cap=&quot;,cap(mIDSlice)) mIDSlice = append(mIDSlice, &quot;id-3&quot;) fmt.Println(mIDSlice) fmt.Println(&quot;After len=&quot;,len(mIDSlice)) fmt.Println(&quot;After cap=&quot;,cap(mIDSlice))&#125;程序执行结果：len= 2cap= 2[id-1 id-2 id-3]After len= 3After cap= 4// 拷贝切片 test 1func main() &#123; copyForSlice()&#125;func copyForSlice() &#123; mIDSliceDst := make([]string, 2) mIDSliceDst[0] = &quot;id-dst-1&quot; mIDSliceDst[1] = &quot;id-dst-2&quot; mIDSliceSrc := make([]string, 2) mIDSliceSrc[0] = &quot;id-src-1&quot; mIDSliceSrc[1] = &quot;id-src-2&quot; copy(mIDSliceDst, mIDSliceSrc) fmt.Println(mIDSliceDst)&#125;程序执行结果：[id-src-1 id-src-2]// 拷贝切片 test 2func main() &#123; copyForSlice()&#125;func copyForSlice() &#123; mIDSliceDst := make([]string, 3) mIDSliceDst[0] = &quot;id-dst-1&quot; mIDSliceDst[1] = &quot;id-dst-2&quot; mIDSliceDst[2] = &quot;id-dst-3&quot; mIDSliceSrc := make([]string, 2) mIDSliceSrc[0] = &quot;id-src-1&quot; mIDSliceSrc[1] = &quot;id-src-2&quot; copy(mIDSliceDst, mIDSliceSrc) fmt.Println(mIDSliceDst)&#125;程序执行结果：[id-src-1 id-src-2 id-dst-3]// 拷贝切片 test 3func main() &#123; copyForSlice()&#125;// 拷贝切片func copyForSlice() &#123; mIDSliceDst := make([]string, 2) mIDSliceDst[0] = &quot;id-dst-1&quot; mIDSliceDst[1] = &quot;id-dst-2&quot; mIDSliceSrc := make([]string, 3) mIDSliceSrc[0] = &quot;id-src-1&quot; mIDSliceSrc[1] = &quot;id-src-2&quot; copy(mIDSliceDst, mIDSliceSrc) fmt.Println(mIDSliceDst)&#125;程序执行结果：[id-src-1 id-src-2]说明copy操作不会为mIDSliceDst扩容// delete test 1func main() &#123; deleteFormMap()&#125;func deleteFormMap() &#123; mIDMap := make(map[int]string) mIDMap[0] = &quot;id-1&quot; mIDMap[1] = &quot;id-2&quot; delete(mIDMap, 0) fmt.Println(mIDMap)&#125;程序执行结果：map[1:id-2] panic &amp; recover12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758处理异常，panic抛出异常，recover捕获异常// panic test 1func main() &#123; receivePanic()&#125;func receivePanic() &#123; panic(&quot;I am panic&quot;)&#125;程序执行结果：panic: I am panicgoroutine 1 [running]:main.receivePanic(...) D:/SourceCode/GitHub/Golang/src/github.com/lnsyyj/gin_test_project/test/main.go:13main.main() D:/SourceCode/GitHub/Golang/src/github.com/lnsyyj/gin_test_project/test/main.go:9 +0x40// panic test 2func main() &#123; receivePanic()&#125;func receivePanic() &#123; defer func() &#123; recover() &#125;() panic(&quot;I am panic&quot;)&#125;程序执行结果：Process finished with exit code 0// panic test 3func main() &#123; receivePanic()&#125;func receivePanic() &#123; defer coverPanic() panic(&quot;I am panic&quot;)&#125;func coverPanic() &#123; message := recover() switch message.(type) &#123; case string: fmt.Println(&quot;string message : &quot;, message) case error: fmt.Println(&quot;error message : &quot;, message) default: fmt.Println(&quot;unknown panic : &quot;, message) &#125;&#125;程序执行结果：string message : I am panic len &amp; cap &amp; close123456789101112131415161718192021222324252627282930313233343536len -&gt; string、array、slice、map、chancap -&gt; slice、array、chanclose -&gt; chan// len &amp; cap testfunc main() &#123; getLen()&#125;func getLen() &#123; mIDSliceDst := make([]string, 2, 5) mIDSliceDst[0] = &quot;id-dst-1&quot; mIDSliceDst[1] = &quot;id-dst-2&quot; mIDSliceDst = append(mIDSliceDst, &quot;id-dst-3&quot;) mIDSliceDst = append(mIDSliceDst, &quot;id-dst-4&quot;) mIDSliceDst = append(mIDSliceDst, &quot;id-dst-5&quot;) mIDSliceDst = append(mIDSliceDst, &quot;id-dst-6&quot;) fmt.Println(&quot;mIDSliceDst len : &quot;, len(mIDSliceDst)) fmt.Println(&quot;mIDSliceDst cap: &quot;, cap(mIDSliceDst))&#125;程序执行结果：mIDSliceDst len : 6mIDSliceDst cap: 10// close testfunc main() &#123; closeChan()&#125;func closeChan() &#123; mChan := make(chan int, 1) defer close(mChan) mChan &lt;- 1 // 业务代码&#125; 结构体创建&amp;初始化1234567891011121314151617181920212223242526272829303132func main() &#123; TestForStruct()&#125;type Dog struct &#123; ID int Name string Age int&#125;func TestForStruct() &#123; // 方式1 var dog Dog dog.ID = 0 dog.Name = &quot;KiKi&quot; dog.Age = 3 fmt.Println(dog) // 方式2 dog_2 := Dog&#123;ID: 1, Name: &quot;Yaya&quot;, Age: 2&#125; fmt.Println(dog_2) // 方式3 dog_3 := new(Dog) dog_3.ID = 3 dog_3.Name = &quot;Tom&quot; dog_3.Age = 4 fmt.Println(dog_3)&#125;程序执行结果：&#123;0 KiKi 3&#125;&#123;1 Yaya 2&#125;&amp;&#123;3 Tom 4&#125; 属性及函数12345678910111213141516171819202122232425func main() &#123; TestForStruct()&#125;type Dog struct &#123; ID int Name string Age int&#125;func (d *Dog)Run() &#123; fmt.Println(&quot;ID : &quot;, d.ID, &quot; Dog is running&quot;)&#125;func TestForStruct() &#123; // 方式1 var dog Dog dog.ID = 0 dog.Name = &quot;KiKi&quot; dog.Age = 3 dog.Run()&#125;程序执行结果：ID : 0 Dog is running 组合12 ###]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph iSCIS gateway 监控]]></title>
    <url>%2F2019%2F11%2F27%2FCeph-iSCIS-gateway-%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[监控ISCSI GATEWAYSCeph为iSCSI gateway环境提供了一个附加工具，以监视导出的RADOS Block Device (RBD) images的性能。 gwtop工具是一个类似top的工具，用于显示通过iSCSI导出到客户端的RBD images的聚合性能指标。 这些指标来自Performance Metrics Domain Agent（PMDA）。 来自Linux-IO target (LIO) PMDA的信息用于列出每个已导出的RBD image以及所连接的客户端及其相关的I/O指标。 Requirements: 正在运行的Ceph iSCSI gateway Installing: 1、以root用户身份在每个iSCSI gateway节点上安装ceph-iscsi-tools package： 1# yum install ceph-iscsi-tools 2、以root用户身份在每个iSCSI gateway节点上安装performance co-pilot package： 1# yum install pcp 3、以root用户身份在每个iSCSI gateway节点上安装LIO PMDA package： 1# yum install pcp-pmda-lio 4、以root用户身份，在每个iSCSI gateway节点上启用并启动performance co-pilot service： 12# systemctl enable pmcd# systemctl start pmcd 5、以root用户身份注册pcp-pmda-lio agent： 12cd /var/lib/pcp/pmdas/lio./Install 默认情况下，gwtop假定iSCSI gateway configuration object存储在rbd pool中的一个名为gateway.conf的RADOS object中。 此configuration定义了收集性能统计信息的iSCSI gateways。 可以使用-g或-c标志将其覆盖。 有关更多详细信息，请参见gwtop –help。 LIO configuration决定要从performance co-pilot提取的性能统计信息的类型。 gwtop启动时，它会查看LIO configuration，如果找到user-space disks，则gwtop会自动选择LIO collector。 示例gwtop输出 1234567891011gwtop 2/2 Gateways CPU% MIN: 4 MAX: 5 Network Total In: 2M Out: 3M 10:20:00Capacity: 8G Disks: 8 IOPS: 503 Clients: 1 Ceph: HEALTH_OK OSDs: 3Pool.Image Src Size iops rMB/s wMB/s Clientiscsi.t1703 500M 0 0.00 0.00iscsi.testme1 500M 0 0.00 0.00iscsi.testme2 500M 0 0.00 0.00iscsi.testme3 500M 0 0.00 0.00iscsi.testme5 500M 0 0.00 0.00rbd.myhost_1 T 4G 504 1.95 0.00 rh460p(CON)rbd.test_2 1G 0 0.00 0.00rbd.testme 500M 0 0.00 0.00 在Client列中，(CON)表示iSCSI initiator (client)当前已登录iSCSI gateway。 如果显示-multi-，则多个clients映射到单个RBD image。]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph iSCSI gateway 安装和配置]]></title>
    <url>%2F2019%2F11%2F27%2FCeph-iSCSI-gateway-%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[使用ANSI配置ISCSI TARGETCeph iSCSI gateway是iSCSI target node，也是Ceph client node。 Ceph iSCSI gateway可以是独立节点，也可以位于Ceph Object Store Disk (OSD) node上。 完成以下步骤，将安装Ceph iSCSI gateway并将其配置为基本操作。 要求： 正在运行的Ceph Luminous（12.2.x）集群或更高版本 Red Hat Enterprise Linux/CentOS 7.5（或更高版本）； Linux内核v4.16（或更高版本） 在所有iSCSI gateway节点上安装ceph-iscsi package 安装： 在Ansible installer节点（可以是管理节点也可以是专用部署节点）上，执行以下步骤： 1、以root用户身份安装ceph-ansible package： 1# yum install ceph-ansible 2、在/etc/ansible/hosts文件中为gateway group添加一个entry： 123[iscsigws]ceph-igw-1ceph-igw-2 注意如果将iSCSI gateway与OSD部署同一节点，则将OSD节点添加到[iscsigws] section。 配置： ceph-ansible package在/usr/share/ceph-ansible/group_vars/目录中放置了一个名为iscsigws.yml.sample的文件。 创建此示例文件的副本iscsigws.yml。 查看以下Ansible变量和说明，并进行相应更新。 有关高级变量的完整列表，请参见iscsigws.yml.sample。 Variable（变量） Meaning/Purpose（含义/目的） seed_monitor 每个gateway都需要访问ceph集群以进行rados和rbd calls。 这意味着iSCSI gateway必须已定义适当的/etc/ceph/目录。 seed_monitor host用于填充iSCSI gateway的/etc/ceph/目录。 cluster_name 自定义存储集群名称（默认为ceph） gateway_keyring Define a custom keyring name. deploy_settings 如果设置为true，则在运行playbook时deploy the settings。 perform_system_checks 这是一个布尔值，用于检查每个gateway上的multipath和lvm configuration settings。 必须至少在第一次运行时将其设置为true，以确保正确配置了multipathd和lvm。 api_user API的用户名。 默认值为admin。 api_password 使用API的密码。 默认值为admin。 api_port 使用API的TCP端口号。 默认值为5000。 api_secure 如果必须使用TLS，则为True。 默认为false。 如果为true，则用户必须创建必要的certificate和key files。 有关详细信息，请参见gwcli man文件。 trusted_ip_list 有权访问API的IPv4或IPv6地址的列表。 默认情况下，只有iSCSI gateway节点可以访问。 Deploying: 在Ansible installer节点上，执行以下步骤。 1、以root用户身份执行Ansible playbook： 12# cd /usr/share/ceph-ansible# ansible-playbook site.yml --limit iscsigws 注意Ansible playbook将处理RPM dependencies，设置daemons并安装gwcli，因此可用于创建iSCSI targets并将RBD images导出为LUN。 在以前的版本中，iscsigws.yml可以定义iSCSI target和其他objects，如clients, images和LUNs, 但现在不再支持该功能。 2、从iSCSI gateway节点验证配置： 1# gwcli ls 请参阅Configuring the iSCSI Target using the Command Line Interface，使用gwcli工具创建gateways, LUNs和clients。 1重要提示尝试使用targetcli工具更改配置将导致以下问题，例如ALUA配置错误和路径故障转移问题。 可能会损坏数据，configuration across iSCSI gateways不匹配，WWN information不匹配，这将导致client multipath问题。 Service Management: ceph-iscsi package安装configuration management logic和一个名为rbd-target-api的Systemd service。启用Systemd service后，rbd-target-api将在引导时启动，并将恢复Linux IO状态。Ansible playbook会在部署期间禁用target service。以下是与rbd-target-api Systemd服务交互命令。 1# systemctl &lt;start|stop|restart|reload&gt; rbd-target-api reload reload request将强制rbd-target-api重新读取配置并将其应用于当前正在运行的环境。 通常不需要这样做，因为changes是从Ansible并行部署到所有iSCSI gateway节点的。 stop stop request将关闭gateway的portal interfaces，断开与客户端的连接，并从内核中清除当前的LIO配置。这将使iSCSI gateway返回到clean状态。 当客户端断开连接时，客户端multipathing layer会将active I/O（活动的I/O）重新安排到其他iSCSI gateways。 Removing the Configuration: ceph-ansible package提供了Ansible手册，可删除iSCSI gateway配置和相关的RBD images。 Ansible playbook是/usr/share/ceph-ansible/purge_gateways.yml。 运行此Ansible playbook时，系统会提示您执行清除的类型： lio : 在此模式下，将在已定义的所有iSCSI gateways上清除LIO配置。 在Ceph storage集群中，创建的Disks保持不变。 all : 选择all后，将删除LIO配置以及在iSCSI gateway环境中定义的所有RBD images，其他不相关的RBD images将不会删除。 123警告清除操作是对iSCSI gateway环境的破坏性操作。警告如果RBD images具有snapshots或clones并通过Ceph iSCSI gateway导出，则清除操作将失败。 1234567891011121314151617181920212223242526272829303132333435363738394041[root@rh7-iscsi-client ceph-ansible]# ansible-playbook purge_gateways.ymlWhich configuration elements should be purged? (all, lio or abort) [abort]: allPLAY [Confirm removal of the iSCSI gateway configuration] *********************GATHERING FACTS ***************************************************************ok: [localhost]TASK: [Exit playbook if user aborted the purge] *******************************skipping: [localhost]TASK: [set_fact ] *************************************************************ok: [localhost]PLAY [Removing the gateway configuration] *************************************GATHERING FACTS ***************************************************************ok: [ceph-igw-1]ok: [ceph-igw-2]TASK: [igw_purge | purging the gateway configuration] *************************changed: [ceph-igw-1]changed: [ceph-igw-2]TASK: [igw_purge | deleting configured rbd devices] ***************************changed: [ceph-igw-1]changed: [ceph-igw-2]PLAY RECAP ********************************************************************ceph-igw-1 : ok=3 changed=2 unreachable=0 failed=0ceph-igw-2 : ok=3 changed=2 unreachable=0 failed=0localhost : ok=2 changed=0 unreachable=0 failed=0 使用COMMAND LINE INTERFACE配置ISCSI TARGETCeph iSCSI gateway是iSCSI target节点，也是Ceph client节点。 Ceph iSCSI gateway可以是独立节点，也可以位于Ceph Object Store Disk (OSD)节点上。 完成以下步骤，将安装Ceph iSCSI gateway并将其配置为基本操作。 Requirements: 正在运行的Ceph Luminous（12.2.x）集群或更高版本 Red Hat Enterprise Linux/CentOS 7.5（或更高版本）； Linux内核v4.16（或更高版本） 必须从Linux发行版的软件repository中安装以下软件包： targetcli-2.1.fb47 or newer package python-rtslib-2.1.fb68 or newer package tcmu-runner-1.4.0 or newer package ceph-iscsi-3.2 or newer package 1重要说明：如果存在这些packages的先前版本，则必须在安装较新版本之前首先将其删除。 在继续Installing section之前，请在Ceph iSCSI gateway节点上执行以下步骤： 1、如果Ceph iSCSI gateway未在OSD节点上，则将/etc/ceph/中的Ceph配置文件复制到iSCSI gateway 节点。 Ceph配置文件必须存在于/etc/ceph/下的iSCSI gateway节点上。 2、安装和配置Ceph Command-line Interface 3、请在防火墙上打开TCP端口3260和5000。 1注意对端口5000的访问应仅限于受信任的内部网络或仅使用gwcli或正在运行ceph-mgr daemons的单个主机。 4、创建一个新的或使用现有的RADOS Block Device (RBD)。 Installing: 如果您使用upstream ceph-iscsi package，请遵循 manual install instructions。 对于基于rpm的指令，请执行以下命令： 1、以root用户身份，在所有iSCSI gateway节点上，安装ceph-iscsi package： 1# yum install ceph-iscsi 2、以root用户身份，在所有iSCSI gateway节点上，安装tcmu-runner package： 1# yum install tcmu-runner Setup: 1、gwcli需要一个名称为rbd的pool，因此它可以存储iSCSI配置之类的元数据。 要检查是否已创建此pool，请运行： 1# ceph osd lspools 如果不存在，则可以在RADOS pool operations page上找到创建pool的说明。 2、在iSCSI gateway节点上，以root身份在/etc/ceph/目录中创建一个名为iscsi-gateway.cfg的文件： 1# touch /etc/ceph/iscsi-gateway.cfg 2.1、编辑iscsi-gateway.cfg文件并添加以下行： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[config]# Name of the Ceph storage cluster. A suitable Ceph configuration file allowing# access to the Ceph storage cluster from the gateway node is required, if not# colocated on an OSD node.cluster_name = ceph# Place a copy of the ceph cluster&apos;s admin keyring in the gateway&apos;s /etc/ceph# drectory and reference the filename heregateway_keyring = ceph.client.admin.keyring# API settings.# The API supports a number of options that allow you to tailor it to your# local environment. If you want to run the API under https, you will need to# create cert/key files that are compatible for each iSCSI gateway node, that is# not locked to a specific node. SSL cert and key files *must* be called# &apos;iscsi-gateway.crt&apos; and &apos;iscsi-gateway.key&apos; and placed in the &apos;/etc/ceph/&apos; directory# on *each* gateway node. With the SSL files in place, you can use &apos;api_secure = true&apos;# to switch to https mode.# To support the API, the bear minimum settings are:api_secure = false# Additional API configuration options are as follows, defaults shown.# api_user = admin# api_password = admin# api_port = 5001# trusted_ip_list = 192.168.0.10,192.168.0.11# ------------------------------------------------------# 翻译如下[config]＃Ceph存储集群的名称。如果不位于OSD节点上，则需要一个合适的Ceph配置文件，该文件允许从gateway节点访问Ceph存储群集。cluster_name = ceph＃将ceph集群的admin keyring的副本放置在gateway的/etc/ceph文件夹中，并在此处引用filenamegateway_keyring = ceph.client.admin.keyring＃API设置。＃API支持许多选项，可让您根据本地环境进行定制。如果要在https下运行API，则需要为每个iSCSI gateway节点创建兼容的cert/key文件，该节点未锁定到特定节点。必须将SSL cert和key文件命名为&apos;iscsi-gateway.crt&apos;和&apos;iscsi-gateway.key&apos;，并放置在每个gateway节点上的&apos;/etc/ceph/&apos;目录中。放置好SSL文件后，您可以使用&apos;api_secure = true&apos;切换到https模式。＃为了支持API，至少需要配置如下：api_secure = false＃其他API配置选项如下，显示的默认值。（trusted翻译为信任）# api_user = admin# api_password = admin# api_port = 5001# trusted_ip_list = 192.168.0.10,192.168.0.11 注意trusted_ip_list是每个iscsi gateway上IP地址的列表，将用于管理操作，如创建target，lun导出等。该IP可以与用于iSCSI data的IP相同，例如与RBD image之间的READ/WRITE命令，但建议使用单独的IP。 1重要说明：在所有iSCSI gateway节点上，iscsi-gateway.cfg文件必须相同。 2.2、以root用户身份将iscsi-gateway.cfg文件复制到所有iSCSI gateway节点。 2.3、以root用户身份，在所有iSCSI gateway节点上，启用并启动API服务： 123# systemctl daemon-reload# systemctl enable rbd-target-api# systemctl start rbd-target-api Configuring: gwcli将创建和配置iSCSI target和RBD images，并在上一部分中的gateways设置之间复制配置。 较低级别的工具（例如targetcli和rbd）可用于查询本地配置，但不应用于对其进行修改。 下一节将演示如何创建iSCSI target并将RBD image导出为LUN 0。 1、以root用户身份，在iSCSI gateway节点上，启动iSCSI gateway command-line interface： 1# gwcli 2、转到iscsi-targets并创建名为iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw的目标： 12&gt; /&gt; cd /iscsi-target&gt; /iscsi-target&gt; create iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw 3、创建iSCSI gateways。 下面使用的IP是用于iSCSI data（如READ和WRITE命令）的IP。 它们可以是trusted_ip_list中列出的用于管理操作相同的IP，但是建议使用不同的IP。 123&gt; /iscsi-target&gt; cd iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw/gateways&gt; /iscsi-target...-igw/gateways&gt; create ceph-gw-1 10.172.19.21&gt; /iscsi-target...-igw/gateways&gt; create ceph-gw-2 10.172.19.22 如果不使用RHEL/CentOS或使用upstream或ceph-iscsi-test kernel，则必须使用skipchecks=true参数。 这将避免Red Hat kernel和rpm checks： 123&gt; /iscsi-target&gt; cd iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw/gateways&gt; /iscsi-target...-igw/gateways&gt; create ceph-gw-1 10.172.19.21 skipchecks=true&gt; /iscsi-target...-igw/gateways&gt; create ceph-gw-2 10.172.19.22 skipchecks=true 4、在rbd pool中添加名称为disk_1的RBD image： 12&gt; /iscsi-target...-igw/gateways&gt; cd /disks&gt; /disks&gt; create pool=rbd image=disk_1 size=90G 5、创建一个客户端，使用initiator名称iqn.1994-05.com.redhat:rh7-client： 12&gt; /disks&gt; cd /iscsi-target/iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw/hosts&gt; /iscsi-target...eph-igw/hosts&gt; create iqn.1994-05.com.redhat:rh7-client 6、将客户端的CHAP用户名设置为myiscsiusername，将密码设置为myiscsipassword： 1&gt; /iscsi-target...at:rh7-client&gt; auth username=myiscsiusername password=myiscsipassword 警告必须始终配置CHAP。 如果没有CHAP，target将拒绝任何登录请求。 7、将disk添加到客户端： 1&gt; /iscsi-target...at:rh7-client&gt; disk add rbd/disk_1 下一步是配置iSCSI initiators。 手动安装CEPH-ISCSIRequirements 要完成ceph-iscsi的安装，有4个步骤： 1、从Linux发行版的软件repository安装common packages 2、安装Git以直接从其Git repositories中获取其余packages 3、确保使用兼容的kernel 4、安装ceph-iscsi的所有组件并启动相关的daemons： tcmu-runner rtslib-fb configshell-fb targetcli-fb ceph-iscsi 1、安装COMMON PACKAGESceph-iscsi和target工具将使用以下packages。 必须从Linux发行版的软件repository中将它们安装在将成为iSCSI gateway的每台计算机上： libnl3 libkmod librbd1 pyparsing python kmod python pyudev python gobject python urwid python pyparsing python rados python rbd python netifaces python crypto python requests python flask pyOpenSSL 2、安装GIT为了安装使用Ceph运行iSCSI所需的所有packages，您需要使用Git直接从其repository中下载它们。 在CentOS/RHEL上执行： 1&gt; sudo yum install git 在Debian/Ubuntu上执行： 1&gt; sudo apt install git 要了解有关Git及其工作方式的更多信息，请访问https://git-scm.com 3、确保使用了兼容的KERNEL确保使用支持Ceph iSCSI patches的kernel： 具有内核v4.16或更高版本的所有Linux发行版 Red Hat Enterprise Linux或CentOS 7.5更高版本 如果您已经在使用兼容的kernel，则可以转到下一步。 但是，如果您不使用兼容的kernel，请查看发行版的文档以获取有关如何构建此kernel的特定说明。 唯一的Ceph iSCSI特定要求是必须启用以下构建选项： 123CONFIG_TARGET_CORE=mCONFIG_TCM_USER2=mCONFIG_ISCSI_TARGET=m 4、安装CEPH-ISCSI最后，可以直接从其Git repositories中获取其余工具，并启动其相关服务 TCMU-RUNNER Installation: 12&gt; git clone https://github.com/open-iscsi/tcmu-runner&gt; cd tcmu-runner 运行以下命令以安装所有必需的依赖项： 1&gt; ./extra/install_dep.sh 现在，您可以构建tcmu-runner。 为此，请使用以下构建命令： 12&gt; cmake -Dwith-glfs=false -Dwith-qcow=false -DSUPPORT_SYSTEMD=ON -DCMAKE_INSTALL_PREFIX=/usr&gt; make install 启用并启动守护程序： 123&gt; systemctl daemon-reload&gt; systemctl enable tcmu-runner&gt; systemctl start tcmu-runner RTSLIB-FB Installation: 123&gt; git clone https://github.com/open-iscsi/rtslib-fb.git&gt; cd rtslib-fb&gt; python setup.py install CONFIGSHELL-FB Installation: 123&gt; git clone https://github.com/open-iscsi/configshell-fb.git&gt; cd configshell-fb&gt; python setup.py install TARGETCLI-FB Installation: 12345&gt; git clone https://github.com/open-iscsi/targetcli-fb.git&gt; cd targetcli-fb&gt; python setup.py install&gt; mkdir /etc/target&gt; mkdir /var/target 警告ceph-iscsi工具假定它们正在管理系统上的所有targets。 如果已设置targets并由targetcli管理，则必须禁用targets服务。 CEPH-ISCSIInstallation: 12345&gt; git clone https://github.com/ceph/ceph-iscsi.git&gt; cd ceph-iscsi&gt; python setup.py install --install-scripts=/usr/bin&gt; cp usr/lib/systemd/system/rbd-target-gw.service /lib/systemd/system&gt; cp usr/lib/systemd/system/rbd-target-api.service /lib/systemd/system 启用并启动daemon： 12345&gt; systemctl daemon-reload&gt; systemctl enable rbd-target-gw&gt; systemctl start rbd-target-gw&gt; systemctl enable rbd-target-api&gt; systemctl start rbd-target-api 安装完成。 进入main ceph-iscsi CLI page上的setup部分。]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 常用操作]]></title>
    <url>%2F2019%2F11%2F22%2FUbuntu-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Ubuntu版本 123456789101112NAME=&quot;Ubuntu&quot;VERSION=&quot;18.04.3 LTS (Bionic Beaver)&quot;ID=ubuntuID_LIKE=debianPRETTY_NAME=&quot;Ubuntu 18.04.3 LTS&quot;VERSION_ID=&quot;18.04&quot;HOME_URL=&quot;https://www.ubuntu.com/&quot;SUPPORT_URL=&quot;https://help.ubuntu.com/&quot;BUG_REPORT_URL=&quot;https://bugs.launchpad.net/ubuntu/&quot;PRIVACY_POLICY_URL=&quot;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&quot;VERSION_CODENAME=bionicUBUNTU_CODENAME=bionic 常用命令1、add-apt-repository add-apt-repository是用于添加apt source.list条目的脚本。它可用于添加任何repository，还提供用于添加Launchpad PPA repository的简写语法（Personal Package Archive，个人软件包存档）。 12345678sudo add-apt-repository ppa:g2p/storagesudo apt-get updatesudo apt-get install bcache-toolssudo apt-get install -y software-properties-commonsudo add-apt-repository ppa:ansible/ansible-2.8sudo apt-get updatesudo apt-get -y install ansible 2、vim 粘贴串行问题 12345在粘贴前设置（粘贴前不会自动缩进）set paste在粘贴后恢复set nopaste 3、安装package常用操作 12345sudo apt clean #清空缓存sudo apt install -d software_name #只下载不安装，缓存位置 /var/cache/apt/archivessudo dpkg -i *.debapt-get install -f 修复损坏的软件包，尝试卸载出错的包，重新安装正确版本 设置网络123456789101112131415root@yujiang-ceph-1:~# cat /etc/netplan/50-cloud-init.yaml # This file is generated from information provided by# the datasource. Changes to it will not persist across an instance.# To disable cloud-init&apos;s network configuration capabilities, write a file# /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following:# network: &#123;config: disabled&#125;network: ethernets: ens160: dhcp4: false addresses: [192.168.1.127/24] gateway4: 192.168.1.1 nameservers: addresses: [192.168.1.1, 114.114.114.114] version: 2 编译deb12345678910111213解压xz文件xz -d prometheus_2.1.0+ds-1.debian.tar.xztar -xvf prometheus_2.1.0+ds-1.debian.tar压缩xz文件tar cvf prometheus_2.1.0+ds-1.debian.tar debian/xz -z prometheus_2.1.0+ds-1.debian.tarapt-get install debhelper dh-golang golang-github-aws-aws-sdk-go-dev golang-github-azure-azure-sdk-for-go-dev golang-github-azure-go-autorest-dev golang-github-cespare-xxhash-dev golang-github-cockroachdb-cmux-dev golang-github-fsnotify-fsnotify-dev golang-github-go-kit-kit-dev golang-github-gogo-protobuf-dev golang-github-golang-snappy-dev golang-github-gophercloud-gophercloud-dev golang-github-grpc-ecosystem-grpc-gateway-dev golang-github-hashicorp-go-cleanhttp-dev golang-github-hashicorp-serf-dev golang-github-miekg-dns-dev golang-github-mwitkow-go-conntrack-dev golang-github-opentracing-contrib-go-stdlib-dev golang-github-opentracing-opentracing-go-dev golang-github-pkg-errors-dev golang-github-prometheus-client-golang-dev golang-github-prometheus-client-model-dev golang-github-prometheus-common-dev golang-github-prometheus-tsdb-dev golang-github-samuel-go-zookeeper-dev golang-go golang-golang-x-net-dev golang-golang-x-oauth2-google-dev golang-golang-x-time-dev golang-google-api-dev golang-google-genproto-dev golang-google-grpc-dev golang-gopkg-alecthomas-kingpin.v2-dev golang-gopkg-yaml.v2-dev dpkg-buildpackage -uc -usdpkg-buildpackage -rfakeroot -Tclean]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph 数据recovery流量控制]]></title>
    <url>%2F2019%2F11%2F16%2FCeph-%E6%95%B0%E6%8D%AErecovery%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[ceph在扩容或缩容期间会有数据rebalance。如何控制在rebalance时，尽量降低对client IO的影响？调研如下： 首先，在什么情况下ceph会出现数据rebalance？本质上，用户数据写入ceph时，会被切分成大小相等的object，这些object由PG承载，分布到不同的OSD上（每个OSD一般会对应一块硬盘）。数据的迁移会以PG为单位进行，所以当PG发生变化时，就会有数据rebalance。 那么在什么时候PG会变化呢？从用户使用角度讲一般有如下几种场景： 添加/删除OSD 重新调整pool的PG数 Client IO优先降低recovery的I/O优先级 1234567调整后[root@ceph ~]# ceph daemon osd.2 config show | grep osd_recovery_op_priority &quot;osd_recovery_op_priority&quot;: &quot;3&quot;,使用默认值[root@ceph ~]# ceph daemon osd.2 config show | grep osd_client_op_priority &quot;osd_client_op_priority&quot;: &quot;63&quot;, 降低recovery的I/O带宽及backfill带宽 1234567调整后[root@ceph ~]# ceph daemon osd.2 config show | grep osd_recovery_max_active &quot;osd_recovery_max_active&quot;: &quot;1&quot;,调整后[root@ceph ~]# ceph daemon osd.2 config show | grep osd_recovery_sleep &quot;osd_recovery_sleep&quot;: &quot;0.200000&quot;, 调整相关命令 12345678910111213[root@ceph ~]# ceph daemon osd.2 config set osd_recovery_op_priority 3&#123; &quot;success&quot;: &quot;osd_recovery_op_priority = &apos;3&apos; (not observed, change may require restart) &quot;&#125;[root@ceph ~]# ceph daemon osd.2 config show | grep osd_recovery_op_priority &quot;osd_recovery_op_priority&quot;: &quot;3&quot;,获取osd ID[root@ceph ~]# ll /var/run/ceph/ | grep osd | awk &apos;&#123;print $9&#125;&apos; | sed &apos;s/ceph-\(.*\).asok/\1/&apos;osd.13osd.2osd.5osd.8 添加OSD时1、BACK FILLING 当新的OSD加入群集时，CRUSH将把placement groups从群集中的OSD重新分配给新添加的OSD。强制新OSD立即接受重新分配的placement groups会给新OSD带来过多的负担。用placement groups backfilling OSD在后台运行。backfilling完成后，新的OSD将在准备就绪后开始处理请求。 在backfilling操作期间，您可能会看到以下几种状态之一： backfill_wait：表示backfilling操作尚未完成，但尚未进行 backfilling：表示正在进行backfilling操作 backfill_toofull：表示已请求backfill操作，但由于存储容量不足而无法完成 如果无法重新backfilled placement group，则可以认为该placement group considered incomplete（不完整） backfill_toofull状态可能是瞬态的。随着PG的移动，空间可能变得可用。 backfill_toofull与backfill_wait类似，因为一旦条件发生变化，backfill就可以继续进行。 Ceph提供了许多设置来管理将placement groups重新分配给OSD（尤其是新OSD）相关的负载峰值。默认情况下，osd_max_backfill设置OSD之间最大并发backfill数为1。backfill full ratio可以使OSD拒绝接受backfill请求（默认为90％），使用ceph osd set-backfillfull-ratio命令进行更改。如果OSD拒绝backfill请求，则osd backfill retry interval使OSD可以重试该请求（默认为30秒后）。OSD还可以设置osd backfill scan min和osd backfill scan max以管理扫描间隔（默认为64和512）。 1234Option(&quot;osd_max_backfills&quot;, Option::TYPE_UINT, Option::LEVEL_ADVANCED).set_default(1).set_description(&quot;Maximum number of concurrent local and remote backfills or recoveries per OSD &quot;).set_long_description(&quot;There can be osd_max_backfills local reservations AND the same remote reservations per OSD. So a value of 1 lets this OSD participate as 1 PG primary in recovery and 1 shard of another recovering PG.&quot;), 参考资料【1】https://blog.csdn.net/Linux_kiss/article/details/82857117 【2】http://www.zphj1987.com/2017/08/10/Ceph-recover-speed-control/]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QEMU AND BLOCK DEVICES]]></title>
    <url>%2F2019%2F11%2F15%2FQEMU-AND-BLOCK-DEVICES%2F</url>
    <content type="text"><![CDATA[最常见的Ceph Block Device用例是向虚拟机提供block device images。例如，用户可以创建”golden” image。 然后，对image做snapshot。最后，用户clone snapshot（通常多次）。 有关详细信息，请参见 Snapshots。snapshot的clone具有copy-on-write能力，意味着Ceph可以快速将block device images配置给虚拟机，因为客户端不必在每次启动新虚拟机时都下载整个映像。 Ceph Block Devices可以与QEMU虚拟机集成。 有关QEMU的详细信息，请参阅QEMU Open Source Processor Emulator。 有关QEMU文档，请参见QEMU Manual。 有关安装的详细信息，请参见 Installation。 1重要说明要将Ceph Block Devices与QEMU一起使用，你必须能够访问正在运行的Ceph的集群。 USAGE（用法）QEMU命令行要求您指定pool名称和image名称。 您也可以指定snapshot名称。 QEMU将假设Ceph configuration文件位于默认位置（例如，/etc/ceph/$cluster.conf），并且您以默认的client.admin user身份执行命令，除非您明确指定另一个Ceph configuration文件路径或另一个user。指定user时，QEMU使用ID而不是完整的TYPE:ID。有关详细信息，请参见User Management - User。请勿在user ID的开头添加客户端类型（即，client），否则您将收到验证错误。您应该具有admin user的key，或者使用:id={user}选项指定的另一个user的key，存储在默认路径中的keyring文件中。（即，/etc/ceph或具有适当文件所有权和权限的本地目录）用法如下： 1qemu-img &#123;command&#125; [options] rbd:&#123;pool-name&#125;/&#123;image-name&#125;[@snapshot-name][:option1=value1][:option2=value2...] 例如，指定id和conf选项如下所示： 1qemu-img &#123;command&#125; [options] rbd:glance-pool/maipo:id=glance:conf=/etc/ceph/ceph.conf 提示：包含:, @, 或 =可以使用\开头的字符进行转义。 CREATING IMAGES WITH QEMU（使用QEMU创建IMAGES）可以从QEMU创建block device image。 必须指定rbd，pool名称和要创建的image名称。 还必须指定image的size。 1qemu-img create -f raw rbd:&#123;pool-name&#125;/&#123;image-name&#125; &#123;size&#125; 例如： 1qemu-img create -f raw rbd:data/foo 10G 重要说明：raw data format（原始数据格式）实际上是与RBD一起使用的唯一明智格式选项。从技术上讲，您可以使用其他QEMU支持的格式（例如qcow2或vmdk），但是这样做会增加额外的开销，并且在启用缓存（请参阅下文）时，对于虚拟机实时迁移也将使该卷不安全。 RESIZING IMAGES WITH QEMU（使用QEMU调整IMAGES大小）您可以从QEMU调整block device image的大小。 您必须指定rbd，pool名称以及要调整大小的image名称。 您还必须指定image的size。 1qemu-img resize rbd:&#123;pool-name&#125;/&#123;image-name&#125; &#123;size&#125; 例如： 1qemu-img resize rbd:data/foo 10G RETRIEVING IMAGE INFO WITH QEMU（使用QEMU检索IMAGE信息）您可以从QEMU中检索block device image信息。 您必须指定rbd，pool名称和image名称。 1qemu-img info rbd:&#123;pool-name&#125;/&#123;image-name&#125; 例如： 1qemu-img info rbd:data/foo RUNNING QEMU WITH RBD（运行QEMU与RBD）QEMU可以将block device从host传递到guest，但是从QEMU 0.15开始，无需将image映射为host上的block device。相反，QEMU可以直接通过librbd作为virtual block device访问image。这样会更好，因为它避免了额外的context（上下文）切换，并且可以利用RBD caching。 您可以使用qemu-img将现有的virtual machine images转换为Ceph block device images。例如，如果您有一个qcow2 image，则可以运行： 1qemu-img convert -f qcow2 -O raw debian_squeeze.qcow2 rbd:data/squeeze 要运行从该image启动的virtual machine，可以运行： 1qemu -m 1024 -drive format=raw,file=rbd:data/squeeze RBD caching可以明显提高性能。 从QEMU 1.2开始，QEMU的cache选项控制librbd caching： 1qemu -m 1024 -drive format=rbd,file=rbd:data/squeeze,cache=writeback 如果您使用的是较旧版本的QEMU，则可以将librbd cache configuration设置为”file”参数的一部分（如任何Ceph配置选项一样）： 1qemu -m 1024 -drive format=raw,file=rbd:data/squeeze:rbd_cache=true,cache=writeback 重要说明：如果设置rbd_cache=true，则必须设置cache=writeback。如果不使用cache=writeback，则QEMU不会将刷新请求发送到librbd，如果QEMU在此配置中异常退出，则 rbd 顶部的文件系统可能会损坏。 ENABLING DISCARD/TRIM（启用DISCARD/TRIM）从Ceph 0.46版和QEMU 1.1版开始，Ceph Block Devices支持discard（丢弃）操作。这意味着guest可以发送TRIM请求，让Ceph block device回收未使用的空间。在客户机中mount ext4或XFS时，通过discard选项启用它。 为了使此功能可供guest使用，必须为block device显式启用它。 为此，必须指定与drive关联的dispatch_granularity： 12qemu -m 1024 -drive format=raw,file=rbd:data/squeeze,id=drive1,if=none \ -device driver=ide-hd,drive=drive1,discard_granularity=512 请注意，这使用了IDE driver。 virtio driver不支持discard。 如果使用libvirt，请使用virsh edit编辑domain的配置文件，以包含xmlns:qemu值。然后，添加qemu:commandline block作为该domain的子级。以下示例为如何将qemu id=设置到两个不同的devices，并且discard_granularity值不同。 12345678&lt;domain type=&apos;kvm&apos; xmlns:qemu=&apos;http://libvirt.org/schemas/domain/qemu/1.0&apos;&gt; &lt;qemu:commandline&gt; &lt;qemu:arg value=&apos;-set&apos;/&gt; &lt;qemu:arg value=&apos;block.scsi0-0-0.discard_granularity=4096&apos;/&gt; &lt;qemu:arg value=&apos;-set&apos;/&gt; &lt;qemu:arg value=&apos;block.scsi0-0-1.discard_granularity=65536&apos;/&gt; &lt;/qemu:commandline&gt;&lt;/domain&gt; QEMU CACHE OPTIONS（QEMU缓存选项）QEMU的cache选项与以下Ceph RBD Cache设置相对应。 Writeback: 1rbd_cache = true Writethrough: 12rbd_cache = truerbd_cache_max_dirty = 0 None: 1rbd_cache = false QEMU的cache设置会覆盖Ceph的cache设置（包括在Ceph配置文件中设置）。 实验1234567891011121314151617181920212223242526272829[root@ceph ~]# qemu-img create -f raw rbd:rbd/yujiang 10GFormatting &apos;rbd:rbd/yujiang&apos;, fmt=raw size=10737418240 cluster_size=0 [root@ceph ~]# rbd info yujiangrbd image &apos;yujiang&apos;: size 10GiB in 2560 objects order 22 (4MiB objects) block_name_prefix: rbd_data.25dbd6b8b4567 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten flags: create_timestamp: Fri Nov 15 15:15:20 2019[root@ceph ~]# qemu-img resize rbd:rbd/yujiang 20GImage resized.[root@ceph ~]# rbd info yujiangrbd image &apos;yujiang&apos;: size 20GiB in 5120 objects order 22 (4MiB objects) block_name_prefix: rbd_data.25dbd6b8b4567 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten flags: create_timestamp: Fri Nov 15 15:15:20 2019[root@ceph ~]# qemu-img info rbd:rbd/yujiangimage: rbd:rbd/yujiangfile format: rawvirtual size: 20G (21474836480 bytes)disk size: unavailable]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NIC bonding in CentOS 7]]></title>
    <url>%2F2019%2F11%2F13%2FNIC-bonding-in-CentOS-7%2F</url>
    <content type="text"></content>
      <tags>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph osd blacklist]]></title>
    <url>%2F2019%2F11%2F13%2Fceph-osd-blacklist%2F</url>
    <content type="text"><![CDATA[什么是OSD blacklist，如何处理？环境Red Hat Ceph Storage 问题我正在运行ceph osd dump命令，它确实列出了blacklist items： 123# ceph osd dump[...]blacklist 10.37.192.139:0/1308721908 expires 2019-02-27 10:10:52.049084 这是什么意思，我该如何解决？ 决议尽管有一些控制命令可删除blacklist entries（例如ceph osd blacklist rm ADDRESS[:source_port]），但blacklists通常会自动维护，无需手动干预。因此，您无需采取任何措施。如有疑问，请联系Red Hat支持。 根本原因blacklist最常用于CephFS场景中，以防止滞后的元数据服务器对OSD上的数据进行不良更改。 诊断步骤 123# ceph osd blacklist lslisted 1 entries10.37.192.139:0/1308721908 2019-02-27 10:10:52.049084 该解决方案是Red Hat快速发布计划的一部分，提供了Red Hat工程师在为客户提供支持时创建的庞大解决方案库。 为了使您立即获得所需的知识，这些文章可能以未经编辑的原始形式出现。 CEPH FILESYSTEM CLIENT EVICTION（CEPH文件系统客户端驱逐）当文件系统客户端无响应或行为异常时，可能有必要强制终止其对文件系统的访问。 此过程称为eviction（驱逐）。 驱逐CephFS客户端会阻止其与MDS daemons和OSD daemons进一步通信。 如果客户端正在对文件系统进行buffered IO，则所有未刷新的数据都将丢失。 客户端可以自动退出（如果无法及时与MDS通信），也可以手动退出（由系统管理员）。 客户端驱逐过程适用于各种客户端，包括FUSE mounts，kernel mounts，nfs-ganesha gateways以及任何使用libcephfs的进程。 AUTOMATIC CLIENT EVICTION（自动客户端逐出）在三种情况下，可能会自动将客户驱逐： 在active MDS daemon上，如果客户端在session_autoclose（文件系统变量）秒（默认为300秒）以上未与MDS通信，则它将自动被驱逐。 在active MDS daemon上，如果客户端在mds_cap_revoke_eviction_timeout（配置选项）秒内未响应cap revoke messages。 默认情况下禁用。 在MDS启动期间（包括故障转移时），MDS称为reconnect的状态。 在此状态期间，它将等待所有客户端连接到新的MDS daemon。 如果客户端未在时间窗口内这样做（mds_reconnect_timeout，默认为45秒），则将其驱逐。 如果出现以上任何一种情况，warning message将发送到cluster log。 MANUAL CLIENT EVICTION（手动客户端驱逐）有时，管理员可能希望手动驱逐客户端。 如果客户端死亡，并且管理员不想等待其session超时；或者，如果客户端行为异常并且管理员无权访问客户端节点来卸载它。 首先检查客户列表： 1234567891011121314151617181920212223ceph tell mds.0 client ls[ &#123; &quot;id&quot;: 4305, &quot;num_leases&quot;: 0, &quot;num_caps&quot;: 3, &quot;state&quot;: &quot;open&quot;, &quot;replay_requests&quot;: 0, &quot;completed_requests&quot;: 0, &quot;reconnecting&quot;: false, &quot;inst&quot;: &quot;client.4305 172.21.9.34:0/422650892&quot;, &quot;client_metadata&quot;: &#123; &quot;ceph_sha1&quot;: &quot;ae81e49d369875ac8b569ff3e3c456a31b8f3af5&quot;, &quot;ceph_version&quot;: &quot;ceph version 12.0.0-1934-gae81e49 (ae81e49d369875ac8b569ff3e3c456a31b8f3af5)&quot;, &quot;entity_id&quot;: &quot;0&quot;, &quot;hostname&quot;: &quot;senta04&quot;, &quot;mount_point&quot;: &quot;/tmp/tmpcMpF1b/mnt.0&quot;, &quot;pid&quot;: &quot;29377&quot;, &quot;root&quot;: &quot;/&quot; &#125; &#125;] 一旦识别出要逐出的客户机，就可以使用其唯一ID或各种其他属性来识别它： 123# These all workceph tell mds.0 client evict id=4305ceph tell mds.0 client evict client_metadata.=4305 ADVANCED: UN-BLACKLISTING A CLIENT（进阶：取消blacklist客户）通常，列入blacklist的客户端可能无法重新连接到服务器：必须先将其unmount，然后再重新mount。 但是，在某些情况下，允许被驱逐的客户端尝试重新连接可能会很有用。 由于CephFS使用RADOS OSD blacklist控制客户端驱逐，因此可以通过从blacklist中删除CephFS客户端来重新连接它们： 123456$ ceph osd blacklist lslisted 1 entries127.0.0.1:0/3710147553 2018-03-19 11:32:24.716146$ ceph osd blacklist rm 127.0.0.1:0/3710147553un-blacklisting 127.0.0.1:0/3710147553 如果其他客户端访问了列入blacklist的客户端正在buffered IO的文件，则这样做可能会使数据完整性受到威胁。 也不能保证产生一个功能完备的客户端 — 在驱逐后恢复完全健康的客户端的最佳方法是unmount客户端并重新mount。 如果您尝试以这种方式重新连接客户端，则在FUSE客户端中将client_reconnect_stale设置为true，以提示客户端尝试重新连接。 ADVANCED: CONFIGURING BLACKLISTING（进阶：配置blacklist）如果由于客户端主机速度慢或网络不可靠而频繁驱逐客户端，并且您无法解决根本问题，那么您可能希望要求MDS的严格性降低。 可以通过放弃其MDS sessions来响应慢速客户端，但允许他们重新打开sessions并允许他们继续与OSD对话。 要启用此模式，请在MDS节点上将mds_session_blacklist_on_timeout设置为false。 对于手动驱逐的等效行为，请将mds_session_blacklist_on_evict设置为false。 请注意，如果禁用了blacklist，则驱逐客户端只会对您发送命令的MDS产生影响。 在具有multiple active MDS daemons的系统上，您需要向每个active daemon发送驱逐命令。 启用blacklist（默认设置）后，仅将驱逐命令发送到单个MDS就足够了，因为blacklist会将其传播到其他MDS。 BACKGROUND: BLACKLISTING AND OSD EPOCH BARRIER（背景：blacklist和OSD epoch barrier）在将客户端列入blacklist之后，有必要确保其他客户端和MDS daemons在尝试访问被列入blacklist的客户端可能已访问的任何数据对象之前，具有最新的OSDMap（包括blacklist entry）。 使用内部的”osdmap epoch barrier”机制可以确保这一点。 barrier的目的是确保当我们分发任何允许touching相同 RADOS objects的功能时，分发的客户端必须具有最新的 OSD map，不与已cancel的操作（来自 ENOSPC）或blacklist客户端（逐出）进行竞争。 更具体地说，设置epoch barrier的情况是： Client eviction — 客户端驱逐（客户端被列入blacklist，其他客户端必须等待post-blacklist epoch后才能touch相同的objects）。 客户端中的OSD map full flag handling（客户端可以从pre-full epoch取消某些OSD操作，因此其他客户端必须等到full epoch或更晚才能touching相同的objects）。 MDS启动，因为我们不持续维护barrier epoch，因此，必须假定重新启动后始终需要最新的OSD map。 请注意，这是简单的global value。 我们可以在每个inode的基础上进行维护。 但是我们没有，因为： 它将更加复杂。 每个inode将使用额外的4个字节的内存。 因为几乎每个人都拥有最新的OSD map，所以效率不会更高。 而且，在大多数情况下，每个人都会轻而易举地克服这一barrier，而不是waiting。 在极少数情况下遇到barrier，因此很少会看到每个inode粒度带来好处。 epoch barrier与所有capability messages一起发送，并指示message的接收者避免在看到OSD epoch之前向OSD发送更多的RADOS操作。 这主要适用于客户端（将其数据直接写到文件中），但也适用于MDS，因为诸如文件大小probing和文件删除之类的操作是直接从MDS完成的。 blacklist相关命令1、从blacklist中添加（可选项，直到&lt;expire&gt;秒后）或删除&lt;addr&gt;，默认3600秒1osd blacklist add|rm &lt;EntityAddr&gt; &#123;&lt;float[0.0-]&gt;&#125; add (optionally until &lt;expire&gt; seconds from now) or remove &lt;addr&gt; from blacklist 实验1，添加删除blacklist测试 1234567891011121314[root@ceph3 ~]# ceph osd blacklist add 10.20.10.28blacklisting 10.20.10.28:0/0 until 2019-11-13 12:55:53.700776 (3600 sec)[root@ceph3 ~]# ceph osd blacklist add 10.20.10.13 6000blacklisting 10.20.10.13:0/0 until 2019-11-13 13:36:16.575894 (6000 sec)[root@ceph3 ~]# ceph osd blacklist lslisted 2 entries10.20.10.13:0/0 2019-11-13 13:36:16.57589410.20.10.28:0/0 2019-11-13 12:55:53.700776[root@ceph3 ~]# ceph osd blacklist lslisted 1 entries10.20.10.28:0/0 2019-11-13 10:23:00.029669 实验2，当client在blacklist中时，在client端尝试mount cephfs（ceph-client 10.20.10.2） 12345678910111213141516171819202122231、将ceph-client加入blacklist[root@ceph1 ~]# ceph osd blacklist add 10.20.10.2[root@ceph1 ~]# ceph osd blacklist lslisted 1 entries10.20.10.2:0/0 2019-11-13 15:42:54.2603582、ceph-client尝试mount cephfs[root@ceph-client ~]# ceph-fuse /root/ceph-fuse/ --verboseceph-fuse[1664]: starting ceph client2019-11-13 14:45:18.902688 7f8f3b0db0c0 -1 init, newargv = 0x55c15933c000 newargc=10ceph-fuse[1664]: ceph mount failed with (1) Operation not permitted3、将ceph-client从blacklist中删除[root@ceph1 ~]# ceph osd blacklist rm 10.20.10.2un-blacklisting 10.20.10.2:0/0[root@ceph-client ~]# ceph-fuse /root/ceph-fuse/ ceph-fuse[1704]: starting ceph client2019-11-13 14:49:16.400939 7f9326c7c0c0 -1 init, newargv = 0x557665de4ea0 newargc=9ceph-fuse[1704]: starting fuse[root@ceph-client ~]# df -Thceph-fuse fuse.ceph-fuse 93G 0 93G 0% /root/ceph-fuse 2、清除所有列入blacklist的客户端1osd blacklist clear clear all blacklisted clients 实验1 123456789101112131415[root@ceph3 ~]# ceph osd blacklist add 10.20.10.28blacklisting 10.20.10.28:0/0 until 2019-11-13 12:53:48.463948 (3600 sec)[root@ceph3 ~]# ceph osd blacklist add 10.20.10.13blacklisting 10.20.10.13:0/0 until 2019-11-13 12:53:56.846733 (3600 sec)[root@ceph3 ~]# ceph osd blacklist lslisted 2 entries10.20.10.13:0/0 2019-11-13 12:53:56.84673310.20.10.28:0/0 2019-11-13 12:53:48.463948[root@ceph3 ~]# ceph osd blacklist clear removed all blacklist entries[root@ceph3 ~]# ceph osd blacklist lslisted 0 entries 3、显示列入blacklist的客户端1osd blacklist ls --format json show blacklisted clients 实验1 123456[root@ceph1 ~]# ceph osd blacklist ls --format jsonlisted 1 entries[&#123;&quot;addr&quot;:&quot;10.20.10.2:0/0&quot;,&quot;until&quot;:&quot;2019-11-13 17:10:56.217959&quot;&#125;]/0 表示：AsyncMessenger stuff approximately unique ID set by the Constructor for use in entity_addr_t 4、如果客户端在session_autoclose &lt;value&gt;秒（默认为300秒）以上未与MDS通信，则它将自动被驱逐。1fs set &lt;fs_name&gt; max_mds|max_file_size|allow_new_snaps|inline_data|cluster_down|allow_multimds|allow_dirfrags| balancer|standby_count_wanted|session_timeout|session_autoclose &lt;val&gt; &#123;&lt;confirm&gt;&#125; set fs parameter &lt;var&gt; to &lt;val&gt; 实验1 1[root@ceph1 ~]# ceph fs set cephfs session_autoclose 400 5、获取有关一个文件系统的信息1ceph fs get &lt;fs_name&gt; --format json 实验1 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879[root@ceph1 ~]# ceph fs get cephfs --format json&#123; &quot;mdsmap&quot;: &#123; &quot;epoch&quot;: 19, &quot;flags&quot;: 12, &quot;ever_allowed_features&quot;: 0, &quot;explicitly_allowed_features&quot;: 0, &quot;created&quot;: &quot;2019-11-11 11:16:05.316461&quot;, &quot;modified&quot;: &quot;2019-11-13 15:59:17.551876&quot;, &quot;tableserver&quot;: 0, &quot;root&quot;: 0, &quot;session_timeout&quot;: 60, &quot;session_autoclose&quot;: 400, &quot;max_file_size&quot;: 1099511627776, &quot;last_failure&quot;: 0, &quot;last_failure_osd_epoch&quot;: 104, &quot;compat&quot;: &#123; &quot;compat&quot;: &#123;&#125;, &quot;ro_compat&quot;: &#123;&#125;, &quot;incompat&quot;: &#123; &quot;feature_1&quot;: &quot;base v0.20&quot;, &quot;feature_2&quot;: &quot;client writeable ranges&quot;, &quot;feature_3&quot;: &quot;default file layouts on dirs&quot;, &quot;feature_4&quot;: &quot;dir inode in separate object&quot;, &quot;feature_5&quot;: &quot;mds uses versioned encoding&quot;, &quot;feature_6&quot;: &quot;dirfrag is stored in omap&quot;, &quot;feature_8&quot;: &quot;no anchor table&quot;, &quot;feature_9&quot;: &quot;file layout v2&quot; &#125; &#125;, &quot;max_mds&quot;: 1, &quot;in&quot;: [0], &quot;up&quot;: &#123; &quot;mds_0&quot;: 4335 &#125;, &quot;failed&quot;: [], &quot;damaged&quot;: [], &quot;stopped&quot;: [], &quot;info&quot;: &#123; &quot;gid_4335&quot;: &#123; &quot;gid&quot;: 4335, &quot;name&quot;: &quot;ceph2&quot;, &quot;rank&quot;: 0, &quot;incarnation&quot;: 14, &quot;state&quot;: &quot;up:active&quot;, &quot;state_seq&quot;: 41535, &quot;addr&quot;: &quot;10.20.10.13:6804/622620898&quot;, &quot;standby_for_rank&quot;: 0, &quot;standby_for_fscid&quot;: -1, &quot;standby_for_name&quot;: &quot;&quot;, &quot;standby_replay&quot;: true, &quot;export_targets&quot;: [], &quot;features&quot;: 4611087853746454523 &#125;, &quot;gid_4456&quot;: &#123; &quot;gid&quot;: 4456, &quot;name&quot;: &quot;ceph3&quot;, &quot;rank&quot;: 0, &quot;incarnation&quot;: 0, &quot;state&quot;: &quot;up:standby-replay&quot;, &quot;state_seq&quot;: 2, &quot;addr&quot;: &quot;10.20.10.25:6805/1639008809&quot;, &quot;standby_for_rank&quot;: 0, &quot;standby_for_fscid&quot;: -1, &quot;standby_for_name&quot;: &quot;&quot;, &quot;standby_replay&quot;: true, &quot;export_targets&quot;: [], &quot;features&quot;: 4611087853746454523 &#125; &#125;, &quot;data_pools&quot;: [6], &quot;metadata_pool&quot;: 7, &quot;enabled&quot;: true, &quot;fs_name&quot;: &quot;cephfs&quot;, &quot;balancer&quot;: &quot;&quot;, &quot;standby_count_wanted&quot;: 1 &#125;, &quot;id&quot;: 1&#125; blacklist相关配置123456789101112131415161718192021222324252627282930313233343536373839404142434445MON相关配置 客户端blacklist entries保留在OSD map中的持续时间（以秒为单位） Option(&quot;mon_osd_blacklist_default_expire&quot;, Option::TYPE_FLOAT, Option::LEVEL_ADVANCED) .set_default(1_hr) .add_service(&quot;mon&quot;) .set_description(&quot;Duration in seconds that blacklist entries for clients remain in the OSD map&quot;), MDS daemons的blacklist entries保留在OSD map中的持续时间（以秒为单位） Option(&quot;mon_mds_blacklist_interval&quot;, Option::TYPE_FLOAT, Option::LEVEL_DEV) .set_default(1_day) .set_min(1_hr) .add_service(&quot;mon&quot;) .set_description(&quot;Duration in seconds that blacklist entries for MDS daemons remain in the OSD map&quot;),RBD相关配置 是否将损坏锁的客户端列入blacklist Option(&quot;rbd_blacklist_on_break_lock&quot;, Option::TYPE_BOOL, Option::LEVEL_ADVANCED) .set_default(true) .set_description(&quot;whether to blacklist clients whose lock was broken&quot;), blacklist的秒数 - OSD 默认值为 0 Option(&quot;rbd_blacklist_expire_seconds&quot;, Option::TYPE_UINT, Option::LEVEL_ADVANCED) .set_default(0) .set_description(&quot;number of seconds to blacklist - set to 0 for OSD default&quot;),MDS相关配置 是否将sessions已过期的客户端列入blacklist Option(&quot;mds_session_blacklist_on_timeout&quot;, Option::TYPE_BOOL, Option::LEVEL_ADVANCED) .set_default(true) .set_description(&quot;blacklist clients whose sessions have become stale&quot;), 是否将被逐出的客户端列入blacklist Option(&quot;mds_session_blacklist_on_evict&quot;, Option::TYPE_BOOL, Option::LEVEL_ADVANCED) .set_default(true) .set_description(&quot;blacklist clients that have been evicted&quot;), 数秒后，没有响应MDS的“cap revoke messages”的客户端将被驱逐。（默认为0，表示关闭该功能） Option(&quot;mds_cap_revoke_eviction_timeout&quot;, Option::TYPE_FLOAT, Option::LEVEL_ADVANCED) .set_default(0) .set_description(&quot;number of seconds after which clients which have not responded to cap revoke messages by the MDS are evicted.&quot;), MDS重新连接恢复状态期间等待客户端重新连接的超时时间（以秒为单位） Option(&quot;mds_reconnect_timeout&quot;, Option::TYPE_FLOAT, Option::LEVEL_ADVANCED) .set_default(45) .set_description(&quot;timeout in seconds to wait for clients to reconnect during MDS reconnect recovery state&quot;), 参考资料【1】https://access.redhat.com/solutions/3944931 【2】https://docs.ceph.com/docs/mimic/cephfs/eviction/ 接口（CLI后加–format json可以以json格式输出结果）1、添加客户端到blacklist（add (optionally until &lt;expire&gt;seconds from now)&lt;addr&gt;from blacklist） 1osd blacklist add &lt;EntityAddr&gt; &#123;&lt;float[0.0-]&gt;&#125; 2、从blacklist中删除客户端（remove &lt;addr&gt; from blacklist） 1osd blacklist rm &lt;EntityAddr&gt; 3、清除所有列入blacklist的客户端（clear all blacklisted clients） 1osd blacklist clear 4、显示列入blacklist的客户端（show blacklisted clients） 1osd blacklist ls 5、设置session_autoclose，客户端在指定秒数未与MDS通信，则驱逐接口，加入blacklist（set fs parameter &lt;var&gt;to &lt;val&gt;） 1fs set &lt;fs_name&gt; session_autoclose &lt;val&gt; 6、获取有关一个文件系统的session_autoclose信息 1ceph fs get &lt;fs_name&gt;]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[udev设备管理]]></title>
    <url>%2F2019%2F10%2F29%2Fudev%E8%AE%BE%E5%A4%87%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Linuxcast学习笔记，视频地址：https://www.youtube.com/watch?v=eV4InZop--0 udev是什么udev是动态管理设备的机制（/dev/目录下的设备）。udev允许我们自己写一些rule配置文件来控制udev默认的行为动作。默认配置文件在/etc/udev/目录下，在/etc/udev/rules.d/下为默认rule。如70-persistent-net.rules文件，udev在工作时，每次检查/etc/udev/rules.d/目录下的配置文件，并且按照数字的顺序来加载并应用这些配置。 udev允许我们在一个设备连接到计算机的时候，或者已经连接上，或者卸载的时候执行一些特殊的动作。（设备连接时、设备连接上、设备断开时） 如何使用udev修改设备默认名称如果我们想修改一个设备，那么我们需要唯一的定位一个设备，通过udevadm命令。通常serial（串号） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@ceph2 ~]# udevadm info -a -n /dev/vdcUdevadm info starts with the device specified by the devpath and thenwalks up the chain of parent devices. It prints for every devicefound, all possible attributes in the udev rules key format.A rule to match, can be composed by the attributes of the deviceand the attributes from one single parent device. looking at device &apos;/devices/pci0000:00/0000:00:09.0/virtio4/block/vdc&apos;: KERNEL==&quot;vdc&quot; SUBSYSTEM==&quot;block&quot; DRIVER==&quot;&quot; ATTR&#123;ro&#125;==&quot;0&quot; ATTR&#123;size&#125;==&quot;104857600&quot; ATTR&#123;stat&#125;==&quot; 7223 11 474136 19244 400088 30660 1866945 871549 0 840441 872404&quot; ATTR&#123;cache_type&#125;==&quot;write back&quot; ATTR&#123;range&#125;==&quot;16&quot; ATTR&#123;discard_alignment&#125;==&quot;0&quot; ATTR&#123;ext_range&#125;==&quot;256&quot; ATTR&#123;serial&#125;==&quot;e850ae75-fcb2-4432-a&quot; ATTR&#123;alignment_offset&#125;==&quot;0&quot; ATTR&#123;inflight&#125;==&quot; 0 0&quot; ATTR&#123;removable&#125;==&quot;0&quot; ATTR&#123;capability&#125;==&quot;50&quot; looking at parent device &apos;/devices/pci0000:00/0000:00:09.0/virtio4&apos;: KERNELS==&quot;virtio4&quot; SUBSYSTEMS==&quot;virtio&quot; DRIVERS==&quot;virtio_blk&quot; ATTRS&#123;device&#125;==&quot;0x0002&quot; ATTRS&#123;features&#125;==&quot;0010101001110000000000000000110010000000000000000000000000000000&quot; ATTRS&#123;status&#125;==&quot;0x0000000f&quot; ATTRS&#123;vendor&#125;==&quot;0x1af4&quot; looking at parent device &apos;/devices/pci0000:00/0000:00:09.0&apos;: KERNELS==&quot;0000:00:09.0&quot; SUBSYSTEMS==&quot;pci&quot; DRIVERS==&quot;virtio-pci&quot; ATTRS&#123;irq&#125;==&quot;10&quot; ATTRS&#123;subsystem_vendor&#125;==&quot;0x1af4&quot; ATTRS&#123;broken_parity_status&#125;==&quot;0&quot; ATTRS&#123;class&#125;==&quot;0x010000&quot; ATTRS&#123;driver_override&#125;==&quot;(null)&quot; ATTRS&#123;consistent_dma_mask_bits&#125;==&quot;64&quot; ATTRS&#123;dma_mask_bits&#125;==&quot;64&quot; ATTRS&#123;local_cpus&#125;==&quot;f&quot; ATTRS&#123;device&#125;==&quot;0x1001&quot; ATTRS&#123;enable&#125;==&quot;1&quot; ATTRS&#123;msi_bus&#125;==&quot;&quot; ATTRS&#123;local_cpulist&#125;==&quot;0-3&quot; ATTRS&#123;vendor&#125;==&quot;0x1af4&quot; ATTRS&#123;subsystem_device&#125;==&quot;0x0002&quot; ATTRS&#123;numa_node&#125;==&quot;-1&quot; ATTRS&#123;d3cold_allowed&#125;==&quot;0&quot; looking at parent device &apos;/devices/pci0000:00&apos;: KERNELS==&quot;pci0000:00&quot; SUBSYSTEMS==&quot;&quot; DRIVERS==&quot;&quot; 在/etc/udev/rules.d/下创建rule，99-linuxcast.rules，编辑规则。 1KERNEL==&quot;sd*&quot;, ATTR&#123;serial&#125;==&quot;e850ae75-fcb2-4432-a&quot;, NAME=&quot;yujiangvdc%n&quot; KERNEL意思是内核识别出来这个设备是什么名字，两个等号==是做比较，一个等号=是赋值。 通过KERNEL与ATTR{serial}就可以唯一定位到一个设备，要把这个设备修改为其他名字用NAME，udev会自动把%n替换成分区号。]]></content>
      <tags>
        <tag>udev</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph L to N 升级总结]]></title>
    <url>%2F2019%2F10%2F25%2Fceph-L-to-N-%E5%8D%87%E7%BA%A7%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[官方原文 参考1 从Luminous之前的版本进行升级（比如jewel版本）12345678必须先升级到Luminous（12.2.z），然后再尝试升级到Nautilus。 另外，您的集群必须在运行Luminous的同时至少完成了所有PG的一次scrub，并在OSD map中设置了recovery_deletes和purged_snapdirs flags。[root@ceph1 ~]# ceph osd dump | grep ^flagsflags sortbitwise,recovery_deletes,purged_snapdirsrecovery_deletes flags：Ceph现在在recovery过程中处理delete操作，而不是在peering过程中。以前，将down或out超过15分钟的OSD带回到群集中会导致placement group peering时间延长。peering过程需要很长时间才能完成，因为delete操作是在合并placement group日志作为peering过程的一部分时内联处理的。结果，对处于peering状态的placement group的操作被blocked。通过此更新，Ceph可以在正常recovery过程中而不是peering过程中处理delete操作。结果可以使peering过程更快完成，并且操作不再blocked。（This was fixed with the help of Red Hat Ceph Storage feature request #1452780 and this was released in 2.4 errata version 10.2.7-32.el7cp.）purged_snapdirs flags：一旦snapsets全部转换，则设置purged_snapdirs OSDMap flags，这样可以更轻松地测试upgrade + conversion是否已完成。特别是，micim+将能够更简单地测试该flags，而无需等待完整的PG统计信息来知道升级到luminous以外的版本是否安全。 注意1231、在从Luminous升级到Nautilus的过程中，将monitors升级到Nautilus后，将无法使用Luminous ceph-osd daemon创建新的OSD。避免在升级过程中添加或替换任何OSD。2、避免在升级过程中创建任何RADOS pools。3、您可以使用ceph version(s)命令在每个阶段监视升级进度，该命令将告诉您每种ceph daemon正在运行的ceph版本。 自研管理平台用到的接口，对比输出是否有修改，以免影响前台功能Ceph节点升级过程1、确认OSD map包含recovery_deletes和purged_snapdirs flags（否则，将导致您的monitor daemons在启动时拒绝加入quorum，从而使其无法运行） 12345执行命令：ceph osd dump | grep ^flags命令输出：flags sortbitwise,recovery_deletes,purged_snapdirs如果没有recovery_deletes和purged_snapdirs flags需要手动触发pg scrub，并等待大约24-48小时（根据数据量评估）执行命令：ceph pg dump pgs_brief | cut -d &quot; &quot; -f 1 | xargs -n1 ceph pg scrub 2、确保集群稳定，集群状态HEALTH_OK，没有down掉的OSD或无法恢复的OSD。 12345678执行命令：ceph health detail命令输出：HEALTH_OK执行命令：ceph osd tree | grep down命令输出：空执行命令：ceph osd tree | grep out命令输出：空 3、设置noout flags 1234567执行命令：ceph osd set noout命令输出：noout is set执行命令：ceph health detail命令输出：HEALTH_WARN noout flag(s) setOSDMAP_FLAGS noout flag(s) set 4、配置centos ceph 14 Luminous mirror 12 5、升级monitors（通过安装新ceph packages并重新启动monitor daemons来升级monitors） 在每个monitors主机上执行： 12345675.1、安装monitor packages执行命令：yum install ceph-mon命令输出：提示安装ceph-mon以及依赖5.2、重启ceph monitor服务执行命令：systemctl restart ceph-mon.target命令输出：空 monitors启动之后，查找nautilus字符串来验证monitor升级是否完成。确保有min_mon_release 14 (nautilus)字样，如果没有说明尚未升级和重启monitor。 1234执行命令：ceph mon dump | grep min_mon_release命令输出：dumped monmap epoch 2min_mon_release 14 (nautilus) 6、升级ceph-mgr（通过安装新ceph packages并重新启动ceph-mgr daemons来升级ceph-mgr） 如果使用Ceph Dashboard需要安装ceph-mgr-dashboard package。 在每个ceph-mgr主机上执行： 12345678910116.1、安装ceph-mgr packages执行命令：yum install ceph-mgr命令输出：提示安装ceph-mgr以及依赖6.2、安装ceph-mgr-dashboard packages执行命令：yum install ceph-mgr-dashboard命令输出：提示安装ceph-mgr-dashboard以及依赖6.3、重启ceph mgr服务执行命令：systemctl restart ceph-mgr.target命令输出：空 通过ceph -s验证ceph-mgr daemons是否正在运行（确保mgr状态active，standbys mgr加入）。 1234567执行命令：ceph -s命令输出：... services: mon: 3 daemons, quorum community-ceph-2,community-ceph-3,community-ceph-1 (age 25m) mgr: community-ceph-2(active, since 3d), standbys: community-ceph-3, community-ceph-1... 7、升级ceph-osd（通过安装新ceph packages并重新启动ceph-osd daemons来升级ceph-osd） 在每个ceph-osd主机上执行： 12345677.1、安装ceph-osd packages执行命令：yum install ceph-osd命令输出：提示安装ceph-osd以及依赖7.2、重启ceph osd服务执行命令：systemctl restart ceph-osd.target命令输出：空 查看OSD升级的进度 123456执行命令：ceph osd versions命令输出：&#123; &quot;ceph version 12.2.x (...) luminous (stable)&quot;: 2, &quot;ceph version 14.2.4 (...) nautilus (stable)&quot;: 4,&#125; 使用ceph-volume接管ceph-disk创建的OSD，在每个OSD主机上执行，执行前确保每个OSD都正在运行，无down或out的OSD： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051521、确保OSD运行正常执行命令：ceph osd tree | grep down命令输出：空执行命令：ceph osd tree | grep out命令输出：空2、所有使用ceph-disk创建的并正在运行的OSDs，从OSD data partition或directory中捕获元数据执行命令：ceph-volume simple scan命令输出：执行命令后，会生成类似/etc/ceph/osd/0-ab0a204a-42e3-4a47-ab4c-0888edf429cb.json文件，文件内容为：&#123; &quot;active&quot;: &quot;ok&quot;, &quot;block&quot;: &#123; &quot;path&quot;: &quot;/dev/disk/by-partuuid/0818811f-d70e-4ff0-91c9-58cd701c9a19&quot;, &quot;uuid&quot;: &quot;0818811f-d70e-4ff0-91c9-58cd701c9a19&quot; &#125;, &quot;block_uuid&quot;: &quot;0818811f-d70e-4ff0-91c9-58cd701c9a19&quot;, &quot;bluefs&quot;: 1, &quot;ceph_fsid&quot;: &quot;c4051efa-1997-43ef-8497-fb02bdf08233&quot;, &quot;cluster_name&quot;: &quot;ceph&quot;, &quot;data&quot;: &#123; &quot;path&quot;: &quot;/dev/vdc1&quot;, &quot;uuid&quot;: &quot;ab0a204a-42e3-4a47-ab4c-0888edf429cb&quot; &#125;, &quot;fsid&quot;: &quot;ab0a204a-42e3-4a47-ab4c-0888edf429cb&quot;, &quot;keyring&quot;: &quot;AQB1FLFdXVHVARAARTKkxT1xgrDNU/QECUqdxA==&quot;, &quot;kv_backend&quot;: &quot;rocksdb&quot;, &quot;magic&quot;: &quot;ceph osd volume v026&quot;, &quot;mkfs_done&quot;: &quot;yes&quot;, &quot;ready&quot;: &quot;ready&quot;, &quot;systemd&quot;: &quot;&quot;, &quot;type&quot;: &quot;bluestore&quot;, &quot;whoami&quot;: 0&#125;3、使systemd units可以mount已配置的devices，并启动Ceph OSD执行命令：ceph-volume simple activate --all命令输出：--&gt; activating OSD specified in /etc/ceph/osd/1-fe327306-54a4-4362-870d-92d28cf65e42.jsonRunning command: ln -snf /dev/vdc2 /var/lib/ceph/osd/ceph-1/blockRunning command: chown -R ceph:ceph /dev/vdc2Running command: systemctl enable ceph-volume@simple-1-fe327306-54a4-4362-870d-92d28cf65e42Running command: ln -sf /dev/null /etc/systemd/system/ceph-disk@.service--&gt; All ceph-disk systemd units have been disabled to prevent OSDs getting triggered by UDEV eventsRunning command: systemctl enable --runtime ceph-osd@1Running command: systemctl start ceph-osd@1--&gt; Successfully activated OSD 1 with FSID fe327306-54a4-4362-870d-92d28cf65e424、重启每个OSD主机，确认OSD是否开机自启执行命令：reboot命令输出：无 8、升级ceph-mds（通过安装新ceph packages并重新启动ceph-mds daemons来升级ceph-mds） 在每个ceph-mds主机上执行： 123456789101112131415161718192021222324252627282930313233343536373839404142434445记录rank数量，并将ranks数减少到1，通过ceph mds stat命令输出可以查看rank数量。&#123;0=ceph3=up:active&#125;代表rank数为1，&#123;0=ceph3=up:active,1=ceph2=up:active&#125;代表rank数为2，以此类推。8.1、查看当前rank数量执行命令：ceph mds stat命令输出：cephfs-1/1/1 up &#123;0=ceph3=up:active&#125;, 2 up:standby如果rank为2，命令输出为：cephfs-2/2/2 up &#123;0=ceph3=up:active,1=ceph2=up:active&#125;, 1 up:standby8.2、查看cephfs名称执行命令：ceph fs ls命令输出：name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]8.3、将ranks数减少到1执行命令：ceph fs set &lt;fs_name&gt; max_mds 1命令输出：空8.4、通过定期检查状态，等待集群停用所有non-zero ranks，当&#123;0=ceph3=up:active&#125;时为已停用所有non-zero ranks执行命令：ceph status命令输出：cephfs-1/1/1 up &#123;0=ceph3=up:active&#125;, 2 up:standby8.5、使用以下命令使所有standby MDS daemons在适当的主机offline执行命令：systemctl stop ceph-mds@&lt;daemon_name&gt;命令输出：空8.6、确认只有一个MDS处于online，并且cephfs只有一个rank 0执行命令：ceph status命令输出：... mds: cephfs-1/1/1 up &#123;0=ceph3=up:active&#125;...8.7、通过安装新packages并重新启动daemon来升级MDS daemon（在每个mds节点上执行）执行命令：yum install ceph-mds命令输出：提示安装ceph-mds以及依赖执行命令：systemctl restart ceph-mds.target命令输出：空8.8、重新启动所有已的offline standby MDS daemons执行命令：systemctl restart ceph-mds.target命令输出：空8.9、恢复max_mds原始值执行命令：ceph fs set &lt;fs_name&gt; max_mds &lt;original_max_mds&gt;命令输出：空 9、升级ceph-radosgw（通过安装新ceph packages并重新启动ceph-radosgw daemons来升级ceph-radosgw） 在每个ceph-radosgw主机上执行： 12345675.1、安装radosgw packages执行命令：yum install ceph-radosgw命令输出：提示安装ceph-radosgw以及依赖5.2、重启ceph radosgw服务执行命令：systemctl restart ceph-radosgw.target命令输出：空 10、启用所有Nautilus的新功能来完成升级 12执行命令：ceph osd require-osd-release nautilus命令输出：空 11、清除noout flags 12执行命令：ceph osd unset noout命令输出：noout is unset 12、启用新的 v2 network protocol 12执行命令：ceph mon enable-msgr2命令输出：空 Client节点升级过程1、Client节点升级package 12执行命令：yum install ceph-common librados2 librbd1 python-rbd python-rados -y命令输出：提示安装ceph-common librados2 librbd1 python-rbd python-rados以及依赖 2、Client节点确认升级后的版本 12执行命令：ceph --version命令输出：ceph version 14.2.4 (75f4de193b3ea58512f204623e6c5a16e6c1e1ba) nautilus (stable) Ceph端升级后修复1、Legacy BlueStore stats reporting detected on 6 OSD(s)问题描述： ​ 使用ceph -s命令查看集群状态时，出现Legacy BlueStore stats reporting detected on 6 OSD(s) 解决办法： 123systemctl stop ceph-osd@$OSDIDceph-bluestore-tool repair –path /var/lib/ceph/osd/ceph-$OSDIDsystemctl start ceph-osd@$OSDID 参考资料： 【1】http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/036002.html 关于执行ceph-bluestore-tool repair报错问题确认：https://tracker.ceph.com/issues/42297 2、3 monitors have not enabled msgr2问题描述： ​ 使用ceph -s命令查看集群状态时，出现 3 monitors have not enabled msgr2 解决办法： 12ceph mon enable-msgr2systemctl restart ceph-mon@ceph1.service]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[samba]]></title>
    <url>%2F2019%2F10%2F21%2Fsamba%2F</url>
    <content type="text"><![CDATA[系统环境 12cat /etc/redhat-release CentOS Linux release 7.4.1708 (Core) samba安装安装samba 123yum install -y sambasamba Version 4.9.1 Ceph KRBD使用samba1、首先创建pool 1ceph osd pool create rbd 8 2、在pool中创建rbd 1234567891011rbd create --size 10G rbd/rbd-1# rbd info rbd/rbd-1rbd image &apos;rbd-1&apos;: size 10GiB in 2560 objects order 22 (4MiB objects) block_name_prefix: rbd_data.10c06b8b4567 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten flags: create_timestamp: Wed Oct 23 10:04:16 2019 3、在linux上创建挂载点 1mkdir ceph-rbd-1-mountpoint 4、map pool中的rbd到linux 123456789rbd map rbd/rbd-1# lsmod | grep rbdrbd 83733 2 libceph 306742 1 rbd# sudo rbd showmappedid pool image snap device 0 rbd rbd-1 - /dev/rbd0 5、格式化文件系统 1mkfs.xfs /dev/rbd/rbd/rbd-1 6、mount到挂载点 12345mount /dev/rbd/rbd/rbd-1 /root/ceph-rbd-1-mountpoint/# df -ThFilesystem Type Size Used Avail Use% Mounted on/dev/rbd0 xfs 10G 33M 10G 1% /root/ceph-rbd-1-mountpoint 7、samba预操作（关闭防火墙） 123456systemctl stop firewalldsetenforce 0持久化关闭selinuxvim /etc/selinux/configSELINUX=disabled 8、配置samba（samba默认会共享linux home目录，如果自定义需要自己配置），并重启服务 1234567891011121314151617181920212223242526272829303132333435363738394041424344vim /etc/samba/smb.conf[global] workgroup = SAMBA security = user passdb backend = tdbsam printing = cups printcap name = cups load printers = yes cups options = raw[homes] comment = Home Directories valid users = %S, %D%w%S browseable = No read only = No inherit acls = Yes writable = Yes[ceph] comment = ceph path = /root/ceph-rbd-1-mountpoint/ valid users = %S, %D%w%S browseable = No read only = No inherit acls = Yes writable = Yes[printers] comment = All Printers path = /var/tmp printable = Yes create mask = 0600 browseable = No[print$] comment = Printer Drivers path = /var/lib/samba/drivers write list = @printadmin root force group = @printadmin create mask = 0664 directory mask = 0775systemctl restart smb 9、为samba添加ceph用户 1smbpasswd -a ceph 10、客户端访问samba 1\\10.20.10.23\ceph window清除samba连接，重新登录 123456789101112131415C:\Users\JiangYu&gt;net use会记录新的网络连接。状态 本地 远程 网络-------------------------------------------------------------------------------OK \\10.20.10.23\IPC$ Microsoft Windows Network命令成功完成。C:\Users\JiangYu&gt;net use \\10.20.10.23\IPC$ /del\\10.20.10.23\IPC$ 已经删除。C:\Users\JiangYu&gt;net use会记录新的网络连接。列表是空的。 11、因为是使用root用户创建的ceph-rbd-1-mountpoint目录，所以ceph用户没有权限对这个目录进行写操作，需要修改目录所有者，修改后就可以对这个目录进行创建文件等写操作了 1chown ceph:ceph ceph-rbd-1-mountpoint/ samba配置1、查看配置文件帮助手册 1man smb.conf 5 2、smb.conf配置文件语法 1;或#开头为注释 3、检查配置文件语法命令 1testparm /etc/samba/smb.conf 4、Security-Enhanced Linux (SELinux) Notes 123456789101112131415161718192021222324启用samba_domain_controller Boolean以允许Samba PDC使用useradd和groupadd二进制。以root用户身份运行以下命令以启用此Boolean：setsebool -P samba_domain_controller on如果要通过Samba共享home directories，请打开samba_enable_home_dirs Boolean。以root用户身份运行以下命令以启用此Boolean：setsebool -P samba_enable_home_dirs on如果创建新directories，例如新的顶级directories，请使用samba_share_t对其进行标记，以便SELinux允许Samba对其进行读写。不要使用samba_share_t标记系统目录，例如/etc/和/home/，因为这样的目录应该已经具有SELinux label了。运行&quot;ls -ldZ /path/to/directory&quot;命令以查看给定目录的当前SELinux label。仅在创建的文件和目录上设置SELinux labels。使用chcon命令临时更改label：chcon -t samba_share_t /path/to/directory重新标记文件系统或运行诸如restorecon之类的命令时，通过chcon进行的更改将丢失。使用samba_export_all_ro或samba_export_all_rw Boolean共享系统目录。 要共享这样的目录并仅允许只读权限：setsebool -P samba_export_all_ro on要共享此类目录并允许读写权限：setsebool -P samba_export_all_rw on要运行脚本（preexec/root prexec/print command/...），请将它们复制到/var/lib/samba/scripts/目录中，以便SELinux允许smbd运行它们。注意，如果将脚本移动到/var/lib/samba/scripts/，它们将保留其现有的SELinux labels，这些labels可能是SELinux不允许smbd运行的labels。复制脚本将得到正确的SELinux labels。以root用户身份运行&quot;restorecon -R -v /var/lib/samba/scripts&quot;命令，以将正确的SELinux labels应用于这些文件。 5、配置文件段 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213#======================= Global Settings =====================================[global]# ----------------------- Network-Related Options -------------------------## workgroup = Windows NT domain name 或 workgroup name, 例如, MYGROUP.## server string = 等效于Windows NT Description字段（用于描述）.## netbios name = 用于指定不与hostname绑定的server name，最多15个字符。## interfaces = 用于将Samba配置为侦听多个network interfaces。如果您有多个interfaces，则可以使用&quot;interfaces =&quot;选项来配置Samba侦听哪些interface。 切勿忽略localhost interface (lo)。## hosts allow = 允许连接的主机。This option can also be used on a per-share basis.## hosts deny = 不允许主机连接。 This option can also be used on a per-share basis.# workgroup = MYGROUP server string = Samba Server Version %v; netbios name = MYSERVER; interfaces = lo eth0 192.168.12.2/24 192.168.13.2/24; hosts allow = 127. 192.168.12. 192.168.13.# --------------------------- Logging Options -----------------------------## log file = 指定日志文件写入的位置以及它们的拆分方式。## max log size = 指定允许的最大日志文件大小。 日志文件达到&quot;max log size&quot;指定的大小时，将对其进行rotated。# # log files split per-machine: log file = /var/log/samba/log.%m # maximum size of 50KB per log file, then rotate: max log size = 50# ----------------------- Standalone Server Options ------------------------## security = Samba运行的模式。可以将其设置为user, share (不建议使用), 或 server (不建议使用).## passdb backend = 用于存储用户信息的backend（后端）。新安装应使用tdbsam或ldapsam。tdbsam不需要其他配置。&quot;smbpasswd&quot;可用于向后兼容。# security = user passdb backend = tdbsam# ----------------------- Domain Members Options ------------------------## security = 必须设置为 domain 或 ads.## passdb backend = 用于存储用户信息的backend（后端）。新安装应使用tdbsam或ldapsam。tdbsam不需要其他配置。&quot;smbpasswd&quot;可用于向后兼容。## realm = 当设置了&quot;security = ads&quot;选项时才会用到该选项。## password server = 当设置了&quot;security = server&quot;选项或无法使用DNS定位Domain Controller时，才使用此选项。参数列表可以包括My_PDC_Name, [My_BDC_Name], 和[My_Next_BDC_Name]## password server = My_PDC_Name [My_BDC_Name] [My_Next_BDC_Name]## 使用 &quot;password server = *&quot; 自动定位 Domain Controllers.; security = domain; passdb backend = tdbsam; realm = MY_REALM; password server = &lt;NT-Server-Name&gt;# ----------------------- Domain Controller Options ------------------------## security = 必须将domain controllers设置为user。## passdb backend = 用于存储用户信息的backend（后端）。新安装应使用tdbsam或ldapsam。tdbsam不需要其他配置。&quot;smbpasswd&quot;可用于向后兼容。## domain master = 将Samba指定为Domain Master Browser，从而允许Samba整理subnets之间的browse lists。 如果您已经有Windows NT domain controller执行此任务，请不要使用&quot;domain master&quot;选项。## domain logons = 允许Samba为Windows workstations提供网络登录服务。## logon script = 指定在登录时在客户端上运行的脚本。 必须在名为NETLOGON的共享中提供这些脚本。## logon path = 指定（使用UNC path）用户配置文件的存储位置。##; security = user; passdb backend = tdbsam; domain master = yes; domain logons = yes # 以下登录脚本名称由machine name确定(%m):; logon script = %m.bat # 以下登录脚本名称由UNIX user确定:; logon script = %u.bat; logon path = \\%L\Profiles\%u # 使用empty path禁用profile support; logon path = # 可以在domain controller或独立machine上使用各种脚本来添加或删除相应的UNIX帐户：; add user script = /usr/sbin/useradd &quot;%u&quot; -n -g users; add group script = /usr/sbin/groupadd &quot;%g&quot;; add machine script = /usr/sbin/useradd -n -c &quot;Workstation (%u)&quot; -M -d /nohome -s /bin/false &quot;%u&quot;; delete user script = /usr/sbin/userdel &quot;%u&quot;; delete user from group script = /usr/sbin/userdel &quot;%u&quot; &quot;%g&quot;; delete group script = /usr/sbin/groupdel &quot;%g&quot;# ----------------------- Browser Control Options ----------------------------## local master = 设置为no时，Samba不会成为网络上的master browser。 设置为yes时，将应用常规election（选举） rules。## os level = 确定服务器在master browser选举中的优先级。 默认值应该合理。## preferred master = 设置为yes时，Samba会在启动时强制进行local browser选举（并使其赢得选举的几率略高）。#; local master = no; os level = 33; preferred master = yes#----------------------------- Name Resolution -------------------------------## 本节详细介绍了对 Windows Internet Name Service (WINS) 的支持.## 注意：Samba可以是WINS服务器，也可以是WINS客户端，但不能同时是两者。## wins support = 设置为yes时，Samba的NMBD组件启用其WINS服务器。## wins server = 告诉Samba的NMBD组件是WINS客户端。## wins proxy = 设置为yes时，Samba代表不支持WINS的客户端回答name resolution查询。 为此，网络上至少必须有一个WINS服务器。 默认为no。## dns proxy = 设置为yes时，Samba尝试通过DNS nslookups解析NetBIOS名称。; wins support = yes; wins server = w.x.y.z; wins proxy = yes; dns proxy = yes# --------------------------- Printing Options -----------------------------## 本部分中的选项使您可以配置non-default（非默认） printing system。## load printers = 设置为yes时，将自动加载printers列表，而不是单独进行设置。## cups options = 允许您将选项传递到CUPS库。 例如，将此选项设置为raw，则可以在Windows客户端上使用驱动程序。## printcap name = 用于指定备用的printcap文件。# load printers = yes cups options = raw; printcap name = /etc/printcap # 自动获取UNIX System V系统上的printers列表：; printcap name = lpstat; printing = cups# --------------------------- File System Options ---------------------------## 如果文件系统支持扩展属性，并且启用了这些属性（通常通过&quot;user_xattr&quot; mount选项），则可以取消注释本节中的选项。# 这些选项允许管理员指定DOS属性存储在扩展属性中，并且还确保Samba不会更改permission bits。## 注意：这些选项可以按per-share使用。 全局设置它们（在[global]部分中）使它们成为所有共享的默认值。; map archive = no; map hidden = no; map read only = no; map system = no; store dos attributes = yes#============================ Share Definitions ==============================[homes] comment = Home Directories browseable = no writable = yes; valid users = %S; valid users = MYDOMAIN\%S[printers] comment = All Printers path = /var/spool/samba browseable = no guest ok = no writable = no printable = yes# 取消注释以下内容，并为Domain Logons创建netlogon目录：; [netlogon]; comment = Network Logon Service; path = /var/lib/samba/netlogon; guest ok = yes; writable = no; share modes = no# 取消注释以下内容以提供特定的roaming profile share。# 默认为使用用户的home目录：; [Profiles]; path = /var/lib/samba/profiles; browseable = no; guest ok = yes# 一个只读公共目录，但&quot;staff&quot; group中的用户（具有写权限）除外：; [public]; comment = Public Stuff; path = /home/samba; public = yes; writable = no; printable = no; write list = +staff 6、用户操作 1234smbpasswd -a 增加用户（linux系统用户）smbpasswd -d 冻结用户smbpasswd -e 解冻用户smbpasswd -n 将用户的密码设置成空. LinuxCast笔记 samba相关rpm123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112Name : ctdbSummary : A Clustered Database based on Samba&apos;s Trivial Database (TDB)Description : CTDB是Samba和其他项目用来存储临时数据的TDB数据库的集群实现。如果应用程序已经在使用TDB来存储临时数据，则很容易将该应用程序转换为群集感知型，而使用CTDB。Name : samba-clientSummary : Samba client programsDescription : samba-client package提供了一些SMB/CIFS客户端，以补充Linux中内置的SMB/CIFS filesystem。这些客户端允许访问SMB/CIFS shares并打印到SMB/CIFS printers。Name : samba-develSummary : Developer tools for Samba librariesDescription : samba-devel package包含开发Samba套件时的SMB，RPC和其他程序所需libraries的header files。Name : samba-vfs-glusterfsSummary : Samba VFS module for GlusterFSDescription : 包含GlusterFS集成Samba VFS的模块。Name : ctdb-testsSummary : CTDB clustered database test suiteDescription : CTDB的测试套件。CTDB是Samba和其他项目用来存储临时数据的TDB数据库的集群实现。如果应用程序已经在使用TDB来存储临时数据，则很容易将该应用程序转换为群集感知型，而使用CTDB。Name : samba-client-libsSummary : Samba client librariesDescription : samba-client-libs package包含SMB/CIFS客户端所需的internal libraries。Name : samba-krb5-printingSummary : Samba CUPS backend for printing with KerberosDescription : 如果您需要 Kerberos 进行print jobs，通过 SMB后端连接到printer cups，则需要安装该软件包。它将允许cups访问发出print job的用户的 Kerberos credentials cache。Name : samba-winbindSummary : Samba winbindDescription : samba-winbind package提供了winbind NSS library和一些客户端工具。 Winbind使Linux成为Windows domains中的正式成员，并在Linux上使用Windows user和group帐户。Name : libsmbclientSummary : The SMB client libraryDescription : libsmbclient包含来自Samba套件的SMB客户端library。Name : samba-common-libsSummary : Libraries used by both Samba servers and clientsDescription : samba-common-libs package包含SMB/CIFS客户端所需的internal libraries。Name : samba-libsSummary : Samba librariesDescription : samba-libs package包含Samba套件的SMB，RPC和其他协议所需的libraries。Name : samba-winbind-clientsSummary : Samba winbind clientsDescription : samba-winbind-clients package提供wbinfo和ntlm_auth工具。Name : libsmbclient-develSummary : Developer tools for the SMB client libraryDescription : libsmbclient-devel package包含开发相关Samba套件的SMB client library link所需的header files和libraries。Name : samba-common-toolsSummary : Tools for Samba servers and clientsDescription : samba-common-tools package包含用于Samba servers和SMB/CIFS clients的工具。Name : samba-pythonSummary : Samba Python librariesDescription : samba-python package包含Python程序中使用SMB，RPC和其他Samba提供的协议的程序所需的Python libraries。Name : samba-winbind-krb5-locatorSummary : Samba winbind krb5 locatorDescription : winbind krb5 locator是系统kerberos library的plugin，以允许本地kerberos library使用与samba和winbind相同的KDC。Name : libwbclientSummary : The winbind client libraryDescription : libwbclient package包含Samba套件中的winbind client library。Name : samba-dcSummary : Samba AD Domain ControllerDescription : samba-dc package提供AD Domain Controller功能Name : samba-python-testSummary : Samba Python librariesDescription : samba-python-test package包含Samba test suite使用的Python libraries。 如果要运行全套Samba测试，则需要安装此package。Name : samba-winbind-modulesSummary : Samba winbind modulesDescription : samba-winbind-modules package提供了与Winbind Daemon通信所需的NSS library和PAM module。Name : libwbclient-develSummary : Developer tools for the winbind libraryDescription : libwbclient-devel package为wbclient library提供了developer tools。Name : samba-dc-libsSummary : Samba AD Domain Controller LibrariesDescription : samba-dc-libs package包含DC去链接SMB，RPC和其他协议所需的库。Name : samba-testSummary : Testing tools for Samba servers and clientsDescription : samba-test为Samba的server和client packages提供测试工具。Name : sambaSummary : Server and Client software to interoperate with Windows machinesDescription : Samba is the standard Windows interoperability suite of programs : for Linux and Unix.Name : samba-debuginfoSummary : Debug information for package sambaDescription : 该软件包提供了samba软件包的debug information。Name : samba-test-libsSummary : Libraries need by the testing tools for Samba servers and clientsDescription : samba-test-libs提供测试工具所需的libraries。Name : samba-commonSummary : Samba servers 和 clients使用的文件Description : samba-common提供Samba的server和client packages所需的文件。Name : samba-pidlSummary : Perl IDL编译器Description : samba-pidl package包含Samba和Wireshark用于解析IDL和类似协议的Perl IDL编译器 参考资料【1】https://blog.csdn.net/skdkjzz/article/details/42101363]]></content>
      <tags>
        <tag>samba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph DISKPREDICTION MODULE]]></title>
    <url>%2F2019%2F10%2F21%2FCeph-DISKPREDICTION-MODULE%2F</url>
    <content type="text"><![CDATA[DISKPREDICTION MODULE磁盘预测模块支持两种模式：cloud mode和local mode。 在cloud mode下，磁盘和Ceph操作状态信息是从Ceph群集中收集的，并通过Internet发送到基于云的DiskPrediction服务器。 DiskPrediction服务器分析数据，并提供Ceph群集的性能和磁盘运行状况的分析和预测结果。 local mode不需要任何外部服务器即可进行数据分析和输出结果。 在local mode下，磁盘diskprediction module将内部predictor module用于磁盘预测服务，然后将磁盘预测结果返回给Ceph系统。 Local predictor: 70% 的准确性 Cloud predictor for free: 95% 的准确性 ENABLING运行以下命令以在Ceph环境中启用磁盘预测模块： 12ceph mgr module enable diskprediction_cloudceph mgr module enable diskprediction_local 选择预测模式： 1ceph config set global device_failure_prediction_mode local 或 1ceph config set global device_failure_prediction_mode cloud 要禁用预测，请执行以下操作： 1ceph config set global device_failure_prediction_mode none CONNECTION SETTINGSconnection settings用于Ceph和DiskPrediction服务器之间的连接。 LOCAL MODEdiskprediction module利用Ceph设备运行状况检查来收集磁盘运行状况指标，并使用内部predictor module来生成磁盘故障预测并返回Ceph。 因此，在local mode下不需要连接设置。 local predictor module至少需要六个设备健康状况数据集才能实施预测。 运行以下命令以使用本地预测变量预测设备的预期寿命。 1234ceph device predict-life-expectancy &lt;device id&gt;[root@community-ceph-1 ~]# ceph device predict-life-expectancy 0d2a946c-413f-43f4-bunknown CLOUD MODE在cloud mode下，需要用户注册。 用户必须在https://www.diskprophet.com/#/上注册其帐户，以接收以下DiskPrediction服务器信息以进行连接设置。 Certificate file path: 确认用户注册后，系统将发送一封确认电子邮件，其中包括证书文件下载链接。 下载证书文件并将其保存到Ceph系统。 运行以下命令来验证文件。 如果没有证书文件验证，则无法完成连接设置。 DiskPrediction server: DiskPrediction服务器名称。 如果需要，它可以是IP地址。 Connection account: 用于在Ceph和DiskPrediction服务器之间建立连接的帐户名 Connection password: 用于在Ceph和DiskPrediction服务器之间建立连接的密码 运行以下命令以完成连接设置。 1ceph device set-cloud-prediction-config &lt;diskprediction_server&gt; &lt;connection_account&gt; &lt;connection_password&gt; &lt;certificate file path&gt; 您可以使用以下命令显示连接设置： 1ceph device show-prediction-config 其他可选配置设置如下： diskprediction_upload_metrics_interval: 指示定期将Ceph性能指标发送到DiskPrediction服务器的频率。 默认值为10分钟。 diskprediction_upload_smart_interval: 指示定期将Ceph物理设备信息发送到DiskPrediction服务器的频率。 默认值为12小时。 diskprediction_retrieve_prediction_interval: 指示Ceph有时会定期从DiskPrediction服务器检索物理设备预测数据。 默认值为12小时。 DISKPREDICTION DATAdiskprediction module主动向/从DiskPrediction服务器发送/检索以下数据。 METRICS DATA Ceph cluster status key Description cluster_health Ceph health check status num_mon Number of monitor node num_mon_quorum Number of monitors in quorum num_osd Total number of OSD num_osd_up Number of OSDs that are up num_osd_in Number of OSDs that are in cluster osd_epoch Current epoch of OSD map osd_bytes Total capacity of cluster in bytes osd_bytes_used Number of used bytes on cluster osd_bytes_avail Number of available bytes on cluster num_pool Number of pools num_pg Total number of placement groups num_pg_active_clean Number of placement groups in active+clean state num_pg_active Number of placement groups in active state num_pg_peering Number of placement groups in peering state num_object Total number of objects on cluster num_object_degraded Number of degraded (missing replicas) objects num_object_misplaced Number of misplaced (wrong location in the cluster) objects num_object_unfound Number of unfound objects num_bytes Total number of bytes of all objects num_mds_up Number of MDSs that are up num_mds_in Number of MDS that are in cluster num_mds_failed Number of failed MDS mds_epoch Current epoch of MDS map Ceph mon/osd performance counts Mon: key Description num_sessions Current number of opened monitor sessions session_add Number of created monitor sessions session_rm Number of remove_session calls in monitor session_trim Number of trimed monitor sessions num_elections Number of elections monitor took part in election_call Number of elections started by monitor election_win Number of elections won by monitor election_lose Number of elections lost by monitor Osd: key Description op_wip Replication operations currently being processed (primary) op_in_bytes Client operations total write size op_r Client read operations op_out_bytes Client operations total read size op_w Client write operations op_latency Latency of client operations (including queue time) op_process_latency Latency of client operations (excluding queue time) op_r_latency Latency of read operation (including queue time) op_r_process_latency Latency of read operation (excluding queue time) op_w_in_bytes Client data written op_w_latency Latency of write operation (including queue time) op_w_process_latency Latency of write operation (excluding queue time) op_rw Client read-modify-write operations op_rw_in_bytes Client read-modify-write operations write in op_rw_out_bytes Client read-modify-write operations read out op_rw_latency Latency of read-modify-write operation (including queue time) op_rw_process_latency Latency of read-modify-write operation (excluding queue time) Ceph pool statistics key Description bytes_used Per pool bytes used max_avail Max available number of bytes in the pool objects Number of objects in the pool wr_bytes Number of bytes written in the pool dirty Number of bytes dirty in the pool rd_bytes Number of bytes read in the pool stored_raw Bytes used in pool including copies made Ceph physical device metadata key Description disk_domain_id Physical device identify id disk_name Device attachment name disk_wwn Device wwn model Device model name serial_number Device serial number size Device size vendor Device vendor name Ceph each objects correlation information The module agent information The module agent cluster information The module agent host information SMART DATA Ceph physical device SMART data (provided by Ceph devicehealth module) PREDICTION DATA Ceph physical device prediction data RECEIVING PREDICTED HEALTH STATUS FROM A CEPH OSD DISK DRIVE（从CEPH OSD磁盘驱动器中接收预期的健康状况）您可以使用以下命令从Ceph OSD磁盘驱动器接收预测的健康状态。 1ceph device get-predicted-status &lt;device id&gt; get-predicted-status命令返回： 1234567&#123; &quot;near_failure&quot;: &quot;Good&quot;, &quot;disk_wwn&quot;: &quot;5000011111111111&quot;, &quot;serial_number&quot;: &quot;111111111&quot;, &quot;predicted&quot;: &quot;2018-05-30 18:33:12&quot;, &quot;attachment&quot;: &quot;sdb&quot;&#125; Attribute Description near_failure The disk failure prediction state: Good/Warning/Bad/Unknown disk_wwn Disk WWN number serial_number Disk serial number predicted Predicted date attachment device name on the local system 磁盘故障预测状态的near_failure属性在下表中指示磁盘预期寿命。 near_failure Life expectancy (weeks) Good &gt; 6 weeks Warning 2 weeks ~ 6 weeks Bad &lt; 2 weeks DEBUGGING如果要调试DiskPrediction module映射到Ceph日志记录级别，请使用以下命令。 123[mgr] debug mgr = 20 将日志记录设置为管理器调试时，模块将打印出带有前缀mgr [diskprediction]的日志记录消息，以便于过滤。]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph DEVICE MANAGEMENT]]></title>
    <url>%2F2019%2F10%2F21%2FCeph-DEVICE-MANAGEMENT%2F</url>
    <content type="text"><![CDATA[DEVICE MANAGEMENTCeph跟踪哪个daemons消耗了哪些hardware storage devices（例如HDD，SSD），并收集有关这些devices的运行状况指标，以提供预测和/或自动响应硬件故障的工具。 DEVICE TRACKING您可以查询哪些存储设备正在使用： 123456789ceph device lsDEVICE HOST:DEV DAEMONS LIFE EXPECTANCY 0d2a946c-413f-43f4-b community-ceph-2.novalocal:vdc osd.0451e48d6-913e-4f93-a community-ceph-1.novalocal:vdd osd.5935b6018-1dfe-4cf9-8 community-ceph-1.novalocal:vdc osd.2abe09d21-d950-47b0-9 community-ceph-2.novalocal:vdd osd.3bf37729e-9d83-48e9-9 community-ceph-3.novalocal:vdc osd.1d48dcf29-fe58-4e3e-a community-ceph-3.novalocal:vdd osd.4 您还可以按daemon或host列出devices： 1234567891011ceph device ls-by-daemon &lt;daemon&gt;ceph device ls-by-host &lt;host&gt;[root@community-ceph-1 ~]# ceph device ls-by-daemon osd.0DEVICE HOST:DEV EXPECTED FAILURE 0d2a946c-413f-43f4-b community-ceph-2.novalocal:vdc [root@community-ceph-1 ~]# ceph device ls-by-host community-ceph-2.novalocalDEVICE DEV DAEMONS EXPECTED FAILURE 0d2a946c-413f-43f4-b vdc osd.0 abe09d21-d950-47b0-9 vdd osd.3 对于任何单个设备，您可以通过以下方式查询有关其位置以及如何使用它的信息： 123456ceph device info &lt;devid&gt;[root@community-ceph-1 ~]# ceph device info 0d2a946c-413f-43f4-bdevice 0d2a946c-413f-43f4-battachment community-ceph-2.novalocal:vdcdaemons osd.0 ENABLING MONITORINGCeph还可以监视与您的设备关联的健康指标。 例如，SATA硬盘实现了一个称为SMART的标准，该标准提供了有关设备使用情况和运行状况的内部指标，例如开机小时数，电源循环次数或不可恢复的读取错误。 其他设备类型（例如SAS和NVMe）实现了一组相似的指标（通过略有不同的标准）。 Ceph可以通过smartctl工具收集所有这些信息。 您可以通过以下方式启用或禁用运行状况监视： 1ceph device monitoring on 或 1ceph device monitoring off SCRAPING如果启用了监视，则将定期自动scraped指标。 该间隔可以配置为： 1ceph config set mgr mgr/devicehealth/scrape_frequency &lt;seconds&gt; 默认值为每24小时scrape一次。 您可以使用以下方法手动触发所有设备的scrape： 1ceph device scrape-health-metrics 单个设备可以用以下方式scraped： 1ceph device scrape-health-metrics &lt;device-id&gt; 或单个daemon的设备可以通过以下方式进行scraped： 1ceph device scrape-daemon-health-metrics &lt;who&gt; 可以使用以下命令检索设备的存储健康指标（可选地，用于特定时间戳）： 1ceph device get-health-metrics &lt;devid&gt; [sample-timestamp] FAILURE PREDICTIONCeph可以根据其收集的健康指标预测预期寿命和设备故障。 共有三种模式： none: 禁用设备故障预测。 local: 使用来自ceph-mgr daemon的预训练预测模型 cloud: 使用ProphetStor运行的外部云服务共享设备运行状况和性能指标，并使用其免费服务或付费服务进行更准确的预测 预测模式可以配置为： 1ceph config set global device_failure_prediction_mode &lt;mode&gt; 预测通常在后台定期进行，因此填充预期寿命值可能需要一些时间。 您可以从以下输出中看到所有设备的预期寿命： 1ceph device ls 您还可以使用以下方法查询特定设备的metadata： 1ceph device info &lt;devid&gt; 您可以使用以下命令显式地强制预测设备的预期寿命： 1ceph device predict-life-expectancy &lt;devid&gt; 如果您未使用Ceph的内部设备故障预测，但是拥有一些有关设备故障的外部信息源，则可以通过以下方式告知Ceph设备的预期寿命： 1ceph device set-life-expectancy &lt;devid&gt; &lt;from&gt; [&lt;to&gt;] 预期寿命以时间间隔表示，因此不确定性可以以宽间隔的形式表示。 间隔结束也可以不指定。 HEALTH ALERTSmgr/devicehealth/warn_threshold控制在生成运行状况警告之前，预期的设备故障必须多长时间。 可以使用以下方法检查所有设备的存储预期寿命，并生成任何适当的健康警报： 1ceph device check-health AUTOMATIC MITIGATION如果启用了mgr/devicehealth/self_heal选项（默认情况下），则对于预计将很快发现故障的设备，模块将通过将设备标记为“out”来自动将数据迁移到这些设备之外。 mgr/devicehealth/mark_out_threshold控制在我们将osd自动标记为“out”之前，预期的设备故障必须多长时间。 原文：https://docs.ceph.com/docs/master/rados/operations/devices/#devices]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph版本升级新特性]]></title>
    <url>%2F2019%2F10%2F16%2FCeph%E7%89%88%E6%9C%AC%E5%8D%87%E7%BA%A7%E6%96%B0%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[V14.2.0 NAUTILUS MAJOR CHANGES FROM MIMIC参考： 【1】https://docs.ceph.com/docs/master/releases/nautilus/ 【2】https://blog.csdn.net/Z_Stand/article/details/89739908 这是Ceph Nautilus的第一个稳定版本。 Dashboard Ceph Dashboard增加了许多新功能： Support for multiple users / roles SSO (SAMLv2) for user authentication Auditing support（审计支持） New landing page, showing more metrics and health info I18N support（国际化） REST API documentation with Swagger API Swagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。总体目标是使客户端和文件系统作为服务器以同样的速度来更新。文件的方法，参数和模型紧密集成到服务器端的代码，允许API来始终保持同步。作者：天马行空LQ 链接：https://www.jianshu.com/p/66a14ea07622来源：简书 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 Ceph management新功能： OSD management (mark as down/out, change OSD settings, recovery profiles) Cluster config settings editor Ceph Pool management (create/modify/delete) ECP management RBD mirroring configuration Embedded Grafana Dashboards (derived from Ceph Metrics) CRUSH map viewer NFS Ganesha management iSCSI target management (via Ceph iSCSI Gateway) RBD QoS configuration Ceph Manager (ceph-mgr) module management Prometheus alert Management 而且，Ceph Dashboard现在被拆分到ceph-mgr-dashboard的package。 如果您的package management software在安装ceph-mgr时失败，则可能需要单独安装ceph-mgr-dashboard。 RADOS 现在可以随时减少每个pool的placement groups (PGs)数，并且可以根据群集利用率或管理员提示自动调整PG数。 新的v2 wire protocol支持线路加密 群集可以跟踪OSD和Monitor daemons消耗的物理存储设备以及运行状况指标（即SMART），并且群集可以通过预先训练的预测模型或者基于云预测服务来报告预测的HDD或SSD故障。 可通过ceph osd numa-status命令轻松监视OSD daemons的NUMA节点，并通过osd_numa_node config选项进行配置。 现在，当使用BlueStore OSD时，空间利用率将按object data，omap data和internal metadata，pool以及压缩前和压缩后的大小进行细分。 在执行recovery和backfill时，OSD可以更有效地选择重要的PG和objects优先处理。 在设备出现问题以后，像recovery这种长期运行在后台的进程，可以使用ceph status命令查看进度。 增加了一个实验性的（耦合层） Coupled-Layer “Clay” erasure code plugin，该plugin可减少大多数recovery操作所需的网络带宽和IO。 RGW S3 lifecycle可以在storage classes与tiering层之间转换 Beast取代了civetweb成为默认的web frontend，从而提高了整体性能。 支持新的publish/subscribe基础架构，允许RGW将events推送至serverless frameworks如knative或data pipelies如Kafka。 新增一系列身份验证功能，使用OAuth2和OpenID::connect的STS联合以及OPA（开放策略代理）身份验证委派原型。 新的archive zone federation功能可将所有objects（包括历史记录）完全保留在一个单独的zone中。 CephFS 对于具有large caches和large RAM并长期运行的客户端，MDS的稳定性已大大提高。Cache trimming（调整）和client capability recall现在受到限制，以防止MDS过载。 现在可以在Rook管理的群集环境中通过NFS-Ganesha导出CephFS。Ceph负责管理集群并确保高可用性和可伸缩性。 入门演示。 预计在未来Nautilus的次要版本中实现此功能的更多自动化。 MDS mds_standby_for_*，mon_force_standby_active和mds_standby_replay配置选项已过时。 相反，操作者现在可以在CephFS文件系统上设置新的allow_standby_replay标志。 该设置会使文件系统由standbys变为standby-replay，并且任何可用的rank都会生效。（一个 rank 可看作是一个元数据分片） MDS支持客户端释放缓存的同时释放自己的存储端缓存，这个过程可由MDS admin socket命令 cache drop来完成 现在可以检查MDS中正在进行的scrub的进度。 此外，scrub可能会暂停或中止。 有关更多信息，请参见scrub文档。 通过ceph volume command-line-interface提供了一个用于创建volumes的新interface。 新的cephfs-shell工具可用于处理CephFS文件系统而无需mounting。 为了简洁，清楚和有用，已重新格式化了来自ceph status与CephFS相关的输出。 Lazy IO已进行了改进。客户端可以使用ceph_open C/C++ API的新CEPH_O_LAZY flag将其打开或通过配置选项client_force_lazyio将其打开。（LazyIO放松了POSIX语义。 即使文件由多个客户端上的多个应用程序打开，也允许缓冲的读/写操作。 应用程序负责自己管理缓存的一致性。自Nautilus发行以来，Libcephfs支持LazyIO。） 现在可以通过ceph fs fail命令快速关闭CephFS文件系统。有关更多信息，请参见 the administration page。 RBD Images可以在尽量短的停机时间内迁移，以帮助在pool之间移动images或移动到新的layouts。 新的rbd perf image iotop和rbd perf image iostat命令为所有RBD images提供了类似于iotop和iostat的IO监视器。 现在，ceph-mgr Prometheus exporter新增一个用于所有RBD images的IO监视器。 支持pool中的单独image namespaces，以便进行租户隔离。 Misc Ceph 拥有一组新的orchestrator modules，可直接与外部orchestrators像ceph-ansible, DeepSea, Rook, or simply ssh通过一致的CLI interface(and, eventually, Dashboard) 进行交互。 V13.2.0 MIMIC MAJOR CHANGES FROM LUMINOUS Dashboard Ceph Luminous中引入的（只读）Ceph manager dashboard已由openATTIC Ceph management tool新实现取代，提供了具有许多其他管理功能。 RADOS 现在，配置选项可以由monitor集中存储和管理。 进行recovery或rebalancing操作时，monitor daemon占用的disk space大大减少。 当OSD从最近的故障中恢复时，异步恢复功能可减少请求的tail latency（少量响应的延迟高于均值，我们把这些响应称为尾延迟）。 scrub时OSD冲突请求抢占tail latency减少。 RGW RGW可以将zone （或subset of buckets）复制到外部云存储服务（例如S3）。 RGW在versioned buckets功能上支持S3 multi-factor authentication API。 AWS Multi-Factor Authentication（MFA）它在用户名和密码的基础上增加了一层额外的保护。启用MFA后，当用户登录AWS网站时，系统将提示他们输入用户名和密码以及来自其AWS MFA设备的身份验证代码。这些因素综合在一起，为您的AWS账户设置和资源提供了更高的安全性。 版本控制是在相同的存储桶中保留对象的多个变量的方法。对于 Amazon S3 桶中存储的每个对象，您可以使用版本控制功能来保存、检索和还原它们的各个版本。使用版本控制能够轻松从用户意外操作和应用程序故障中恢复数据。 Beast frontend不再进行实验，被认为是稳定的并可以使用。 CephFS Snapshots与多个MDS daemons结合使用时，现在稳定。 RBD Image clones不再需要明确的protect和unprotect步骤。 可以将Images deep-copied（包括与parent image和associated snapshots的任何克隆链接）到新pool或修改data layouts。 Misc 由于在Stretch中缺少GCC 8，我们已删除了Mimic的Debian构建。我们希望Debian的构建将在2019年初发布Buster后回归，并希望在Buster可用后构建最终的Luminous发行版（以及以后的Mimic point发行版）。]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AUTOSCALING PLACEMENT GROUPS]]></title>
    <url>%2F2019%2F10%2F15%2FAUTOSCALING-PLACEMENT-GROUPS%2F</url>
    <content type="text"><![CDATA[AUTOSCALING PLACEMENT GROUPSPlacement groups (PGs)是ceph分布数据的内部实现。您可以通过启用pg-autoscaling允许根据集群的使用方式提出建议或自动调整PG。 系统中的每个pool都有一个pg_autoscale_mode属性，可以将其设置为off，on或warn。 off: Disable该pool的autoscaling，Choosing the number of Placement Groups on: 为指定的pool启用PG count自动调整。 warn: 当调整PG count时提出health alerts 要为已有的pool设置autoscaling mode，请执行以下操作： 1ceph osd pool set &lt;pool-name&gt; pg_autoscale_mode &lt;mode&gt; 例如，要对池foo启用autoscaling，请执行以下操作： 1ceph osd pool set foo pg_autoscale_mode on 您还可以使用以下命令配置默认pg_autoscale_mode，该默认pg_autoscale_mode应用于以后创建的任何pool： 1ceph config set global osd_pool_default_pg_autoscale_mode &lt;mode&gt; VIEWING PG SCALING RECOMMENDATIONS您可以使用以下命令查看每个pool，pool的相对利用率以及对PG count的任何建议更改： 1ceph osd pool autoscale-status 输出将类似于： 12345678910POOL SIZE TARGET SIZE RATE RAW CAPACITY RATIO TARGET RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE cephfs_metadata 1540k 3.0 594.0G 0.0000 4.0 8 warn default.rgw.meta 1536k 3.0 594.0G 0.0000 1.0 8 warn cephfs_data 0 3.0 594.0G 0.0000 1.0 8 warn default.rgw.buckets.index 0 3.0 594.0G 0.0000 1.0 8 warn default.rgw.control 0 3.0 594.0G 0.0000 1.0 8 warn yujiang 0 553.2G 1.0 594.0G 0.9313 1.0 512 on .rgw.root 1344k 3.0 594.0G 0.0000 1.0 8 warn rbd 576.0k 3.0 594.0G 0.0000 1.0 4 on default.rgw.log 0 3.0 594.0G 0.0000 1.0 8 warn SIZE是存储在pool中的数据量。TARGET SIZE（如果存在）是管理员希望最终存储在该pool中的数据量。系统使用两个值中的较大者进行计算。 RATE是pool的multiplier（乘数或倍数），它确定要消耗多少raw（原始） storage capacity。例如，3个副本池的比率为3.0，而k=4，m=2擦除编码池的比率为1.5。 RAW CAPACITY是OSD上负责存储该pool（可能还有其他pool）数据的raw storage capacity的总量。RATIO是该pool消耗的总容量的比率（即ratio = size * rate / raw capacity）。 TARGET RATIO（如果存在）是管理员指定他们希望该pool使用的存储空间的比率。系统使用actual ratio和target ratio中的较大者进行计算。 如果同时指定了target size bytes和ratio ，则ratio优先。 PG_NUM是该pool的当前PG数。系统认为应将pool的pg_num更改为NEW PG_NUM（如果存在）。它始终是2的幂，并且仅在“理想”值与当前值的差异大于3倍时才存在。 最后一列，AUTOSCALE，是pool pg_autoscale_mode，on, off或warn。 AUTOMATED SCALING最简单的方法是允许集群根据使用情况自动扩展PG。Ceph将查看可用的总存储量和整个系统的target PG数量，查看每个pool中存储了多少数据并尝试分配相应的PG。该系统的方法相对保守，只有当当前 PG （pg_num） 的数量比它认为的要小 3 倍以上时，才会对pool进行更改。 每个 OSD 的target PG 数基于可配置的 mon_target_pg_per_osd（默认值：100），可通过以下功能进行调整： 1ceph config set global mon_target_pg_per_osd 100 autoscaler根据每个per-subtree分析pool并进行调整。由于每个pool可能映射到不同的 CRUSH rule，并且每个rule可以跨不同的设备分发数据，所以Ceph将考虑独立使用层次结构的每个subtree。例如，映射到ssd类的OSD的pool和映射到hdd类的OSD的pool将分别具有最佳PG counts，具体取决于这些相应设备类型的数量。 SPECIFYING EXPECTED POOL SIZE（指定预期的pool大小）首次创建集群或pool时，它将占用集群总容量的一小部分，并在系统中显示只需要少量的placement groups。但是，在大多数情况下，集群管理员最好知道哪些pool会随着时间的推移消耗大部分系统容量。通过向ceph提供这些信息，可以从一开始就使用更适当数量的pg，从而防止pg-num中的后续更改以及在进行调整时与移动数据相关的开销。 pool的target size*可通过两种方式指定：按pool的绝对大小（即字节）或群集总容量的ratio（比率）指定。 例如： 1ceph osd pool set mypool target_size_bytes 100T 会告诉系统mypool预计会占用100 TiB的空间。 或者： 1ceph osd pool set mypool target_size_ratio .9 告诉系统mypool预计会消耗群集总容量的90％。 您还可以使用ceph osd pool create命令的可选--target-size-bytes &lt;bytes&gt;或--target-size-ratio &lt;ratio&gt;参数在创建时设置pool的target size。 请注意，如果指定了不可能的target size值（例如，容量大于整个群集的容量或ratio(s)之和大于1.0），则会引发health警告（POOL_TARET_SIZE_RATIO_OVERCOMMITTED或POOL_TARGET_SIZE_BYTES_OVERCOMMITTED）。https://www.mail-archive.com/ceph-users@lists.ceph.com/msg56416.html SPECIFYING BOUNDS ON A POOL’S PGS（在pool的PGS上指定界限）也可以为一个pool指定最小数量的PG。设置下限可防止Ceph将PG编号减少（或建议减少）到配置的编号以下。 您可以使用以下方法设置pool的最小PG数量： 1ceph osd pool set &lt;pool-name&gt; pg_num_min &lt;num&gt; 您还可以使用ceph osd pool create命令的可选--pg-num-min &lt;num&gt;参数指定创建pool时的最小PG count。 A PRESELECTION OF PG_NUM使用以下方法创建新pool时： 1ceph osd pool create &#123;pool-name&#125; [pg_num] 选择pg_num的值是可选的。 如果您未指定pg_num，则集群可以（默认情况下）根据pool中存储的数据为您自动对其进行调整（请参见上文， Autoscaling placement groups）。 或者，可以显式提供pg_num。 但是，是否指定pg_num值并不影响群集是否自动调整该值。 要启用或禁用自动调整，请执行以下操作： 1ceph osd pool set &#123;pool-name&#125; pg_autoscaler_mode (on|off|warn) 传统上，每个OSD PG的”rule of thumb”为100。使用balancer的附加功能（默认情况下也启用的），每个OSD大约50 PG可能是合理的。autoscaler通常为您提供： 使每个pool中的PG与pool中的数据成比例 考虑到每个PG在OSD上的replication或erasuring-coding，最终每个OSD会有50-100个PG HOW ARE PLACEMENT GROUPS USED（如何使用PLACEMENT GROUPS）placement group (PG)聚集pool中的objects，因为以每个object为基础跟踪object placement和object metadata在计算上是昂贵的，即，具有数百万个object的系统无法实际以每个object为基础跟踪placement。 Ceph客户端将计算object应位于哪个placement group中。它通过hashing object ID并根据定义的pool中PG的数量和pool ID进行操作来实现此目的。有关详细信息，请参见 Mapping PGs to OSDs。 placement group中object的内容存储在一组OSD中。 例如，在大小为2的replicated pool中，每个placement group将在两个OSD上存储objects，如下所示。 如果OSD #2失败，则将另一个分配给Placement Group #1，并用OSD #1中所有objects的副本填充。 如果pool大小从2更改为3，则会将一个额外的OSD分配给该placement group，并将接收该placement group中所有objects的副本。 Placement groups不拥有OSD； 他们与同一资源pool甚至其他资源pool中的其他placement groups共享它。 如果OSD #2失败，则Placement Group #2还必须使用OSD #3恢复objects的副本。 当placement groups的数量增加时，将为新的placement groups分配OSD。CRUSH函数的结果也将更改，并且先前placement groups中的某些objects将被复制到新的placement groups中，并从旧的placement groups中删除。 PLACEMENT GROUPS TRADEOFFS（权衡）数据持久性以及所有OSD之间的均匀分配都需要更多的placement groups，但应将其数量减少到最少，以节省CPU和内存。 DATA DURABILITY（数据持久性）OSD发生故障后，数据丢失的风险会增加，直到完全恢复其中包含的数据为止。 假设有一种情况会导致单个placement group中的数据永久丢失： OSD失败，并且它包含的object的所有副本均丢失。对于placement group中的所有objects，副本的数量突然从3个减少到2个。 Ceph通过选择一个新的OSD重新创建所有objects的第三个副本，开始对该placement group的恢复。 在同一placement group内的另一个OSD在新OSD完全填充第三份副本之前发生故障。 这样，某些objects将只有一个幸存副本。 Ceph选择了另一个OSD并保持复制objects以恢复所需的副本数。 在同一placement group中的第三个OSD在恢复完成之前发生故障。 如果此OSD包含object的唯一剩余副本，则它将永久丢失。 在三个副本pool中包含10个OSD和512个placement groups的集群中，CRUSH将为每个placement groups提供三个OSD。 最后，每个OSD将托管(512 * 3) / 10 = ~150 Placement Groups。 当第一个OSD发生故障时，以上情况将同时启动所有150个placement groups的恢复。 恢复的150个placement groups可能均匀分布在剩余的9个OSD上。 因此，每个剩余的OSD可能会将objects的副本发送给所有其他objects，并且还可能会接收一些要存储的新objects，因为它们已成为新placement group的一部分。 完成恢复所需的时间完全取决于Ceph集群的架构。 假设每个OSD由一台机器上的1TB SSD托管，并且所有OSD都连接到10Gb/s交换机，并且单个OSD的恢复在M分钟内完成。 如果每台计算机使用不带SSD journal的spinners和1Gb/s交换机的两个OSD，则速度至少要慢一个数量级。 在这种大小的集群中，placement groups的数量几乎对数据持久性没有影响。 可能是128或8192，恢复速度不会变慢或变快。 但是，将相同的Ceph集群增加到20个OSD而不是10个OSD可能会加快恢复速度，从而显着提高数据的持久性。 现在，每个OSD只能参与约75个placement groups，而不是只有10个OSD时的约150个placement groups，并且仍然需要全部19个剩余OSD执行相同数量的object副本才能恢复。 但是，如果10个OSD必须每个复制大约100GB，则现在它们必须每个复制50GB。 如果网络是瓶颈，恢复将以两倍的速度进行。 换句话说，当OSD数量增加时，恢复速度会更快。 如果该群集增长到40个OSD，则每个OSD将仅托管约35个placement groups。 如果OSD死亡，则恢复将保持更快的速度，除非它被另一个瓶颈阻塞。 但是，如果该集群增长到200个OSD，则每个OSD将仅托管约7个placement groups。 如果OSD死亡，则将在这些placement groups中的最多约21 (7 * 3)个OSD之间进行恢复：恢复将比有40个OSD时花费的时间更长，这意味着应增加placement groups的数量。 无论恢复时间有多短，第二个OSD在进行过程中都有可能发生故障。 在上述10个OSD群集中，如果它们中的任何一个失败，则〜17个placement groups（即，正在恢复〜150/9个placement groups）将只有一个幸存副本。 并且如果剩余的8个OSD中的任何一个失败，则两个placement groups的最后一个objects很可能会丢失（即〜17/8个placement groups，仅恢复了一个剩余副本）。 当群集的大小增加到20个OSD时，丢失三个OSD损坏的Placement Groups的数量将减少。 第二个OSD丢失将降级〜4个（即恢复到约75个/ 19个Placement Groups），而不是〜17个，而第三个OSD丢失则仅在它是包含尚存副本的四个OSD之一时才丢失数据。 换句话说，如果在恢复时间范围内丢失一个OSD的概率为0.0001％，则它从具有10个OSD的群集中的17 10 0.0001％变为具有20个OSD的群集中的4 20 0.0001％。 简而言之，更多的OSD意味着恢复更快，较低的级联故障风险，从而导致Placement Group的永久丢失。 就数据持久性而言，在少于50个OSD的群集中，具有512或4096个Placement Group大致等效。 注意：向集群添加的新OSD可能需要很长时间才能分配有分配给它的placement groups。 但是，不会降低任何object的质量，也不会影响集群中包含的数据的持久性。 OBJECT DISTRIBUTION WITHIN A POOL（pool内的object分布）理想情况下，object均匀地分布在每个placement group中。 由于CRUSH计算每个object的placement group，但实际上不知道该placement group内每个OSD中存储了多少数据，因此placement group数与OSD数之比可能会显着影响数据的分布。 例如，如果在三个副本pool中有一个用于十个OSD的placement group，则仅使用三个OSD，因为CRUSH别无选择。 当有更多的placement group可用时，object更有可能在其中均匀分布。 CRUSH还尽一切努力在所有现有的placement group中平均分配OSD。 只要Placement Groups比OSD多一个或两个数量级，则分布应该均匀。 例如，用于3个OSD的256个Placement Groups，用于10个OSD的512或1024个Placement Groups等。 数据分布不均可能是由OSD与placement groups之间的比率以外的因素引起的。 由于CRUSH未考虑object的大小，因此一些非常大的object可能会造成不平衡。 假设有100万个4K object（总计4GB）均匀分布在10个OSD的1024个placement groups中。 他们将在每个OSD上使用4GB / 10 = 400MB。 如果将一个400MB object添加到pool中，则支持放置object的placement groups的三个OSD将填充400MB + 400MB = 800MB，而其余七个将仅占据400MB。 MEMORY, CPU AND NETWORK USAGE（内存，CPU和网络使用情况）对于每个placement group，OSD和MON始终需要内存，网络和CPU，并且在恢复期间甚至更多。 通过对placement group内的object进行聚类objects来共享此开销是它们存在的主要原因之一。 最小化placement groups的数量可以节省大量资源。 CHOOSING THE NUMBER OF PLACEMENT GROUPS（选择PLACEMENT GROUPS的数量）如果您有超过50个OSD，我们建议每个OSD大约有50-100个placement groups，以平衡资源使用，数据持久性和分发。 如果OSD少于50个，则最好在上述preselection中进行选择。 对于单个objects pool，可以使用以下公式获取baseline： 123 (OSDs * 100)Total PGs = ------------ pool size pool size是replicated pools的副本数或erasure coded pools的K + M总和（由ceph osd erasure-code-profile get返回）。 然后，您应该检查设计Ceph集群的方式，以最大程度地提高数据持久性，对象分配并最小化资源使用。 结果应始终四舍五入到最接近的2的幂。 只有2的幂可以平衡placement groups之间的objects数量。 其他值将导致OSD上的数据分布不均。 例如，对于具有200个OSD和3个副本的pool大小的集群，您可以按以下方式估计PG的数量： 123(200 * 100)----------- = 6667. Nearest power of 2: 8192 3 当使用多个data pools存储objects时，需要确保在每个pool的placement groups数量与每个OSD的placement groups数量之间取得平衡，以便获得合理的placement groups总数，从而为每个OSD提供合理的低偏差而不会增加系统资源的负担或使对等进程太慢。 例如，一个10个pool的集群，每个pool在十个OSD上具有512个placement groups，则总共有5120个placement groups分布在十个OSD上，即每个OSD 512个placement groups。 那不会使用太多资源。 但是，如果创建了1000个pool，每个pool有512个placement groups，则OSD将分别处理约50,000个placement groups，这将需要更多的资源和时间来进行对等。 您可能会发现PGCalc工具很有帮助。 SET THE NUMBER OF PLACEMENT GROUPS（设置PLACEMENT GROUPS数）要设置pool中的placement groups数量，必须在创建pool时指定placement groups的数量。有关详细信息，请参见Create a Pool。 即使在创建pool之后，您也可以使用以下方法更改placement groups的数量： 1ceph osd pool set &#123;pool-name&#125; pg_num &#123;pg_num&#125; 增加placement groups的数量之后，还必须增加placement（pgp_num）的数量，然后集群才能重新平衡。 pgp_num将是CRUSH算法考虑placement的placement groups的数量。 pg_num的增加会拆分placement groups，但是数据将不会迁移到较新的placement groups，直到placement的placement groups为止。 pgp_num增加了。 pgp_num应该等于pg_num。 要增加用于placement的placement groups的数量，请执行以下操作： 1ceph osd pool set &#123;pool-name&#125; pgp_num &#123;pgp_num&#125; 当减少PG的数量时，将自动为您调整pgp_num。 GET THE NUMBER OF PLACEMENT GROUPS（获取PLACEMENT GROUPS数）要获取pool中的placement groups数，请执行以下操作： 1ceph osd pool get &#123;pool-name&#125; pg_num GET A CLUSTER’S PG STATISTICS（获取集群的PG统计信息）要获取集群中placement groups的统计信息，请执行以下操作： 1ceph pg dump [--format &#123;format&#125;] 有效格式为plain（默认）和json。 GET STATISTICS FOR STUCK PGS（获取STUCK PGS的统计信息）要获取所有处于指定状态的placement groups的统计信息，请执行以下操作： 1ceph pg dump_stuck inactive|unclean|stale|undersized|degraded [--format &lt;format&gt;] [-t|--threshold &lt;seconds&gt;] Inactive Placement groups无法处理读写，因为它们正在等待OSD包含最新数据。 Unclean Placement groups包含的object未复制所需的次数。。 他们应该正在恢复。 Stale Placement groups处于未知状态—承载这些PG的OSD在一段时间内未向monitor报告（由mon_osd_report_timeout配置）。 有效格式为plain（默认）和json。 阈值定义placement group在将其包括在返回的统计信息之前卡住的最小秒数（默认为300秒）。 GET A PG MAP要获取特placement group map，请执行以下操作： 1ceph pg map &#123;pg-id&#125; 例如： 1ceph pg map 1.6c Ceph将返回placement group map，placement group和OSD状态： 1osdmap e13 pg 1.6c (1.6c) -&gt; up [1,0] acting [1,0] GET A PGS STATISTICS（获取PGS统计信息）要检索特定placement group的统计信息，请执行以下操作： 1ceph pg &#123;pg-id&#125; query SCRUB A PLACEMENT GROUP要scrub a placement group，请执行以下操作： 1ceph pg scrub &#123;pg-id&#125; Ceph检查primary和任何replica nodes生成的placement group中所有objects的目录进行比较，以确保没有丢失或不匹配的objects，并且它们的内容一致。 假设所有副本都匹配，则扫描可确保所有与snapshot-related的object metadata都是一致的。 通过日志报告错误。 要清理特定pool中的所有placement groups，请执行以下操作： 1ceph osd pool scrub &#123;pool-name&#125; PRIORITIZE BACKFILL/RECOVERY OF A PLACEMENT GROUP(S)（优先考虑PLACEMENT GROUP的BACKFILL/RECOVERY）您可能会遇到这样的情况，即一堆placement groups需要recovery和/或backfill，并且某些特定的groups保存的数据比其他的更为重要（例如，那些PG可能保存正在运行的机器使用的images的数据，而其他PG可能由不活动的机器使用/较少的相关数据）。 在这种情况下，您可能希望优先考虑恢复这些groups，以便更早恢复存储在这些groups上的数据的性能和/或可用性。 为此（在backfill或recovery期间将特定的placement group(s)标记为优先），请执行以下操作： 12ceph pg force-recovery &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]ceph pg force-backfill &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...] 这将导致Ceph首先在其他placement groups之前对指定的placement groups执行recovery或backfill。 这不会中断当前正在进行的backfill或recovery，但会导致尽快处理指定的PG。 如果您改变主意或优先考虑wrong groups，请使用： 12ceph pg cancel-force-recovery &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]ceph pg cancel-force-backfill &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...] 这将从这些PG中删除“force” flag，并将以默认顺序对其进行处理。 同样，这不会影响当前正在处理的placement groups，只会影响仍在排队的placement groups。 recovery或backfill group后，将自动清除“force” flag。 同样，您可以使用以下命令强制Ceph首先对指定pool中的所有placement groups执行recovery或backfill： 12ceph osd pool force-recovery &#123;pool-name&#125;ceph osd pool force-backfill &#123;pool-name&#125; 或 12ceph osd pool cancel-force-recovery &#123;pool-name&#125;ceph osd pool cancel-force-backfill &#123;pool-name&#125; 如果您改变主意，则可以恢复到默认的recovery或backfill优先级。 请注意，这些命令可能会破坏Ceph内部优先级计算的顺序，因此请谨慎使用！ 特别是，如果您有多个当前共享相同底层OSD的pool，并且某些特定pool中的数据比其他pool更重要，我们建议您使用以下命令以更好的顺序重新排列所有pool的recovery/backfill优先级： 1ceph osd pool set &#123;pool-name&#125; recovery_priority &#123;value&#125; 例如，如果您有10个pool，则可以将最重要的一个优先级设置为10，下一个9，等等。或者您可以不理会大多数pool，而说3个重要的pool分别设置为优先级1或优先级3、2、1。 REVERT LOST（永不消失）如果集群丢失了一个或多个object，并且您决定放弃对丢失数据的搜索，则必须将unfound的object标记为lost。 如果已查询所有可能的位置并且仍然丢失了objects，则可能必须放弃丢失的objects。 当前唯一受支持的选项是“revert”，它可以回滚到该object的先前版本，或者（如果是新object）则完全忘记它。 要将“unfound”的object标记为“lost”，请执行以下操作： 1ceph pg &#123;pg-id&#125; mark_unfound_lost revert|delete 重要说明：请谨慎使用此功能，因为它可能会使期望object存在的应用程序感到困惑。]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph 问题列表]]></title>
    <url>%2F2019%2F10%2F14%2Fceph-%E9%97%AE%E9%A2%98%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[问题列表Ceph升级（L to N）引发的问题 问题ID 1 问题出现版本 pre-14.2.3 问题现象 …POOLS: POOL ID STORED OBJECTS USED %USED MAX AVAIL data 0 63 TiB 44.59M 63 TiB 30.21 48 TiB …but when one OSD was updated it changed toPOOLS: POOL ID STORED OBJECTS USED %USED MAX AVAIL data 0 558 GiB 43.50M 1.7 TiB 1.22 45 TiB 问题触发条件 1、从nautilus之前的集群进行了升级2、然后，您提供一个或多个新的BlueStore OSD，或在升级的OSD上运行“ceph-bluestore-tool repair”。 问题原因 根本原因是，从Nautilus开始，BlueStore维护了每个池的使用情况统计信息，但是它需要对磁盘上的格式进行少量更改。除非您运行ceph-bluestore-tool修复，否则升级后的OSD不会拥有新的统计信息。问题在于，一旦 any OSD报告了er-pool统计信息，mon就开始使用新的统计信息（而不是等到 all OSD都在这样做）。 问题解决办法 为避免此问题，可以1、升级后不要置备新的BlueStore OSD2、更新所有OSD，以保留新的每个池统计信息。现有的BlueStore OSD可以通过以下方式转换： systemctl stop ceph-osd@$N ceph-bluestore-tool repair –path /var/lib/ceph/osd/ceph-$N systemctl start ceph-osd@$N请注意，FileStore根本不支持新版每个池统计信息，因此，如果集群中有文FileStore OSD，则没有解决方法。无需将文件存储OSD替换为bluestore。修复程序[1]正在通过QA检查，将在14.2.3中出现； 它不会在14.2.2完整发布。 ceph-users地址 http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/035889.htmlhttps://github.com/ceph/ceph/pull/28978https://tracker.ceph.com/versions/574 备注 实践修复 问题修复版本（社区计划） 14.2.3 问题ID 2 问题出现版本 14.2.2 问题现象 Legacy BlueStore stats reporting detected on 6 OSD(s) 问题触发条件 1、从nautilus之前的集群进行了升级 问题原因 问题解决办法 systemctl stop ceph-osd@$OSDIDceph-bluestore-tool repair –path /var/lib/ceph/osd/ceph-$OSDIDsystemctl start ceph-osd@$OSDID ceph-users地址 http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/036010.html 备注 可以静默告警 bluestore warn on legacy statfs = false 实践修复 [root@ceph1 ~]# systemctl stop ceph-osd@1.service[root@ceph1 ~]# ceph-bluestore-tool repair –path /var/lib/ceph/osd/ceph-1/2019-10-14 15:39:53.940 7f87c8114f80 -1 bluestore(/var/lib/ceph/osd/ceph-1) fsck error: legacy statfs record found, removing2019-10-14 15:39:53.940 7f87c8114f80 -1 bluestore(/var/lib/ceph/osd/ceph-1) fsck error: missing Pool StatFS record for pool 22019-10-14 15:39:53.940 7f87c8114f80 -1 bluestore(/var/lib/ceph/osd/ceph-1) fsck error: missing Pool StatFS record for pool ffffffffffffffffrepair success[root@ceph1 ~]# systemctl start ceph-osd@1.service 问题修复版本（社区计划） 问题ID 3 问题出现版本 14.2.2 问题现象 Legacy BlueStore stats reporting detected on 6 OSD(s) 问题触发条件 1、从nautilus之前的集群进行了升级 问题原因 问题解决办法 systemctl stop ceph-osd@$OSDIDceph-bluestore-tool repair –path /var/lib/ceph/osd/ceph-$OSDIDsystemctl start ceph-osd@$OSDID ceph-users地址 http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/036002.html 备注 可以静默告警 bluestore warn on legacy statfs = false 实践修复 问题修复版本（社区计划） 问题ID 4 问题出现版本 14.2.4 问题现象 3 monitors have not enabled msgr2告警 问题触发条件 1、从nautilus之前的集群进行了升级 问题原因 messenger v2 protocol（msgr2）是Ceph’s on-wire protocol第二次主要修订。 问题解决办法 ceph mon enable-msgr2systemctl restart ceph-mon@ceph1.service ceph-users地址 备注 实践修复 ceph mon enable-msgr2systemctl restart ceph-mon@ceph1.service 问题修复版本（社区计划） 问题ID 5 问题出现版本 问题现象 问题触发条件 问题原因 问题解决办法 ceph-users地址 备注 实践修复 问题修复版本（社区计划） 问题ID 问题出现版本 问题现象 问题触发条件 问题原因 问题解决办法 ceph-users地址 备注 实践修复 问题修复版本（社区计划） 问题ID 问题出现版本 问题现象 问题触发条件 问题原因 问题解决办法 ceph-users地址 备注 实践修复 问题修复版本（社区计划）]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph MESSENGER V2]]></title>
    <url>%2F2019%2F10%2F14%2FCeph-MESSENGER-V2%2F</url>
    <content type="text"><![CDATA[WHAT IS ITmessenger v2 protocol（msgr2）是Ceph’s on-wire protocol第二次主要修订。它具有几个关键功能： 安全模式，对通过网络传递的所有数据进行加密 改进了authentication payloads的封装，未来可以集成新的authentication模式（例如Kerberos） 改进了早期的advertisement和negotiation（协商）功能，支持未来的protocol（协议）修订 Ceph daemons现在可以绑定到多个端口，从而允许旧版Ceph客户端和支持v2的新客户端连接到同一集群。 默认情况下，monitors现在绑定到新的v2协议的新IANA分配端口3300（ce4h或0xce4），同时还绑定到旧的默认端口6789（旧的v1协议）。 ADDRESS FORMATS在nautilus之前，所有网络地址都呈现为1.2.3.4:567/89012，其中有一个IP地址，一个端口和一个随机数，以唯一地标识网络上的客户端或daemon 。 从nautilus开始，我们现在具有三种不同的地址类型： v2：v2:1.2.3.4:578/89012标识daemon绑定到新v2协议的端口 v1：v1:1.2.3.4:578/89012标识绑定到旧版v1协议的端口的daemon。 以前使用任何前缀显示的任何地址现在都显示为v1: address。 TYPE_ANY地址标识表示客户端可以支持两种协议版本。 在nautilus之前，客户端将显示为1.2.3.4:0/123456，其中端口0表示它们是客户端，并且不接受连接。 从Nautilus开始，这些客户端现在在内部由TYPE_ANY address表示，并且仍显示为不带前缀，因为它们可能会使用v2或v1协议连接到daemons，具体取决于daemons使用的协议。 因为daemons现在绑定到多个端口，所以现在用地址向量而不是单个地址来描述它们。 例如，在Nautilus cluster上dump monitor map会有如下输出： 123456789[root@ceph1 ~]# ceph mon dumpepoch 1fsid 50fcf227-be32-4bcb-8b41-34ca8370bd16last_changed 2019-02-25 11:10:46.700821created 2019-02-25 11:10:46.700821min_mon_release 14 (nautilus)0: [v2:10.0.0.10:3300/0,v1:10.0.0.10:6789/0] mon.foo1: [v2:10.0.0.11:3300/0,v1:10.0.0.11:6789/0] mon.bar2: [v2:10.0.0.12:3300/0,v1:10.0.0.12:6789/0] mon.baz 方括号或地址向量表示可以在多个端口（和协议）上访问同一daemon。如果可能，任何连接到该daemon的客户端或其他daemon都将使用v2协议（listed first）； 否则，它将返回到旧版v1协议。 旧版客户端将仅看到v1地址，并且将继续使用v1协议进行连接。 从Nautilus开始，mon_host配置选项和-m &lt;mon-host&gt;命令行选项支持相同的带括号的地址向量语法。 BIND CONFIGURATION OPTIONS两个新的配置选项控制是否使用v1 and/or v2协议： ms_bind_msgr1 [default: true]控制daemon是否绑定到使用v1协议的端口 ms_bind_msgr2 [default: true]控制daemon是否绑定到使用v2协议的端口 同样，两个选项控制是否使用IPv4和IPv6地址： ms_bind_ipv4 [默认值：true]控制daemon是否绑定到IPv4地址 ms_bind_ipv6 [默认值：false]控制daemon是否绑定到IPv6地址 CONNECTION MODESv2协议支持两种连接模式： crc模式提供： 建立连接时进行强大的初始身份验证（通过cephx，双方相互认证，防止中间人或窃听者进入） CRC32C完整性检查，以防止由于flaky hardware或cosmic rays引起的位翻转 crc模式不提供： 加密（网络上的窃听者可以看到所有经过身份验证后的流量） 免受恶意中间人的攻击（只要他们调整crc32c的值以使其匹配，就可以故意修改流量） secure模式提供： 建立连接时进行强大的初始身份验证（通过cephx，双方相互认证，防止中间人或窃听者进入） 对所有认证后流量完全加密，包括加密完整性检查。 在Nautilus中，secure模式使用AES-GCM stream cipher，这在现代处理器上通常非常快（例如，比SHA-256 cryptographic hash更快）。 CONNECTION MODE CONFIGURATION OPTIONS对于大多数连接，有一些选项可以控制使用哪种模式： ms_cluster_mode用于Ceph daemons之间的集群内通信的连接模式（或允许的模式）。 如果列出了多个模式，则首选第一个列出的模式。 ms_service_mode是客户端连接到群集时允许使用的模式的列表。 ms_client_mode是按优先顺序排列的连接模式列表，供客户端在与Ceph群集通信时使用（或允许）。 有一组并行的选项专门适用于monitors，允许管理员设置与monitors通信的不同（通常更安全）要求。 ms_mon_cluster_mode是monitors之间使用的连接模式（或允许的模式）。 ms_mon_service_mode是客户端或其他Ceph daemons连接到monitors时允许使用的模式的列表。 ms_mon_client_mode是按优先顺序排列的连接模式列表，供客户端或non-monitor daemons在连接monitors时使用。 TRANSITIONING FROM V1-ONLY TO V2-PLUS-V1默认情况下，从Nautilus 14.2.z开始，ms_bind_msgr2为true。 但是，在monitors开始使用v2之前，只有有限的服务可以使用v2地址。 对于大多数用户，monitors已绑定到v1协议的默认旧版端口6789。 在这种情况下，启用v2非常简单： 1ceph mon enable-msgr2 如果monitors绑定到non-standard端口，则需要为v2明确指定其端口。例如，如果monitors mon.a绑定到1.2.3.4:1111，并且您想要在端口1112上添加v2，则： 1ceph mon set-addrs a [v2:1.2.3.4:1112,v1:1.2.3.4:1111] monitors绑定到v2后，每个daemon将在下一次重新启动时开始使用v2地址。 UPDATING CEPH.CONF AND MON_HOST在Nautilus之前，CLI用户或daemon通常将通过/etc/ceph/ceph.conf中的mon_host选项发现monitors。 此选项的语法从Nautilus开始扩展，以允许支持新的方括号列表格式。 例如，像这样的旧行： 1mon_host = 10.0.0.1:6789,10.0.0.2:6789,10.0.0.3:6789 可以更改为： 1mon_host = [v2:10.0.0.1:3300/0,v1:10.0.0.1:6789/0],[v2:10.0.0.2:3300/0,v1:10.0.0.2:6789/0],[v2:10.0.0.3:3300/0,v1:10.0.0.3:6789/0] 但是，使用默认端口（3300和6789）时，可以将其省略： 1mon_host = 10.0.0.1,10.0.0.2,10.0.0.3 一旦在monitors上启用了v2，可能需要更新ceph.conf以不指定任何端口（这通常是最简单的），或者显式指定v2和v1地址。 但是请注意，Nautilus和更高版本才能理解新的带括号语法，因此请不要在尚未升级其ceph packages的主机上进行此更改。 当更新ceph.conf时，请注意新的ceph config generate-minimal-conf命令（它生成一个简单的配置文件，其中包含足够的信息来访问monitors）而ceph config assimilate-conf（将配置文件选项移动到monitors’配置数据库中）可能会有所帮助。 例如，： 12345678# ceph config assimilate-conf &lt; /etc/ceph/ceph.conf# ceph config generate-minimal-config &gt; /etc/ceph/ceph.conf.new# cat /etc/ceph/ceph.conf.new# minimal ceph.conf for 0e5a806b-0ce5-4bc6-b949-aa6f68f5c2a3[global] fsid = 0e5a806b-0ce5-4bc6-b949-aa6f68f5c2a3 mon_host = [v2:10.0.0.1:3300/0,v1:10.0.0.1:6789/0]# mv /etc/ceph/ceph.conf.new /etc/ceph/ceph.conf PROTOCOL有关v2 wire protocol的详细说明，请参见msgr2 protocol。]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UPGRADING CEPH]]></title>
    <url>%2F2019%2F10%2F12%2FUPGRADING-CEPH%2F</url>
    <content type="text"><![CDATA[SUMMARY您可以在群集处于online并in service状态时升级Ceph群集中的daemons！某些类型的daemons依赖于其他daemon。例如，CephMetadata Servers和Ceph Object Gateways依赖于CephMonitors和Ceph OSD Daemons。我们建议按以下顺序升级： 1、Ceph Deploy 2、Ceph Monitors 3、Ceph OSD Daemons 4、Ceph Metadata Servers 5、Ceph Object Gateways 通常，我们建议升级所有daemons以确保它们都在同一版本上（例如，所有ceph-mon daemons，所有ceph-osd daemons等）。我们还建议您先升级集群中的所有daemons，然后再尝试使用新功能。 Upgrade Procedures比较简单，在升级之前需要查看release notes document of your release。基础过程包括三个步骤： 1、 在admin节点上使用ceph-deploy为多个host升级packages（使用ceph-deploy install命令），或者登陆到每个host使用package manager升级Ceph package。例如，在升级Monitors时，ceph-deploy语法可能如下所示： 123ceph-deploy install --release &#123;release-name&#125; ceph-node1[ ceph-node2]ceph-deploy install --release firefly mon1 mon2 mon3 注意：ceph-deploy install命令会将指定节点中的packages从旧版本升级到你指定的版本。（该工具不存在ceph-deploy upgrade命令） 2、 登陆到每个Ceph节点，然后重新启动每个Ceph daemon，有关详细信息，请参见Operating a Cluster。 3、 确保您的群集healthy。有关详细信息，请参见Monitoring a Cluster。 重要说明：升级daemon后，将无法降级。 CEPH DEPLOY在升级Ceph daemons之前，先升级ceph-deploy工具。 1sudo pip install -U ceph-deploy 或 1sudo apt-get install ceph-deploy 或 1sudo yum install ceph-deploy python-pushy UPGRADE PROCEDURES以下各节描述了升级过程。 重要说明：每个Ceph版本可能都有一些其他步骤。在开始升级daemons之前，请详细阅读release notes document of your release。 UPGRADING MONITORS要升级monitors，执行以下步骤： 1、为每个daemon instance升级Ceph package。 可以使用ceph-deploy一次指定所有的monitor节点。例如： 12ceph-deploy install --release &#123;release-name&#125; ceph-node1[ ceph-node2]ceph-deploy install --release hammer mon1 mon2 mon3 你也可以在每个节点上使用distro’s package manager。对于Debian/Ubuntu，在每个主机上执行以下步骤： 12ssh &#123;osd-host&#125;sudo apt-get update &amp;&amp; sudo apt-get install ceph 对于CentOS/Red Hat，执行以下步骤： 12ssh &#123;osd-host&#125;sudo yum update &amp;&amp; sudo yum install ceph 2、重新启动每个monitor，对于Ubuntu 1sudo restart ceph-mon id=&#123;hostname&#125; 对于CentOS/Red Hat/Debian，使用： 1sudo /etc/init.d/ceph restart &#123;mon-id&#125; 对于CentOS/Red Hat通过ceph-deploy部署的集群，monitor ID通常为mon.{hostname}。 3、确保每个monitor都重新quorum 1ceph mon stat 确保你已完成所有Ceph Monitor的升级步骤。 UPGRADING AN OSD要升级Ceph OSD Daemon，请执行以下步骤： 1、 升级Ceph OSD Daemon package。 可以使用ceph-deploy一次指定所有的monitor节点。例如： 12ceph-deploy install --release &#123;release-name&#125; ceph-node1[ ceph-node2]ceph-deploy install --release hammer osd1 osd2 osd3 你也可以在每个节点上使用distro’s package manager。对于Debian/Ubuntu，在每个主机上执行以下步骤： 12ssh &#123;osd-host&#125;sudo apt-get update &amp;&amp; sudo apt-get install ceph 对于CentOS / Red Hat，执行以下步骤： 12ssh &#123;osd-host&#125;sudo yum update &amp;&amp; sudo yum install ceph 2、重新启动OSD，其中N是OSD number。对于Ubuntu，请使用： 1sudo restart ceph-osd id=N 对于主机上的多个OSD，可以使用Upstart重新启动所有OSD。 1sudo restart ceph-osd-all 对于CentOS/Red Hat/Debian，使用： 1sudo /etc/init.d/ceph restart N 3、确保每个升级的Ceph OSD Daemon都已重新加入集群： 1ceph osd stat 确保你已完成所有Ceph OSD Daemons的升级步骤。 UPGRADING A METADATA SERVER要升级Ceph Metadata Server，请执行以下步骤： 1、升级Ceph Metadata Server package。可以使用ceph-deploy一次指定所有的Ceph Metadata Server节点，或在每个节点上使用package manager。例如： 12ceph-deploy install --release &#123;release-name&#125; ceph-node1ceph-deploy install --release hammer mds1 要手动升级packages，请在每个Debian/Ubuntu节点上执行以下步骤： 12ssh &#123;mon-host&#125;sudo apt-get update &amp;&amp; sudo apt-get install ceph-mds 或在CentOS/Red Hat节点上执行： 12ssh &#123;mon-host&#125;sudo yum update &amp;&amp; sudo yum install ceph-mds 2、重新启动metadata server。对于Ubuntu，请使用： 1sudo restart ceph-mds id=&#123;hostname&#125; 对于CentOS/Red Hat/Debian，使用： 1sudo /etc/init.d/ceph restart mds.&#123;hostname&#125; 对于使用ceph-deploy部署的集群，name通常是您在创建时指定的name或hostname。 3、确保metadata server已启动并正在运行： 1ceph mds stat UPGRADING A CLIENT升级packages并在Ceph集群上重新启动daemons后，我们建议您也升级client节点上的ceph-common和client libraries（librbd1和librados2）。 1、 升级package。 12ssh &#123;client-host&#125;apt-get update &amp;&amp; sudo apt-get install ceph-common librados2 librbd1 python-rados python-rbd 2、确保已升级为新版本 1ceph --version 如果没有升级为最新版本，则需要卸载，auto remove dependencies并重新安装。 实践升级Ceph服务端package升级可能带来的影响12（1）服务端升级可能会影响客户业务，导致业务中断一定时间。如果cephfs或rgw前端有流量（install ceph package时会自动停掉mds与rgw service），请先将升级节点流量先转向其他节点。（2）如果容器中有ceph client packages，同样需要逐一转走流量，再升级 原文 UPGRADING FROM PRE-LUMINOUS RELEASES (LIKE JEWEL)您必须先升级到Luminous（12.2.z），然后再尝试升级到Nautilus。 另外，您的集群必须在运行Luminous的同时至少完成了所有PG的一次scrub，并在OSD map中设置了recovery_deletes和purged_snapdirs标志。 UPGRADING FROM MIMIC OR LUMINOUSNOTES 123在从Luminous升级到Nautilus的过程中，将monitors升级到Nautilus后，将无法使用Luminous ceph-osd daemon创建新的OSD。我们建议您避免在升级过程中添加或替换任何OSD。我们建议您避免在升级过程中创建任何RADOS pools。您可以使用ceph version(s)命令在每个阶段监视升级进度，该命令将告诉您每种daemon正在运行的ceph版本。 UPGRADE COMPATIBILITY NOTES（升级兼容性说明）这些更改发生在Mimic和Nautilus版本之间。 ceph pg stat输出已修改为json格式，以匹配ceph df输出： “raw_bytes” field renamed to “total_bytes” “raw_bytes_avail” field renamed to “total_bytes_avail” “raw_bytes_avail” field renamed to “total_bytes_avail” “raw_bytes_used” field renamed to “total_bytes_raw_used” 添加了“total_bytes_used” field 来表示分配给block(slow) device上data objects的（所有OSD）空间 ceph df [detail]输出（GLOBAL section）格式进行了修改： 新的‘USED’ column显示了分配给block(slow) device上data objects的（所有OSD）空间 现在，‘RAW USED’ 是‘USED’空间与为Ceph目的在块设备上分配/保留的空间之和。BlueStore的BlueFS部分。 INSTRUCTIONS（使用说明）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101、如果您的集群最初安装的是Luminous之前的版本，请确保在运行Luminous时集群已完成对所有PG的至少一次完整scrub。否则，将导致您的monitor daemons在启动时拒绝加入quorum，从而使其无法运行。如果不确定Luminous集群是否已完成所有PG的完全scrub，则可以通过运行以下命令检查集群的状态：ceph osd dump | grep ^flags (OSD map必须包含recovery_deletes和purged_snapdirs标志)如果您的OSD map不包含这两个标志，则只需等待大约24-48小时，在标准群集配置中，应该有充足的时间，以便至少一次scrub所有placement groups，然后重复上述过程 重新检查。如果您刚刚完成了对Luminous的升级，并且想在短时间内进行升级到Mimic，可以使用shell命令在所有placement groups上强制执行scrub，例如：ceph pg dump pgs_brief | cut -d &quot; &quot; -f 1 | xargs -n1 ceph pg scrub您应该考虑到这种强制性scrub可能会对您的Ceph客户的性能产生负面影响。2、确保您的群集stable且healthy（没有宕机或无法恢复的OSD）。 （可选，但推荐。）3、在升级期间设置noout标志。 （可选，但建议使用。）ceph osd set noout4、通过安装新软件包并重新启动monitor daemons来升级monitors。 例如，在每个monitors主机上，：systemctl restart ceph-mon.target所有monitors启动之后，通过在mon map中查找nautilus字符串来验证monitor升级是否完成。 命令：ceph mon dump | grep min_mon_release应报告：min_mon_release 14 (nautilus)如果不是，则表示尚未升级和重新启动monitors，或者quorum不包括所有monitors。5、通过安装新packages并重新启动所有manager daemons来升级ceph-mgr daemons。 例如，在每个manager主机上，：systemctl restart ceph-mgr.target请注意，如果您使用的是Ceph Dashboard，则升级ceph-mgr package后可能需要单独安装ceph-mgr-dashboard。ceph-mgr-dashboard的安装脚本将自动为您重新启动manager daemons。 因此，在这种情况下，您可以跳过该步骤以重新启动daemons。通过检查ceph -s来验证ceph-mgr daemons是否正在运行：# ceph -s... services: mon: 3 daemons, quorum foo,bar,baz mgr: foo(active), standbys: bar, baz...6、通过安装新packages并在所有OSD主机上重新启动ceph-osd daemons来升级所有OSD：systemctl restart ceph-osd.target您可以使用ceph versions或ceph osd versions命令监视OSD升级的进度：# ceph osd versions&#123; &quot;ceph version 13.2.5 (...) mimic (stable)&quot;: 12, &quot;ceph version 14.2.0 (...) nautilus (stable)&quot;: 22,&#125;7、如果集群中通过ceph-disk部署了OSD（例如，几乎所有在Mimic版本之前创建的OSD），您都需要让ceph-volume承担启动daemons的责任。 在包含OSD的每个主机上，确保OSD当前正在运行，然后：ceph-volume simple scan （所有使用ceph-disk创建的并正在运行的OSDs，从OSD data partition或directory中捕获元数据）ceph-volume simple activate --all （使systemd units可以mount已配置的devices，并启动Ceph OSD）我们建议按照此步骤重新启动每个OSD主机，以验证OSD是否自动启动。请注意，ceph-volume不具有与ceph-disk相同的hot-plug功能，后者通过udev events自动检测到新连接的磁盘。如果运行上述scan命令时OSD当前未running，或者将基于ceph-disk的OSD移至新主机，或者重新安装了主机OSD，或者/etc/ceph/osd目录丢失， 您将需要显式扫描每个ceph-disk OSD的主数据分区。例如，：ceph-volume simple scan /dev/sdb1输出将包括相应的ceph-volume simple activate命令以启用OSD。8、升级所有CephFS MDS daemons。 对于每个CephFS file system， 8.1 将ranks数减少到1。（如果您打算稍后还原它，请首先记录MDS守护程序的原始数量）： ceph status ceph fs set &lt;fs_name&gt; max_mds 1 8.2 通过定期检查状态，等待集群停用所有non-zero ranks： ceph status 8.3 使用以下命令使所有standby MDS daemons在适当的主机上offline： systemctl stop ceph-mds@&lt;daemon_name&gt; 8.4 确认只有一个MDS处于online，并且您的FS的rank 0： ceph status 8.5 通过安装新packages并重新启动daemon来升级剩余的MDS daemon： systemctl restart ceph-mds.target 8.6 重新启动所有已offline的standby MDS daemons systemctl start ceph-mds.target 8.7 恢复该volume的max_mds原始值： ceph fs set &lt;fs_name&gt; max_mds &lt;original_max_mds&gt; 9、通过升级packages并在所有主机上重新启动daemons来升级所有radosgw daemons：systemctl restart ceph-radosgw.target10、禁用Nautilus之前的OSD并启用所有Nautilus的新功能来完成升级：ceph osd require-osd-release nautilus11、如果您一开始设置noout，请确保清除它：ceph osd unset noout12、使用ceph health验证集群是否healthy如果您的CRUSH tunables（可调参数）早于Hammer，Ceph现在将发出健康警告。 如果您看到有这种效果的健康警报，则可以使用以下方法还原此更改：ceph config set mon mon_crush_min_required_version firefly但是，如果Ceph没有警报，那么我们建议您也将所有现有的CRUSH buckets都切换到straw2，这是Hammer版本中重新添加的。如果您有任何“straw” buckets，这将导致少量的数据移动，但通常不会太严重。ceph osd getcrushmap -o backup-crushmapceph osd crush set-all-straw-buckets-to-straw2如果有问题，您可以还原：ceph osd setcrushmap -i backup-crushmap移至“straw2” buckets将解锁一些最新功能，例如在Luminous中crush-compat balancer mode（https://docs.ceph.com/docs/master/rados/operations/balancer/#balancer）。13、要启用新的 v2 network protocol，请发出以下命令：ceph mon enable-msgr2指示所有与旧版v1 protocol绑定到旧的默认端口6789的monitors，同时也绑定到新的3300 v2 protocol端口。要查看是否所有monitors都已更新，请执行以下操作：ceph mon dump并确认每个monitors都显示v2:和v1:地址。14、对于已升级的每个主机，应更新ceph.conf文件，使其不指定monitor端口（如果您在默认端口上运行monitor），或者显式引用v2和v1地址和端口。 如果仅列出了v1 IP和端口，则一切仍将起作用，但是在得知monitor也使用v2协议后，每个CLI实例或daemon都将需要重新连接，这会减慢速度并阻止完全过渡到v2协议。这也是将ceph.conf中的所有配置选项完全转换到集群的配置数据库中的好时机。 在每个主机上，您可以使用以下命令通过以下命令将所有选项导入monitor：ceph config assimilate-conf -i /etc/ceph/ceph.conf您可以通过以下方式查看集群的配置数据库：ceph config dump要为每个主机创建一个最小但足够的ceph.conf，请执行以下操作：ceph config generate-minimal-conf &gt; /etc/ceph/ceph.conf.newmv /etc/ceph/ceph.conf.new /etc/ceph/ceph.conf确保仅在已升级到Nautilus的主机上使用此新配置，因为它可能包含mon_host值，该值包含Nautilus能理解的IP地址的新v2:和v1:前缀。有关更多信息，请参阅https://docs.ceph.com/docs/master/rados/configuration/msgr2/#msgr2-ceph-conf15、考虑启用telemetry module以将匿名使用情况统计信息和崩溃信息发送给Ceph upstream developers。 查看将要报告的内容（实际上没有向任何人发送任何信息），请执行以下操作：ceph mgr module enable telemetryceph telemetry show如果您对所报告的数据感到满意，则可以选择使用以下方法自动报告high-level cluster metadata：ceph telemetry on有关telemetry module的更多信息，请参见文档：https://docs.ceph.com/docs/master/mgr/telemetry/#telemetry 1、查看当前环境与版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[root@ceph2 ~]# ceph -vceph version 12.2.12 (1436006594665279fe734b4c15d7e08c13ebd777) luminous (stable)[root@ceph2 ~]# cat /etc/redhat-release CentOS Linux release 7.6.1810 (Core) [root@ceph2 ~]# uname -aLinux ceph2 3.10.0-957.el7.x86_64 #1 SMP Thu Nov 8 23:39:32 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux[root@ceph2 ~]# ceph -s cluster: id: c4051efa-1997-43ef-8497-fb02bdf08233 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph3,ceph2 mgr: ceph2(active), standbys: ceph3, ceph1 mds: cephfs-1/1/1 up &#123;0=ceph1=up:active&#125;, 2 up:standby osd: 6 osds: 6 up, 6 in rgw: 3 daemons active data: pools: 7 pools, 176 pgs objects: 244 objects, 5.27KiB usage: 6.04GiB used, 293GiB / 299GiB avail pgs: 176 active+clean io: client: 2.00KiB/s rd, 0B/s wr, 1op/s rd, 1op/s wr[root@ceph2 ~]# ceph osd treeID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.29214 root default -5 0.09738 host ceph1 1 hdd 0.04869 osd.1 up 1.00000 1.00000 4 hdd 0.04869 osd.4 up 1.00000 1.00000 -7 0.09738 host ceph2 2 hdd 0.04869 osd.2 up 1.00000 1.00000 5 hdd 0.04869 osd.5 up 1.00000 1.00000 -3 0.09738 host ceph3 0 hdd 0.04869 osd.0 up 1.00000 1.00000 3 hdd 0.04869 osd.3 up 1.00000 1.00000 [root@ceph2 ~]# rpm -qa | grep cephlibcephfs2-12.2.12-0.el7.x86_64ceph-common-12.2.12-0.el7.x86_64ceph-radosgw-12.2.12-0.el7.x86_64ceph-base-12.2.12-0.el7.x86_64ceph-osd-12.2.12-0.el7.x86_64ceph-mds-12.2.12-0.el7.x86_64python-cephfs-12.2.12-0.el7.x86_64ceph-selinux-12.2.12-0.el7.x86_64ceph-mon-12.2.12-0.el7.x86_64ceph-mgr-12.2.12-0.el7.x86_64[root@ceph2 ~]# rpm -qa | grep rbdpython-rbd-12.2.12-0.el7.x86_64librbd1-12.2.12-0.el7.x86_64[root@ceph2 ~]# rpm -qa | grep radosceph-radosgw-12.2.12-0.el7.x86_64librados2-12.2.12-0.el7.x86_64python-rados-12.2.12-0.el7.x86_64libradosstriper1-12.2.12-0.el7.x86_64 2、修改每台ceph节点的ceph mirror，L版地址修改为N版地址 123456[root@ceph2 ~]# vim /etc/yum.repos.d/ceph_stable.repo [ceph_stable]baseurl = http://mirrors.163.com/ceph/rpm-nautilus/el7/$basearchgpgcheck = 1gpgkey = http://mirrors.163.com/ceph/keys/release.ascname = Ceph Stable repo 3、设置noout 标志停机维护 12345678910111213141516171819202122[root@ceph2 ~]# ceph osd set noout[root@ceph2 ~]# ceph -s cluster: id: c4051efa-1997-43ef-8497-fb02bdf08233 health: HEALTH_WARN insufficient standby MDS daemons available noout flag(s) set services: mon: 3 daemons, quorum ceph1,ceph3,ceph2 mgr: ceph1(active), standbys: ceph3, ceph2 mds: cephfs-1/1/1 up &#123;0=ceph2=up:active&#125; osd: 6 osds: 6 up, 6 in flags noout rgw: 2 daemons active data: pools: 7 pools, 176 pgs objects: 245 objects, 5.30KiB usage: 6.05GiB used, 293GiB / 299GiB avail pgs: 176 active+clean 4、升级每台ceph节点的 ceph packages（一个节点一个节点的升） 升级顺序 123451、Ceph Monitors2、Ceph Mgr3、Ceph OSD Daemons4、Ceph Metadata Servers5、Ceph Object Gateways 升级命令（注意：安装过程中rgw与mds服务会自动被stop） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335[root@ceph2 ~]# yum install cephLoaded plugins: fastestmirrorDetermining fastest mirrorsepel/x86_64/metalink | 6.5 kB 00:00:00 * base: mirrors.huaweicloud.com * epel: mirrors.aliyun.com * extras: mirrors.huaweicloud.com * updates: mirrors.huaweicloud.combase | 3.6 kB 00:00:00 ceph_stable | 2.9 kB 00:00:00 epel | 5.3 kB 00:00:00 extras | 2.9 kB 00:00:00 updates | 2.9 kB 00:00:00 (1/8): epel/x86_64/group_gz | 88 kB 00:00:00 (2/8): ceph_stable/x86_64/primary_db | 192 kB 00:00:01 (3/8): base/7/x86_64/group_gz | 165 kB 00:00:01 (4/8): epel/x86_64/updateinfo | 1.0 MB 00:00:02 (5/8): extras/7/x86_64/primary_db | 152 kB 00:00:01 (6/8): base/7/x86_64/primary_db | 6.0 MB 00:00:04 (7/8): updates/7/x86_64/primary_db | 1.9 MB 00:00:05 (8/8): epel/x86_64/primary_db | 6.9 MB 00:00:16 Resolving Dependencies--&gt; Running transaction check---&gt; Package ceph.x86_64 2:14.2.4-0.el7 will be installed--&gt; Processing Dependency: ceph-osd = 2:14.2.4-0.el7 for package: 2:ceph-14.2.4-0.el7.x86_64--&gt; Processing Dependency: ceph-mds = 2:14.2.4-0.el7 for package: 2:ceph-14.2.4-0.el7.x86_64--&gt; Processing Dependency: ceph-mgr = 2:14.2.4-0.el7 for package: 2:ceph-14.2.4-0.el7.x86_64--&gt; Processing Dependency: ceph-mon = 2:14.2.4-0.el7 for package: 2:ceph-14.2.4-0.el7.x86_64--&gt; Running transaction check---&gt; Package ceph-mds.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package ceph-mds.x86_64 2:14.2.4-0.el7 will be an update--&gt; Processing Dependency: ceph-base = 2:14.2.4-0.el7 for package: 2:ceph-mds-14.2.4-0.el7.x86_64--&gt; Processing Dependency: librdmacm.so.1()(64bit) for package: 2:ceph-mds-14.2.4-0.el7.x86_64---&gt; Package ceph-mgr.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package ceph-mgr.x86_64 2:14.2.4-0.el7 will be an update--&gt; Processing Dependency: python-bcrypt for package: 2:ceph-mgr-14.2.4-0.el7.x86_64---&gt; Package ceph-mon.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package ceph-mon.x86_64 2:14.2.4-0.el7 will be an update---&gt; Package ceph-osd.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package ceph-osd.x86_64 2:14.2.4-0.el7 will be an update--&gt; Processing Dependency: libstoragemgmt for package: 2:ceph-osd-14.2.4-0.el7.x86_64--&gt; Running transaction check---&gt; Package ceph-base.x86_64 2:12.2.12-0.el7 will be updated--&gt; Processing Dependency: ceph-base = 2:12.2.12-0.el7 for package: 2:ceph-selinux-12.2.12-0.el7.x86_64--&gt; Processing Dependency: ceph-base = 2:12.2.12-0.el7 for package: 2:ceph-selinux-12.2.12-0.el7.x86_64--&gt; Processing Dependency: ceph-base = 2:12.2.12-0.el7 for package: 2:ceph-radosgw-12.2.12-0.el7.x86_64---&gt; Package ceph-base.x86_64 2:14.2.4-0.el7 will be an update--&gt; Processing Dependency: librados2 = 2:14.2.4-0.el7 for package: 2:ceph-base-14.2.4-0.el7.x86_64--&gt; Processing Dependency: libcephfs2 = 2:14.2.4-0.el7 for package: 2:ceph-base-14.2.4-0.el7.x86_64--&gt; Processing Dependency: librgw2 = 2:14.2.4-0.el7 for package: 2:ceph-base-14.2.4-0.el7.x86_64--&gt; Processing Dependency: librbd1 = 2:14.2.4-0.el7 for package: 2:ceph-base-14.2.4-0.el7.x86_64--&gt; Processing Dependency: ceph-common = 2:14.2.4-0.el7 for package: 2:ceph-base-14.2.4-0.el7.x86_64--&gt; Processing Dependency: liboath.so.0(LIBOATH_1.10.0)(64bit) for package: 2:ceph-base-14.2.4-0.el7.x86_64--&gt; Processing Dependency: liboath.so.0(LIBOATH_1.12.0)(64bit) for package: 2:ceph-base-14.2.4-0.el7.x86_64--&gt; Processing Dependency: liboath.so.0(LIBOATH_1.2.0)(64bit) for package: 2:ceph-base-14.2.4-0.el7.x86_64--&gt; Processing Dependency: liboath.so.0()(64bit) for package: 2:ceph-base-14.2.4-0.el7.x86_64---&gt; Package librdmacm.x86_64 0:22.1-3.el7 will be installed---&gt; Package libstoragemgmt.x86_64 0:1.7.3-3.el7 will be installed--&gt; Processing Dependency: libstoragemgmt-python for package: libstoragemgmt-1.7.3-3.el7.x86_64--&gt; Processing Dependency: libyajl.so.2()(64bit) for package: libstoragemgmt-1.7.3-3.el7.x86_64--&gt; Processing Dependency: libconfig.so.9()(64bit) for package: libstoragemgmt-1.7.3-3.el7.x86_64---&gt; Package python2-bcrypt.x86_64 0:3.1.6-2.el7 will be installed--&gt; Processing Dependency: python-cffi for package: python2-bcrypt-3.1.6-2.el7.x86_64--&gt; Processing Dependency: python2-six for package: python2-bcrypt-3.1.6-2.el7.x86_64--&gt; Running transaction check---&gt; Package ceph-common.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package ceph-common.x86_64 2:14.2.4-0.el7 will be an update--&gt; Processing Dependency: libradosstriper1 = 2:14.2.4-0.el7 for package: 2:ceph-common-14.2.4-0.el7.x86_64--&gt; Processing Dependency: python-cephfs = 2:14.2.4-0.el7 for package: 2:ceph-common-14.2.4-0.el7.x86_64--&gt; Processing Dependency: python-rbd = 2:14.2.4-0.el7 for package: 2:ceph-common-14.2.4-0.el7.x86_64--&gt; Processing Dependency: python-ceph-argparse = 2:14.2.4-0.el7 for package: 2:ceph-common-14.2.4-0.el7.x86_64--&gt; Processing Dependency: python-rados = 2:14.2.4-0.el7 for package: 2:ceph-common-14.2.4-0.el7.x86_64--&gt; Processing Dependency: python-rgw = 2:14.2.4-0.el7 for package: 2:ceph-common-14.2.4-0.el7.x86_64--&gt; Processing Dependency: librabbitmq.so.4()(64bit) for package: 2:ceph-common-14.2.4-0.el7.x86_64---&gt; Package ceph-radosgw.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package ceph-radosgw.x86_64 2:14.2.4-0.el7 will be an update---&gt; Package ceph-selinux.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package ceph-selinux.x86_64 2:14.2.4-0.el7 will be an update--&gt; Processing Dependency: selinux-policy-base &gt;= 3.13.1-229.el7_6.15 for package: 2:ceph-selinux-14.2.4-0.el7.x86_64---&gt; Package libcephfs2.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package libcephfs2.x86_64 2:14.2.4-0.el7 will be an update---&gt; Package libconfig.x86_64 0:1.4.9-5.el7 will be installed---&gt; Package liboath.x86_64 0:2.6.2-1.el7 will be installed---&gt; Package librados2.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package librados2.x86_64 2:14.2.4-0.el7 will be an update---&gt; Package librbd1.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package librbd1.x86_64 2:14.2.4-0.el7 will be an update---&gt; Package librgw2.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package librgw2.x86_64 2:14.2.4-0.el7 will be an update---&gt; Package libstoragemgmt-python.noarch 0:1.7.3-3.el7 will be installed--&gt; Processing Dependency: libstoragemgmt-python-clibs for package: libstoragemgmt-python-1.7.3-3.el7.noarch---&gt; Package python-cffi.x86_64 0:1.6.0-5.el7 will be installed--&gt; Processing Dependency: python-pycparser for package: python-cffi-1.6.0-5.el7.x86_64---&gt; Package python2-six.noarch 0:1.9.0-0.el7 will be installed---&gt; Package yajl.x86_64 0:2.0.4-4.el7 will be installed--&gt; Running transaction check---&gt; Package librabbitmq.x86_64 0:0.8.0-2.el7 will be installed---&gt; Package libradosstriper1.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package libradosstriper1.x86_64 2:14.2.4-0.el7 will be an update---&gt; Package libstoragemgmt-python-clibs.x86_64 0:1.7.3-3.el7 will be installed---&gt; Package python-ceph-argparse.x86_64 2:14.2.4-0.el7 will be installed---&gt; Package python-cephfs.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package python-cephfs.x86_64 2:14.2.4-0.el7 will be an update---&gt; Package python-pycparser.noarch 0:2.14-1.el7 will be installed--&gt; Processing Dependency: python-ply for package: python-pycparser-2.14-1.el7.noarch---&gt; Package python-rados.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package python-rados.x86_64 2:14.2.4-0.el7 will be an update---&gt; Package python-rbd.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package python-rbd.x86_64 2:14.2.4-0.el7 will be an update---&gt; Package python-rgw.x86_64 2:12.2.12-0.el7 will be updated---&gt; Package python-rgw.x86_64 2:14.2.4-0.el7 will be an update---&gt; Package selinux-policy-targeted.noarch 0:3.13.1-229.el7 will be updated---&gt; Package selinux-policy-targeted.noarch 0:3.13.1-252.el7.1 will be an update--&gt; Processing Dependency: selinux-policy = 3.13.1-252.el7.1 for package: selinux-policy-targeted-3.13.1-252.el7.1.noarch--&gt; Processing Dependency: selinux-policy = 3.13.1-252.el7.1 for package: selinux-policy-targeted-3.13.1-252.el7.1.noarch--&gt; Running transaction check---&gt; Package python-ply.noarch 0:3.4-11.el7 will be installed---&gt; Package selinux-policy.noarch 0:3.13.1-229.el7 will be updated---&gt; Package selinux-policy.noarch 0:3.13.1-252.el7.1 will be an update--&gt; Finished Dependency ResolutionDependencies Resolved============================================================================================================================================================================================================================================= Package Arch Version Repository Size=============================================================================================================================================================================================================================================Installing: ceph x86_64 2:14.2.4-0.el7 ceph_stable 3.0 kInstalling for dependencies: libconfig x86_64 1.4.9-5.el7 base 59 k liboath x86_64 2.6.2-1.el7 epel 51 k librabbitmq x86_64 0.8.0-2.el7 base 37 k librdmacm x86_64 22.1-3.el7 base 63 k libstoragemgmt x86_64 1.7.3-3.el7 base 243 k libstoragemgmt-python noarch 1.7.3-3.el7 base 167 k libstoragemgmt-python-clibs x86_64 1.7.3-3.el7 base 19 k python-ceph-argparse x86_64 2:14.2.4-0.el7 ceph_stable 36 k python-cffi x86_64 1.6.0-5.el7 base 218 k python-ply noarch 3.4-11.el7 base 123 k python-pycparser noarch 2.14-1.el7 base 104 k python2-bcrypt x86_64 3.1.6-2.el7 epel 39 k python2-six noarch 1.9.0-0.el7 epel 2.9 k yajl x86_64 2.0.4-4.el7 base 39 kUpdating for dependencies: ceph-base x86_64 2:14.2.4-0.el7 ceph_stable 5.4 M ceph-common x86_64 2:14.2.4-0.el7 ceph_stable 18 M ceph-mds x86_64 2:14.2.4-0.el7 ceph_stable 1.8 M ceph-mgr x86_64 2:14.2.4-0.el7 ceph_stable 1.5 M ceph-mon x86_64 2:14.2.4-0.el7 ceph_stable 4.5 M ceph-osd x86_64 2:14.2.4-0.el7 ceph_stable 16 M ceph-radosgw x86_64 2:14.2.4-0.el7 ceph_stable 5.3 M ceph-selinux x86_64 2:14.2.4-0.el7 ceph_stable 21 k libcephfs2 x86_64 2:14.2.4-0.el7 ceph_stable 480 k librados2 x86_64 2:14.2.4-0.el7 ceph_stable 3.3 M libradosstriper1 x86_64 2:14.2.4-0.el7 ceph_stable 342 k librbd1 x86_64 2:14.2.4-0.el7 ceph_stable 1.6 M librgw2 x86_64 2:14.2.4-0.el7 ceph_stable 4.6 M python-cephfs x86_64 2:14.2.4-0.el7 ceph_stable 91 k python-rados x86_64 2:14.2.4-0.el7 ceph_stable 190 k python-rbd x86_64 2:14.2.4-0.el7 ceph_stable 171 k python-rgw x86_64 2:14.2.4-0.el7 ceph_stable 76 k selinux-policy noarch 3.13.1-252.el7.1 updates 492 k selinux-policy-targeted noarch 3.13.1-252.el7.1 updates 7.0 MTransaction Summary=============================================================================================================================================================================================================================================Install 1 Package (+14 Dependent packages)Upgrade ( 19 Dependent packages)Total download size: 72 MIs this ok [y/d/N]: yDownloading packages:Delta RPMs disabled because /usr/bin/applydeltarpm not installed.(1/34): ceph-14.2.4-0.el7.x86_64.rpm | 3.0 kB 00:00:00 (2/34): ceph-common-14.2.4-0.el7.x86_64.rpm | 18 MB 00:00:19 (3/34): ceph-mds-14.2.4-0.el7.x86_64.rpm | 1.8 MB 00:00:01 (4/34): ceph-mgr-14.2.4-0.el7.x86_64.rpm | 1.5 MB 00:00:03 (5/34): ceph-mon-14.2.4-0.el7.x86_64.rpm | 4.5 MB 00:00:05 (6/34): ceph-osd-14.2.4-0.el7.x86_64.rpm | 16 MB 00:00:14 (7/34): ceph-radosgw-14.2.4-0.el7.x86_64.rpm | 5.3 MB 00:00:07 (8/34): ceph-selinux-14.2.4-0.el7.x86_64.rpm | 21 kB 00:00:00 (9/34): libconfig-1.4.9-5.el7.x86_64.rpm | 59 kB 00:00:00 (10/34): librabbitmq-0.8.0-2.el7.x86_64.rpm | 37 kB 00:00:00 (11/34): libcephfs2-14.2.4-0.el7.x86_64.rpm | 480 kB 00:00:00 (12/34): librados2-14.2.4-0.el7.x86_64.rpm | 3.3 MB 00:00:05 (13/34): libradosstriper1-14.2.4-0.el7.x86_64.rpm | 342 kB 00:00:00 (14/34): librdmacm-22.1-3.el7.x86_64.rpm | 63 kB 00:00:00 (15/34): liboath-2.6.2-1.el7.x86_64.rpm | 51 kB 00:00:07 (16/34): librbd1-14.2.4-0.el7.x86_64.rpm | 1.6 MB 00:00:01 (17/34): libstoragemgmt-python-1.7.3-3.el7.noarch.rpm | 167 kB 00:00:00 (18/34): libstoragemgmt-1.7.3-3.el7.x86_64.rpm | 243 kB 00:00:00 (19/34): libstoragemgmt-python-clibs-1.7.3-3.el7.x86_64.rpm | 19 kB 00:00:00 (20/34): librgw2-14.2.4-0.el7.x86_64.rpm | 4.6 MB 00:00:02 (21/34): python-ceph-argparse-14.2.4-0.el7.x86_64.rpm | 36 kB 00:00:00 (22/34): python-ply-3.4-11.el7.noarch.rpm | 123 kB 00:00:00 (23/34): python-cffi-1.6.0-5.el7.x86_64.rpm | 218 kB 00:00:00 (24/34): python-cephfs-14.2.4-0.el7.x86_64.rpm | 91 kB 00:00:00 (25/34): python-pycparser-2.14-1.el7.noarch.rpm | 104 kB 00:00:00 (26/34): python-rados-14.2.4-0.el7.x86_64.rpm | 190 kB 00:00:00 (27/34): python-rbd-14.2.4-0.el7.x86_64.rpm | 171 kB 00:00:00 (28/34): python2-six-1.9.0-0.el7.noarch.rpm | 2.9 kB 00:00:00 (29/34): python2-bcrypt-3.1.6-2.el7.x86_64.rpm | 39 kB 00:00:00 (30/34): python-rgw-14.2.4-0.el7.x86_64.rpm | 76 kB 00:00:00 (31/34): selinux-policy-3.13.1-252.el7.1.noarch.rpm | 492 kB 00:00:00 (32/34): yajl-2.0.4-4.el7.x86_64.rpm | 39 kB 00:00:00 (33/34): selinux-policy-targeted-3.13.1-252.el7.1.noarch.rpm | 7.0 MB 00:00:02 (34/34): ceph-base-14.2.4-0.el7.x86_64.rpm | 5.4 MB 00:01:13 ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Total 1.0 MB/s | 72 MB 00:01:13 Running transaction checkRunning transaction testTransaction test succeededRunning transaction Installing : librdmacm-22.1-3.el7.x86_64 1/53 Updating : 2:librados2-14.2.4-0.el7.x86_64 2/53 Updating : 2:python-rados-14.2.4-0.el7.x86_64 3/53 Installing : liboath-2.6.2-1.el7.x86_64 4/53 Updating : 2:librbd1-14.2.4-0.el7.x86_64 5/53 Updating : 2:libcephfs2-14.2.4-0.el7.x86_64 6/53 Installing : librabbitmq-0.8.0-2.el7.x86_64 7/53 Updating : 2:librgw2-14.2.4-0.el7.x86_64 8/53 Installing : 2:python-ceph-argparse-14.2.4-0.el7.x86_64 9/53 Installing : yajl-2.0.4-4.el7.x86_64 10/53 Updating : 2:python-cephfs-14.2.4-0.el7.x86_64 11/53 Updating : 2:python-rgw-14.2.4-0.el7.x86_64 12/53 Updating : 2:python-rbd-14.2.4-0.el7.x86_64 13/53 Updating : 2:libradosstriper1-14.2.4-0.el7.x86_64 14/53 Updating : 2:ceph-common-14.2.4-0.el7.x86_64 15/53 Updating : selinux-policy-3.13.1-252.el7.1.noarch 16/53 Updating : selinux-policy-targeted-3.13.1-252.el7.1.noarch 17/53 Updating : 2:ceph-base-14.2.4-0.el7.x86_64 18/53 Updating : 2:ceph-selinux-14.2.4-0.el7.x86_64 19/53 Updating : 2:ceph-mds-14.2.4-0.el7.x86_64 20/53 Updating : 2:ceph-mon-14.2.4-0.el7.x86_64 21/53 Installing : libconfig-1.4.9-5.el7.x86_64 22/53 Installing : libstoragemgmt-1.7.3-3.el7.x86_64 23/53 Installing : libstoragemgmt-python-clibs-1.7.3-3.el7.x86_64 24/53 Installing : libstoragemgmt-python-1.7.3-3.el7.noarch 25/53 Updating : 2:ceph-osd-14.2.4-0.el7.x86_64 26/53 Installing : python-ply-3.4-11.el7.noarch 27/53 Installing : python-pycparser-2.14-1.el7.noarch 28/53 Installing : python-cffi-1.6.0-5.el7.x86_64 29/53 Installing : python2-six-1.9.0-0.el7.noarch 30/53 Installing : python2-bcrypt-3.1.6-2.el7.x86_64 31/53 Updating : 2:ceph-mgr-14.2.4-0.el7.x86_64 32/53 Installing : 2:ceph-14.2.4-0.el7.x86_64 33/53 Updating : 2:ceph-radosgw-14.2.4-0.el7.x86_64 34/53 Cleanup : 2:ceph-radosgw-12.2.12-0.el7.x86_64 35/53 Cleanup : 2:ceph-mon-12.2.12-0.el7.x86_64 36/53 Cleanup : 2:ceph-osd-12.2.12-0.el7.x86_64 37/53 Cleanup : 2:ceph-mds-12.2.12-0.el7.x86_64 38/53 Cleanup : 2:ceph-mgr-12.2.12-0.el7.x86_64 39/53 Cleanup : 2:ceph-selinux-12.2.12-0.el7.x86_64 40/53 Cleanup : 2:ceph-base-12.2.12-0.el7.x86_64 41/53 Cleanup : selinux-policy-targeted-3.13.1-229.el7.noarch 42/53 Cleanup : 2:ceph-common-12.2.12-0.el7.x86_64 43/53 Cleanup : selinux-policy-3.13.1-229.el7.noarch 44/53 Cleanup : 2:python-rbd-12.2.12-0.el7.x86_64 45/53 Cleanup : 2:python-rgw-12.2.12-0.el7.x86_64 46/53 Cleanup : 2:librgw2-12.2.12-0.el7.x86_64 47/53 Cleanup : 2:python-rados-12.2.12-0.el7.x86_64 48/53 Cleanup : 2:librbd1-12.2.12-0.el7.x86_64 49/53 Cleanup : 2:libradosstriper1-12.2.12-0.el7.x86_64 50/53 Cleanup : 2:python-cephfs-12.2.12-0.el7.x86_64 51/53 Cleanup : 2:libcephfs2-12.2.12-0.el7.x86_64 52/53 Cleanup : 2:librados2-12.2.12-0.el7.x86_64 53/53 Verifying : 2:librados2-14.2.4-0.el7.x86_64 1/53 Verifying : liboath-2.6.2-1.el7.x86_64 2/53 Verifying : 2:python-rgw-14.2.4-0.el7.x86_64 3/53 Verifying : python2-six-1.9.0-0.el7.noarch 4/53 Verifying : 2:ceph-14.2.4-0.el7.x86_64 5/53 Verifying : 2:ceph-mgr-14.2.4-0.el7.x86_64 6/53 Verifying : python2-bcrypt-3.1.6-2.el7.x86_64 7/53 Verifying : 2:ceph-osd-14.2.4-0.el7.x86_64 8/53 Verifying : libstoragemgmt-python-clibs-1.7.3-3.el7.x86_64 9/53 Verifying : 2:ceph-base-14.2.4-0.el7.x86_64 10/53 Verifying : 2:ceph-common-14.2.4-0.el7.x86_64 11/53 Verifying : 2:libradosstriper1-14.2.4-0.el7.x86_64 12/53 Verifying : 2:python-rados-14.2.4-0.el7.x86_64 13/53 Verifying : 2:librbd1-14.2.4-0.el7.x86_64 14/53 Verifying : librdmacm-22.1-3.el7.x86_64 15/53 Verifying : libstoragemgmt-python-1.7.3-3.el7.noarch 16/53 Verifying : 2:ceph-mds-14.2.4-0.el7.x86_64 17/53 Verifying : python-ply-3.4-11.el7.noarch 18/53 Verifying : 2:libcephfs2-14.2.4-0.el7.x86_64 19/53 Verifying : libconfig-1.4.9-5.el7.x86_64 20/53 Verifying : 2:ceph-selinux-14.2.4-0.el7.x86_64 21/53 Verifying : 2:ceph-radosgw-14.2.4-0.el7.x86_64 22/53 Verifying : selinux-policy-targeted-3.13.1-252.el7.1.noarch 23/53 Verifying : 2:ceph-mon-14.2.4-0.el7.x86_64 24/53 Verifying : yajl-2.0.4-4.el7.x86_64 25/53 Verifying : 2:librgw2-14.2.4-0.el7.x86_64 26/53 Verifying : python-cffi-1.6.0-5.el7.x86_64 27/53 Verifying : python-pycparser-2.14-1.el7.noarch 28/53 Verifying : libstoragemgmt-1.7.3-3.el7.x86_64 29/53 Verifying : 2:python-rbd-14.2.4-0.el7.x86_64 30/53 Verifying : 2:python-cephfs-14.2.4-0.el7.x86_64 31/53 Verifying : selinux-policy-3.13.1-252.el7.1.noarch 32/53 Verifying : 2:python-ceph-argparse-14.2.4-0.el7.x86_64 33/53 Verifying : librabbitmq-0.8.0-2.el7.x86_64 34/53 Verifying : selinux-policy-targeted-3.13.1-229.el7.noarch 35/53 Verifying : 2:ceph-mgr-12.2.12-0.el7.x86_64 36/53 Verifying : 2:ceph-osd-12.2.12-0.el7.x86_64 37/53 Verifying : selinux-policy-3.13.1-229.el7.noarch 38/53 Verifying : 2:ceph-base-12.2.12-0.el7.x86_64 39/53 Verifying : 2:python-rados-12.2.12-0.el7.x86_64 40/53 Verifying : 2:python-cephfs-12.2.12-0.el7.x86_64 41/53 Verifying : 2:ceph-common-12.2.12-0.el7.x86_64 42/53 Verifying : 2:ceph-mon-12.2.12-0.el7.x86_64 43/53 Verifying : 2:libradosstriper1-12.2.12-0.el7.x86_64 44/53 Verifying : 2:libcephfs2-12.2.12-0.el7.x86_64 45/53 Verifying : 2:python-rbd-12.2.12-0.el7.x86_64 46/53 Verifying : 2:librbd1-12.2.12-0.el7.x86_64 47/53 Verifying : 2:ceph-radosgw-12.2.12-0.el7.x86_64 48/53 Verifying : 2:ceph-mds-12.2.12-0.el7.x86_64 49/53 Verifying : 2:librgw2-12.2.12-0.el7.x86_64 50/53 Verifying : 2:ceph-selinux-12.2.12-0.el7.x86_64 51/53 Verifying : 2:python-rgw-12.2.12-0.el7.x86_64 52/53 Verifying : 2:librados2-12.2.12-0.el7.x86_64 53/53 Installed: ceph.x86_64 2:14.2.4-0.el7 Dependency Installed: libconfig.x86_64 0:1.4.9-5.el7 liboath.x86_64 0:2.6.2-1.el7 librabbitmq.x86_64 0:0.8.0-2.el7 librdmacm.x86_64 0:22.1-3.el7 libstoragemgmt.x86_64 0:1.7.3-3.el7 libstoragemgmt-python.noarch 0:1.7.3-3.el7 libstoragemgmt-python-clibs.x86_64 0:1.7.3-3.el7 python-ceph-argparse.x86_64 2:14.2.4-0.el7 python-cffi.x86_64 0:1.6.0-5.el7 python-ply.noarch 0:3.4-11.el7 python-pycparser.noarch 0:2.14-1.el7 python2-bcrypt.x86_64 0:3.1.6-2.el7 python2-six.noarch 0:1.9.0-0.el7 yajl.x86_64 0:2.0.4-4.el7 Dependency Updated: ceph-base.x86_64 2:14.2.4-0.el7 ceph-common.x86_64 2:14.2.4-0.el7 ceph-mds.x86_64 2:14.2.4-0.el7 ceph-mgr.x86_64 2:14.2.4-0.el7 ceph-mon.x86_64 2:14.2.4-0.el7 ceph-osd.x86_64 2:14.2.4-0.el7 ceph-radosgw.x86_64 2:14.2.4-0.el7 ceph-selinux.x86_64 2:14.2.4-0.el7 libcephfs2.x86_64 2:14.2.4-0.el7 librados2.x86_64 2:14.2.4-0.el7 libradosstriper1.x86_64 2:14.2.4-0.el7 librbd1.x86_64 2:14.2.4-0.el7 librgw2.x86_64 2:14.2.4-0.el7 python-cephfs.x86_64 2:14.2.4-0.el7 python-rados.x86_64 2:14.2.4-0.el7 python-rbd.x86_64 2:14.2.4-0.el7 python-rgw.x86_64 2:14.2.4-0.el7 selinux-policy.noarch 0:3.13.1-252.el7.1 selinux-policy-targeted.noarch 0:3.13.1-252.el7.1Complete! 5、检查更新后的rpm packages 1234567891011121314151617181920212223[root@ceph2 ~]# rpm -qa | grep cephpython-ceph-argparse-14.2.4-0.el7.x86_64ceph-mon-14.2.4-0.el7.x86_64ceph-14.2.4-0.el7.x86_64libcephfs2-14.2.4-0.el7.x86_64ceph-base-14.2.4-0.el7.x86_64ceph-mds-14.2.4-0.el7.x86_64ceph-osd-14.2.4-0.el7.x86_64ceph-mgr-14.2.4-0.el7.x86_64ceph-radosgw-14.2.4-0.el7.x86_64python-cephfs-14.2.4-0.el7.x86_64ceph-common-14.2.4-0.el7.x86_64ceph-selinux-14.2.4-0.el7.x86_64[root@ceph2 ~]# rpm -qa | grep rbdlibrbd1-14.2.4-0.el7.x86_64python-rbd-14.2.4-0.el7.x86_64[root@ceph2 ~]# rpm -qa | grep radoslibrados2-14.2.4-0.el7.x86_64libradosstriper1-14.2.4-0.el7.x86_64ceph-radosgw-14.2.4-0.el7.x86_64python-rados-14.2.4-0.el7.x86_64 6、重启服务每台ceph节点的ceph service并检查service状态 重启顺序 123451、Ceph Monitors2、Ceph Mgr3、Ceph OSD Daemons4、Ceph Metadata Servers5、Ceph Object Gateways 重启命令 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 重启Ceph Monitors[root@ceph2 ~]# systemctl restart ceph-mon@ceph2.service[root@ceph2 ~]# ceph mon state1: 3 mons at &#123;ceph1=10.20.10.8:6789/0,ceph2=10.20.10.21:6789/0,ceph3=10.20.10.15:6789/0&#125;, election epoch 6, leader 0 ceph1, quorum 0,1,2 ceph1,ceph3,ceph2# 重启Ceph Mgr[root@ceph2 ~]# systemctl restart ceph-mgr@ceph2.service[root@ceph2 ~]# ceph -s cluster: id: c4051efa-1997-43ef-8497-fb02bdf08233 health: HEALTH_WARN insufficient standby MDS daemons available noout flag(s) set services: mon: 3 daemons, quorum ceph1,ceph3,ceph2 mgr: ceph1(active), standbys: ceph3, ceph2 mds: cephfs-1/1/1 up &#123;0=ceph2=up:active&#125; osd: 6 osds: 6 up, 6 in flags noout rgw: 2 daemons active data: pools: 7 pools, 176 pgs objects: 245 objects, 5.30KiB usage: 6.05GiB used, 293GiB / 299GiB avail pgs: 176 active+clean# 重启Ceph OSD Daemons[root@ceph2 ~]# systemctl restart ceph-osd@2.service &amp;&amp; systemctl restart ceph-osd@5.service &amp;&amp; ceph osd stat6 osds: 5 up, 6 in[root@ceph2 ~]# ceph osd stat6 osds: 6 up, 6 in# 重启Ceph Metadata Servers[root@ceph2 ~]# systemctl restart ceph-mds@ceph2.service[root@ceph2 ~]# ceph mds statcephfs-1/1/1 up &#123;0=ceph2=up:reconnect&#125;, 1 up:standby[root@ceph2 ~]# ceph mds statcephfs-1/1/1 up &#123;0=ceph2=up:active&#125;, 1 up:standby# 重启Ceph Object Gateways[root@ceph2 ~]# systemctl restart ceph-radosgw@rgw.ceph2.service[root@ceph2 ~]# ceph -s cluster: id: c4051efa-1997-43ef-8497-fb02bdf08233 health: HEALTH_WARN insufficient standby MDS daemons available noout flag(s) set services: mon: 3 daemons, quorum ceph1,ceph3,ceph2 mgr: ceph1(active), standbys: ceph3, ceph2 mds: cephfs-1/1/1 up &#123;0=ceph2=up:active&#125; osd: 6 osds: 6 up, 6 in flags noout rgw: 2 daemons active data: pools: 7 pools, 176 pgs objects: 245 objects, 5.30KiB usage: 6.05GiB used, 293GiB / 299GiB avail pgs: 176 active+clean 注意：当集群升级过程中，刚刚升级第一个MDS会出现insufficient standby MDS daemons available，当升级第二个MDS后，警告会自动消失。 7、警告解除Legacy BlueStore stats reporting detected on 6 OSD(s) 首先确认集群状态，无其他异常告警，并修复。 12345678910111213141516171819202122232425[root@ceph1 ~]# ceph -s cluster: id: c4051efa-1997-43ef-8497-fb02bdf08233 health: HEALTH_WARN noout flag(s) set Legacy BlueStore stats reporting detected on 6 OSD(s) services: mon: 3 daemons, quorum ceph1,ceph3,ceph2 (age 111s) mgr: ceph2(active, since 6h), standbys: ceph3, ceph1 mds: cephfs:1 &#123;0=ceph2=up:active&#125; 2 up:standby osd: 6 osds: 6 up, 6 in flags noout rgw: 3 daemons active (ceph1, ceph2, ceph3) data: pools: 7 pools, 176 pgs objects: 245 objects, 5.8 KiB usage: 6.1 GiB used, 293 GiB / 299 GiB avail pgs: 176 active+clean[root@ceph1 ~]# systemctl stop ceph-osd@1.service[root@ceph1 ~]# ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-1/repair success[root@ceph1 ~]# systemctl start ceph-osd@1.service 当前出现问题：https://tracker.ceph.com/issues/42297，正在与社区沟通 相关ceph users 123http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/035889.htmlhttp://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/036010.htmlhttp://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/036002.html 8、告警解除3 monitors have not enabled msgr2（nautilus版本中mon需要打开v2，监听3300端口），关于msgr2参见http://lnsyyj.github.io/2019/10/14/Ceph-MESSENGER-V2/ 1[root@ceph1 ~]# ceph mon enable-msgr2 9、解除noout标志 1[root@ceph2 ~]# ceph osd unset noout 10、确认ceph状态 1234567891011121314151617[root@ceph2 ~]# ceph -s cluster: id: c4051efa-1997-43ef-8497-fb02bdf08233 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph3,ceph2 (age 30m) mgr: ceph2(active, since 7h), standbys: ceph3, ceph1 mds: cephfs:1 &#123;0=ceph2=up:active&#125; 2 up:standby osd: 6 osds: 6 up, 6 in rgw: 3 daemons active (ceph1, ceph2, ceph3) data: pools: 7 pools, 176 pgs objects: 245 objects, 5.8 KiB usage: 6.1 GiB used, 293 GiB / 299 GiB avail pgs: 176 active+clean 升级Ceph客户端package升级可能带来的影响12（1）客户端升级可能会影响客户业务，导致业务中断一定时间。（2）可能带来客户端程序不兼容ceph client package的情况。 1、升级package 1[root@ceph1 ~]# yum install ceph-common librados2 librbd1 python-rbd python-rados -y 2、确认升级后的版本 1ceph --version]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CEPH ISCSI GATEWAY]]></title>
    <url>%2F2019%2F10%2F08%2FCEPH-ISCSI-GATEWAY%2F</url>
    <content type="text"><![CDATA[iSCSI gateway将Ceph Storage与iSCSI标准集成在一起，将RADOS Block Device（RBD）images导出为SCSI disks高可用（HA）iSCSI target。iSCSI协议允许客户端（initiators）通过TCP / IP网络将SCSI命令发送到SCSI storage devices（targets）。这允许异构客户端（例如Microsoft Windows）访问Ceph存储群集。 每个iSCSI gateway都运行Linux IO target kernel subsystem（LIO）以提供iSCSI协议支持。LIO利用userspace直通（TCMU）Ceph的librbd库进行交互，并将RBD images公开给iSCSI客户端。借助Ceph的iSCSI gateway，您可以有效地运行完整的block-storage infrastructure，并具有传统Storage Area Network （SAN） 的所有功能和优势。 Requirements要实现Ceph iSCSI gateway，有一些要求。对于高可用的Ceph iSCSI gateway解决方案，建议使用2到4个iSCSI gateway节点。 有关硬件建议，请参阅Hardware Recommendation page获取更多详细信息。 1注意 在iSCSI gateway节点上，RBD images的内存占用量可能会增大。根据映射(mapped)的RBD images的数量，相应地计划内存需求。 Ceph Monitors或OSD没有特定的iSCSI gateway选项，但是降低默认timers以检测OSD至关重要，它可以降低initiator超时的可能性，这一点很重要。建议为存储集群中的每个OSD节点使用以下配置选项： 123[osd]osd heartbeat grace = 20osd heartbeat interval = 5 Ceph Monitor的在线更新 1234ceph tell &lt;daemon_type&gt;.&lt;id&gt; config set &lt;parameter_name&gt; &lt;new_value&gt;ceph tell osd.0 config set osd_heartbeat_grace 20ceph tell osd.0 config set osd_heartbeat_interval 5 OSD节点的在线更新 1234ceph daemon &lt;daemon_type&gt;.&lt;id&gt; config set osd_client_watch_timeout 15ceph daemon osd.0 config set osd_heartbeat_grace 20ceph daemon osd.0 config set osd_heartbeat_interval 5 有关设置Ceph的配置选项的更多详细信息，请参阅Configuration page。 Configuring the iSCSI Target传统上，对Ceph存储集群的块级访问仅限于QEMU和librbd，这是在OpenStack环境中采用的关键因素。从Ceph Luminous版本开始，块级访问正在扩展，以提供标准的iSCSI支持，从而允许更广泛的平台使用，并有可能打开新的用例。 Red Hat Enterprise Linux/CentOS 7.5（或更高版本）；Linux内核v4.16（或更高版本） 使用ceph-ansible或使用command-line interface部署的有效Ceph Storage集群 iSCSI gateways nodes，可以与OSD nodes同一节点，也可以使用专用节点上 iSCSI front-end traffic和Ceph back-end traffic使用单独的网络 安装和配置 Ceph iSCSI gateway的方法： Using Ansible Using the Command Line Interface Configuring the iSCSI Initiators iSCSI Initiator for Linux iSCSI Initiator for Microsoft Windows iSCSI Initiator for VMware ESX 1警告：通过多个iSCSI网关导出RBD image时，不支持使用SCSI persistent group reservations（PGR）和基于SCSI 2 reservations的应用程序。 Monitoring the iSCSI GatewaysCeph为iSCSI gateway环境提供了一个附加工具，以监视导出的RADOS Block Device（RBD）images的性能。 该gwtop工具是一个类似top的工具，用于显示通过iSCSI导出到客户端的RBD images的聚合性能指标。这些指标来自Performance Metrics Domain Agent（PMDA）。来自Linux-IO target（LIO）PMDA的信息用于列出每个已导出的RBD images以及所连接的客户端及其关联的I / O指标。 Requirements 正在运行的Ceph iSCSI gateway Installing 1、在每个iSCSI gateway节点安装ceph-iscsi-tools 1yum install ceph-iscsi-tools 2、在每个iSCSI gateway节点安装performance co-pilot 1yum install pcp 3、在每个iSCSI gateway节点安装LIO PMDA 1yum install pcp-pmda-lio 4、在每个iSCSI gateway节点enable并start performance co-pilot service 12# systemctl enable pmcd# systemctl start pmcd 5、注册pcp-pmda-lio agent 12cd /var/lib/pcp/pmdas/lio./Install 默认情况下，gwtop假定iSCSI gateway configuration object被存储在rbd pool被叫做gateway.conf的RADOS object中。此configuration定义了需要收集性能信息的iSCSI gateways。可以使用-g或-c标志将其覆盖 。gwtop –help获取更多详细信息。 LIO configuration确定要从performance co-pilot提取性能统计信息的类型。当gwtop启动时，它着眼于LIO configuration，如果发现user-space的磁盘，然后gwtop 自动选择LIO收集器。 Example gwtop Outputs 1234567891011gwtop 2/2 Gateways CPU% MIN: 4 MAX: 5 Network Total In: 2M Out: 3M 10:20:00Capacity: 8G Disks: 8 IOPS: 503 Clients: 1 Ceph: HEALTH_OK OSDs: 3Pool.Image Src Size iops rMB/s wMB/s Clientiscsi.t1703 500M 0 0.00 0.00iscsi.testme1 500M 0 0.00 0.00iscsi.testme2 500M 0 0.00 0.00iscsi.testme3 500M 0 0.00 0.00iscsi.testme5 500M 0 0.00 0.00rbd.myhost_1 T 4G 504 1.95 0.00 rh460p(CON)rbd.test_2 1G 0 0.00 0.00rbd.testme 500M 0 0.00 0.00 在Client column，(CON)表示iSCSI initiator (client)当前已登录到iSCSI gateway。如果显示-multi-，则表示多个clients映射到单个RBD image。]]></content>
      <tags>
        <tag>ceph,iscsi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tgt]]></title>
    <url>%2F2019%2F09%2F23%2Ftgt%2F</url>
    <content type="text"><![CDATA[安装编译依赖1yum install gcc rpm-build libibverbs-devel librdmacm-devel libaio-devel docbook-style-xsl -y 123456789101112131415161718192021222324252627[root@dev tgt]# tree scripts/scripts/├── build-pkg.sh #build rpm或deb包├── checkarch.sh #├── checkpatch.pl #├── deb│ ├── changelog│ ├── compat│ ├── control│ ├── copyright│ ├── init│ ├── patches│ │ └── 0001-Use-local-docbook-for-generating-docs.patch│ ├── rules│ ├── source│ │ └── format│ └── tgt.bash-completion├── initd.sample├── Makefile├── tgt-admin├── tgt.bashcomp.sh├── tgt-core-test├── tgtd.service├── tgtd.spec└── tgt-setup-lun3 directories, 20 files]]></content>
      <tags>
        <tag>tgt,ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elastic stack - logstash]]></title>
    <url>%2F2019%2F09%2F18%2Felastic-stack-logstash%2F</url>
    <content type="text"><![CDATA[LogstashLogstash是开源的服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到您最喜欢的“存储库”中。（我们的存储库是Elasticsearch） Logstash部署安装12345678# 检查jdk环境，要就jdk1.8+java -version# 解压安装包tar zxvf logstash-6.5.4.tar.gz# 第一个logstash示例bin/logstash -e &apos;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123; &#125; &#125;&apos; 测试 12345678[root@dev ~]# /usr/share/logstash/bin/logstash -e &apos;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123; &#125; &#125;&apos;hello&#123; &quot;message&quot; =&gt; &quot;hello&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;@timestamp&quot; =&gt; 2019-09-19T02:29:59.833Z, &quot;host&quot; =&gt; &quot;dev&quot;&#125; 配置Logstash的配置有三部分组成： 1234567891011input &#123; # 输入 stdin &#123; ... &#125; #标准输入&#125;filter &#123; # 过滤，对数据进行分割、截取等处理 ...&#125;output &#123; # 输出 stdout &#123; ... &#125; #标准输出&#125; 输入 采集各种样式、大小和来源的数据 过滤 实时解析和转换数据 输出 选择您的存储库，导出您的数据 读取自定义日志如果是自定义结构的日志，这个时候就需要Logstash处理后才能使用。 123456789101112131415161718192021222324252627282930313233343536# Logstash配置文件，规则定义[root@dev conf.d]# cat 99-test-yujiang.conf input &#123; file &#123; path =&gt; &quot;/var/log/yujiang.log&quot; start_position =&gt; &quot;beginning&quot; &#125;&#125;filter &#123; mutate &#123; split =&gt; &#123; &quot;message&quot;=&gt;&quot;|&quot; &#125; &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug&#125;&#125;# 启动Logstash，等待自定义日志文件写入数据[root@dev conf.d]# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/99-test-yujiang.conf&#123; &quot;@version&quot; =&gt; &quot;1&quot;, &quot;message&quot; =&gt; [ [0] &quot;2019-09-19 11:20&quot;, [1] &quot;ERROR&quot;, [2] &quot;hello world&quot; ], &quot;@timestamp&quot; =&gt; 2019-09-19T03:20:21.116Z, &quot;path&quot; =&gt; &quot;/var/log/yujiang.log&quot;, &quot;host&quot; =&gt; &quot;dev&quot;&#125;# 向自定义日志文件中写入数据[root@dev log]# echo &quot;2019-09-19 11:20|ERROR|hello world&quot; &gt;&gt; yujiang.log 将自定义日志写入Elasticsearch 12345678910111213141516171819202122232425262728293031323334353637383940# Logstash配置文件，规则定义[root@dev conf.d]# cat 99-test-yujiang.confinput &#123; file &#123; path =&gt; &quot;/var/log/yujiang.log&quot; start_position =&gt; &quot;beginning&quot; &#125;&#125;filter &#123; mutate &#123; split =&gt; &#123; &quot;message&quot;=&gt;&quot;|&quot; &#125; &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [ &quot;192.168.56.101:9200&quot; ] &#125;&#125;# 在Elasticsearch-head中查看&#123; &quot;_index&quot;: &quot;logstash-2019.09.19&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;ZNuTR20BQ8jxL59AtKFm&quot;, &quot;_version&quot;: 1, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;message&quot;: [ &quot;2019-09-19 11:20&quot;, &quot;ERROR&quot;, &quot;hello world&quot; ], &quot;@timestamp&quot;: &quot;2019-09-19T03:32:04.502Z&quot;, &quot;@version&quot;: &quot;1&quot;, &quot;path&quot;: &quot;/var/log/yujiang.log&quot;, &quot;host&quot;: &quot;dev&quot; &#125;&#125;]]></content>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elastic stack - filebeat]]></title>
    <url>%2F2019%2F09%2F17%2Felastic-stack-filebeat%2F</url>
    <content type="text"><![CDATA[ELK是Elasticsearch、Logstash、Kibana的简称，这三者是核心套件，并不是全部。一般用于集中搜集并展示日志。 filebeatfilebeat的主要作用是采集日志文件，是轻量级的日志采集器。部署filebeat非常简单，这里就不做介绍了。我们来看一下filebeat的配置与使用。 简单配置及演示：​ 这里我们先看一下filebeat.inputs类型为stdin，output.console方式。启动filebeat后，它会从标准输入接收数据，并以JSON格式打印在控制台中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@dev filebeat]# cat /etc/filebeat/test_stdin.yml filebeat.inputs: # filebeat输入配置 - type: stdin # stdin表示标准输入 enabled: true # 用于启用或禁用模块output.console: # Console output enabled: true # 用于启用或禁用模块 codec.json: # 配置JSON编码 pretty: true # Pretty-print JSON event escape_html: true # 在字符串中配置转义HTML symbols[root@dev filebeat]# filebeat -e -c test_stdin.ymlhello yujiang # 在控制台输入&#123; &quot;@timestamp&quot;: &quot;2019-09-17T09:01:00.251Z&quot;, &quot;@metadata&quot;: &#123; # 元数据 &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;doc&quot;, &quot;version&quot;: &quot;6.8.3&quot; &#125;, &quot;offset&quot;: 0, &quot;log&quot;: &#123; &quot;file&quot;: &#123; &quot;path&quot;: &quot;&quot; &#125; &#125;, &quot;prospector&quot;: &#123; # 标准输入勘探器 &quot;type&quot;: &quot;stdin&quot; &#125;, &quot;input&quot;: &#123; # 控制台标准输入 &quot;type&quot;: &quot;stdin&quot; &#125;, &quot;beat&quot;: &#123; # beat版本以及hostname &quot;name&quot;: &quot;dev&quot;, &quot;hostname&quot;: &quot;dev&quot;, &quot;version&quot;: &quot;6.8.3&quot; &#125;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;dev&quot; &#125;, &quot;message&quot;: &quot;hello yujiang&quot;, # 输入的内容 &quot;source&quot;: &quot;&quot;&#125; 日志文件配置及演示1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@dev filebeat]# cat test_log.yml filebeat.inputs: # filebeat输入配置 - type: log # log表示收集日志文件 enabled: true # 用于启用或禁用模块 paths: # 日志文件位置 - /var/log/yujiang.logoutput.console: # Console output enabled: true # 用于启用或禁用模块 codec.json: # 配置JSON编码 pretty: true # Pretty-print JSON event escape_html: true # 在字符串中配置转义HTML symbols# 启动filebeat，这时开启另一个终端，向/var/log/yujiang.log文件中写入hello world[root@dev filebeat]# filebeat -e -c ./test_log.yml 2019-09-17T05:27:25.513-0400 INFO crawler/crawler.go:106 Loading and starting Inputs completed. Enabled inputs: 12019-09-17T05:28:55.525-0400 INFO log/harvester.go:255 Harvester started for file: /var/log/yujiang.log&#123; &quot;@timestamp&quot;: &quot;2019-09-17T09:28:55.525Z&quot;, &quot;@metadata&quot;: &#123; &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;doc&quot;, &quot;version&quot;: &quot;6.8.3&quot; &#125;, &quot;beat&quot;: &#123; &quot;version&quot;: &quot;6.8.3&quot;, &quot;name&quot;: &quot;dev&quot;, &quot;hostname&quot;: &quot;dev&quot; &#125;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;dev&quot; &#125;, &quot;offset&quot;: 0, &quot;log&quot;: &#123; &quot;file&quot;: &#123; &quot;path&quot;: &quot;/var/log/yujiang.log&quot; &#125; &#125;, &quot;message&quot;: &quot;hello world&quot;, &quot;source&quot;: &quot;/var/log/yujiang.log&quot;, &quot;prospector&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;input&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;&#125;# 向/var/log/yujiang.log文件中写入hello world[root@dev log]# echo &quot;hello world&quot; &gt;&gt; yujiang.log 自定义tags配置及演示1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@dev filebeat]# cat test_log_tags.yml filebeat.inputs: # filebeat输入配置 - type: log # log表示收集日志文件 enabled: true # 用于启用或禁用模块 paths: # 日志文件位置 - /var/log/yujiang.log tags: [&quot;web&quot;, &quot;ceph&quot;] # 自定义tagsoutput.console: # Console output enabled: true # 用于启用或禁用模块 codec.json: # 配置JSON编码 pretty: true # Pretty-print JSON event escape_html: true # 在字符串中配置转义HTML symbols# 启动filebeat，这时开启另一个终端，向/var/log/yujiang.log文件中写入hello tags[root@dev filebeat]# filebeat -e -c ./test_log_tags.yml &#123; &quot;@timestamp&quot;: &quot;2019-09-17T09:55:43.909Z&quot;, &quot;@metadata&quot;: &#123; &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;doc&quot;, &quot;version&quot;: &quot;6.8.3&quot; &#125;, &quot;source&quot;: &quot;/var/log/yujiang.log&quot;, &quot;offset&quot;: 12, &quot;beat&quot;: &#123; &quot;name&quot;: &quot;dev&quot;, &quot;hostname&quot;: &quot;dev&quot;, &quot;version&quot;: &quot;6.8.3&quot; &#125;, &quot;message&quot;: &quot;hello tags&quot;, &quot;log&quot;: &#123; &quot;file&quot;: &#123; &quot;path&quot;: &quot;/var/log/yujiang.log&quot; &#125; &#125;, &quot;tags&quot;: [ # 自定义tags &quot;web&quot;, &quot;ceph&quot; ], &quot;prospector&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;input&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;dev&quot; &#125;&#125;# 向/var/log/yujiang.log文件中写入hello world[root@dev log]# echo &quot;hello tags&quot; &gt;&gt; yujiang.log 自定义字段配置及演示from添加到子节点 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@dev filebeat]# cat test_log_tags_fields.ymlfilebeat.inputs: # filebeat输入配置 - type: log # log表示收集日志文件 enabled: true # 用于启用或禁用模块 paths: # 日志文件位置 - /var/log/yujiang.log tags: [&quot;web&quot;, &quot;ceph&quot;] # 添加自定义tags fields: # 添加自定义字段 from: test-web-cephoutput.console: # Console output enabled: true # 用于启用或禁用模块 codec.json: # 配置JSON编码 pretty: true # Pretty-print JSON event escape_html: true # 在字符串中配置转义HTML symbols[root@dev filebeat]# filebeat -e -c test_log_tags_fields.yml &#123; &quot;@timestamp&quot;: &quot;2019-09-17T10:14:23.323Z&quot;, &quot;@metadata&quot;: &#123; &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;doc&quot;, &quot;version&quot;: &quot;6.8.3&quot; &#125;, &quot;source&quot;: &quot;/var/log/yujiang.log&quot;, &quot;tags&quot;: [ &quot;web&quot;, &quot;ceph&quot; ], &quot;input&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;offset&quot;: 23, &quot;message&quot;: &quot;hello tags fields&quot;, &quot;prospector&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;beat&quot;: &#123; &quot;hostname&quot;: &quot;dev&quot;, &quot;version&quot;: &quot;6.8.3&quot;, &quot;name&quot;: &quot;dev&quot; &#125;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;dev&quot; &#125;, &quot;log&quot;: &#123; &quot;file&quot;: &#123; &quot;path&quot;: &quot;/var/log/yujiang.log&quot; &#125; &#125;, &quot;fields&quot;: &#123; &quot;from&quot;: &quot;test-web-ceph&quot; &#125;&#125;[root@dev log]# echo &quot;hello tags fields&quot; &gt;&gt; yujiang.log from添加到root节点 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@dev filebeat]# cat test_log_tags_fields.yml filebeat.inputs: # filebeat输入配置 - type: log # log表示收集日志文件 enabled: true # 用于启用或禁用模块 paths: # 日志文件位置 - /var/log/yujiang.log tags: [&quot;web&quot;, &quot;ceph&quot;] # 添加自定义tags fields: # 添加自定义字段 from: test-web-ceph fields_under_root: true # true为添加到根节点，false为添加到子节点output.console: # Console output enabled: true # 用于启用或禁用模块 codec.json: # 配置JSON编码 pretty: true # Pretty-print JSON event escape_html: true # 在字符串中配置转义HTML symbols[root@dev filebeat]# filebeat -e -c test_log_tags_fields.yml&#123; &quot;@timestamp&quot;: &quot;2019-09-17T10:25:29.414Z&quot;, &quot;@metadata&quot;: &#123; &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;doc&quot;, &quot;version&quot;: &quot;6.8.3&quot; &#125;, &quot;offset&quot;: 77, &quot;tags&quot;: [ &quot;web&quot;, &quot;ceph&quot; ], &quot;prospector&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;input&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;from&quot;: &quot;test-web-ceph&quot;, # from添加到root中 &quot;beat&quot;: &#123; &quot;version&quot;: &quot;6.8.3&quot;, &quot;name&quot;: &quot;dev&quot;, &quot;hostname&quot;: &quot;dev&quot; &#125;, &quot;log&quot;: &#123; &quot;file&quot;: &#123; &quot;path&quot;: &quot;/var/log/yujiang.log&quot; &#125; &#125;, &quot;message&quot;: &quot;hello tags fields fields_under_root&quot;, &quot;source&quot;: &quot;/var/log/yujiang.log&quot;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;dev&quot; &#125;&#125;[root@dev log]# echo &quot;hello tags fields fields_under_root&quot; &gt;&gt; yujiang.log 对接elasticsearch配置及演示日志内容输出到elasticsearch 12345678910111213141516171819202122232425262728[root@dev filebeat]# cat test_es.yml filebeat.inputs: # filebeat输入配置 - type: log # log表示收集日志文件 enabled: true # 用于启用或禁用模块 paths: # 日志文件位置 - /var/log/yujiang.log tags: [&quot;web&quot;, &quot;ceph&quot;] # 添加自定义tags fields: # 添加自定义字段 from: test-web-ceph fields_under_root: true # true为添加到根节点，false为添加到子节点setup.template.settings: index.number_of_shards: 3 #索引的分区数output.elasticsearch: hosts: [&quot;192.168.56.101:9200&quot;][root@dev filebeat]# filebeat -e -c test_es.yml 2019-09-17T23:10:34.272-0400 INFO log/harvester.go:255 Harvester started for file: /var/log/yujiang.log2019-09-17T23:10:35.273-0400 INFO pipeline/output.go:95 Connecting to backoff(elasticsearch(http://192.168.56.101:9200))2019-09-17T23:10:35.279-0400 INFO elasticsearch/client.go:739 Attempting to connect to Elasticsearch version 6.8.32019-09-17T23:10:35.379-0400 INFO template/load.go:128 Template already exists and will not be overwritten.2019-09-17T23:10:35.379-0400 INFO instance/beat.go:889 Template successfully loaded.2019-09-17T23:10:35.380-0400 INFO pipeline/output.go:105 Connection to backoff(elasticsearch(http://192.168.56.101:9200)) established[root@dev log]# echo &quot;hello&quot; &gt; yujiang.log Filebeat工作原理Filebeat主要由两个组件组成：prospector和harvester。 harvester 负责读取单个文件的内容 如果文件在读取时被删除或重命名，Filebeat将继续读取文件 prospector 负责管理harvester并找到所有要读取的文件来源 如果输入类型为日志，则查找器将查找路径匹配的所有文件，并为每个文件启动一个harvester Filebeat目前支持两种prospector类型：log和stdin Filebeat如何保持文件的状态 Filebeat保存每个文件的状态并将状态刷新到磁盘上的注册文件中 该状态用于记住harvester正在读取的最后偏移量，并确保发送所有日志行 如果输出（例如Elasticsearch或Logstash）无法访问，Filebeat会跟踪最后发送的行，并在输出再次可用时继续读取文件 在Filebeat运行时，每个prospector内存中也会保存文件状态信息，当重新启动Filebeat时，将使用注册文件的数据来重建文件状态，Filebeat将每个harvester在从保存的最后偏移量继续读取 文件状态记录在data/registry文件中 123一般会保存在registry文件中，记录文件的偏移量[root@dev filebeat]# cat /var/lib/filebeat/registry [&#123;&quot;source&quot;:&quot;/var/log/boot.log&quot;,&quot;offset&quot;:0,&quot;timestamp&quot;:&quot;2019-09-17T21:50:45.201469161-04:00&quot;,&quot;ttl&quot;:-2,&quot;type&quot;:&quot;log&quot;,&quot;meta&quot;:null,&quot;FileStateOS&quot;:&#123;&quot;inode&quot;:134315203,&quot;device&quot;:64768&#125;&#125;,&#123;&quot;source&quot;:&quot;/var/log/yum.log&quot;,&quot;offset&quot;:9899,&quot;timestamp&quot;:&quot;2019-09-17T21:50:45.160502101-04:00&quot;,&quot;ttl&quot;:-2,&quot;type&quot;:&quot;log&quot;,&quot;meta&quot;:null,&quot;FileStateOS&quot;:&#123;&quot;inode&quot;:134315217,&quot;device&quot;:64768&#125;&#125;,&#123;&quot;source&quot;:&quot;/var/log/yujiang.log&quot;,&quot;offset&quot;:12,&quot;timestamp&quot;:&quot;2019-09-17T23:15:39.385983011-04:00&quot;,&quot;ttl&quot;:-1,&quot;type&quot;:&quot;log&quot;,&quot;meta&quot;:null,&quot;FileStateOS&quot;:&#123;&quot;inode&quot;:134836485,&quot;device&quot;:64768&#125;&#125;] 启动参数说明1234[root@dev filebeat]# filebeat -e -c test_es.yml -d &quot;publish&quot;-e 输出到标准输出，默认输出到syslog和logs下-c 指定配置文件-d 输出debug信息 Filebeat ModuleFilebeat中，有大量的Module，可以简化我们的配置，可直接使用。在filebeat.yml中必须配置filebeat.config.modules.path目录，否则会报错Error in modules manager: modules management requires &#39;filebeat.config.modules.path&#39; setting。 123456789[root@dev filebeat]# cat /etc/filebeat/filebeat.yml # ......filebeat.config: modules: enabled: true path: /etc/filebeat/modules.d/*.yml reload.enabled: true reload.period: 10s# ...... 查看模块 123456789101112131415161718192021222324[root@dev filebeat]# filebeat modules listEnabled:systemDisabled:apache2auditdelasticsearchhaproxyicingaiisiptableskafkakibanalogstashmongodbmysqlnginxosquerypostgresqlredissuricatasystemtraefik 配置filebeat.yml 123456789101112filebeat:output: elasticsearch: hosts: [&quot;localhost:9200&quot;]filebeat.config: modules: enabled: true path: /etc/filebeat/modules.d/*.yml reload.enabled: true reload.period: 10s]]></content>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos 7 升级内核(Centos 7 upgrade kernel)]]></title>
    <url>%2F2019%2F09%2F04%2FCentos-7-%E5%8D%87%E7%BA%A7%E5%86%85%E6%A0%B8-Centos-7-upgrade-kernel%2F</url>
    <content type="text"><![CDATA[1、查看当前版本kernel (Show current version of kernel) 12[root@dev ~]# uname -r3.10.0-957.el7.x86_64 2、导入公钥 (Import the public key) 1[root@dev ~]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org 3、为CentOS-7安装ELRepo (install ELRepo for CentOS-7) 1[root@dev ~]# yum install -y https://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm 4、安装yum-plugin-fastestmirror (Install yum-plugin-fastestmirror) 1[root@dev ~]# yum install -y yum-plugin-fastestmirror 5、查看可以安装的kernel (Show the kernel that can be installed) 1[root@dev ~]# yum --enablerepo=elrepo-kernel provides kernel 6、安装最新kernel，或安装执行版本的kernel (Install the latest kernel, or install the executable version of the kernel) 123[root@dev ~]# yum --enablerepo=elrepo-kernel install kernel-mlor[root@dev ~]# yum --enablerepo=elrepo-kernel install kernel-lt-4.4.190-1.el7.elrepo.x86_64 7、设置默认的启动内核 (Set the default boot kernel) 1234567891011[root@dev ~]# cat /boot/grub2/grub.cfg | grep &quot;CentOS Linux&quot;menuentry &apos;CentOS Linux (4.4.190-1.el7.elrepo.x86_64) 7 (Core)&apos; --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option &apos;gnulinux-3.10.0-862.el7.x86_64-advanced-a66de569-ad95-4599-9f1e-37c19744ace0&apos; &#123;menuentry &apos;CentOS Linux (3.10.0-957.21.3.el7.x86_64) 7 (Core)&apos; --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option &apos;gnulinux-3.10.0-862.el7.x86_64-advanced-a66de569-ad95-4599-9f1e-37c19744ace0&apos; &#123;menuentry &apos;CentOS Linux (3.10.0-862.3.2.el7.x86_64) 7 (Core)&apos; --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option &apos;gnulinux-3.10.0-862.el7.x86_64-advanced-a66de569-ad95-4599-9f1e-37c19744ace0&apos; &#123;menuentry &apos;CentOS Linux (3.10.0-862.el7.x86_64) 7 (Core)&apos; --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option &apos;gnulinux-3.10.0-862.el7.x86_64-advanced-a66de569-ad95-4599-9f1e-37c19744ace0&apos; &#123;menuentry &apos;CentOS Linux (0-rescue-167a8b301e76475680ccb38e7d691aab) 7 (Core)&apos; --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option &apos;gnulinux-0-rescue-167a8b301e76475680ccb38e7d691aab-advanced-a66de569-ad95-4599-9f1e-37c19744ace0&apos; &#123;[root@dev ~]# grub2-set-default &quot;CentOS Linux (4.4.190-1.el7.elrepo.x86_64) 7 (Core)&quot;[root@dev ~]# grub2-editenv listsaved_entry=CentOS Linux (4.4.190-1.el7.elrepo.x86_64) 7 (Core) 8、重启机器 (Restart the machine) 1[root@dev ~]# reboot 9、重启后验证是否成功 (Verify successful after reboot) 12[root@dev ~]# uname -r4.4.190-1.el7.elrepo.x86_64 10、删除老kernel，–count flag用于指定要在系统上保留的内核数 (Delete the old kernel, the –count flag is used to specify the number of cores to keep on the system) 123[root@dev ~]# yum install -y yum-utils[root@dev ~]# package-cleanup --oldkernels --count=2]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LinuxCast]Linux网络基础配置 - 笔记]]></title>
    <url>%2F2019%2F08%2F28%2FLinuxCast-Linux%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE-%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[以太网连接 在Linux中，以太网接口被命名为：eth0、eth1等，0、1代表网卡编号。 通过lspci命令可以查看网卡硬件信息（如果是usb网卡，则可能需要使用lsusb命令） 命令ifconfig命令用来查看接口信息 ifconfig -a 查看所有接口 ifconfig eth0 查看特定接口 命令ifup、ifdown用来启用、禁用一个接口 ifup eth0 ifdown eht0 123456789101112[root@aio1 ~]# ifconfig ens33ens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 9000 inet 192.168.46.200 netmask 255.255.255.0 broadcast 192.168.46.255 inet6 fe80::b3f5:7411:df98:1d00 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:1d:7b:de txqueuelen 1000 (Ethernet) RX packets 16673 bytes 23150661 (22.0 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 2645 bytes 223690 (218.4 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0RX packets 接收包数量TX packets 发送包数量 网络相关配置文件 网卡配置文件 /etc/sysconfig/network-scripts/ifcfg-ens8f0 1234567891011121314151617181920TYPE=Ethernet # 类型=以太网PROXY_METHOD=none # BROWSER_ONLY=no # BOOTPROTO=none # 启动协议DEFROUTE=yes # IPV4_FAILURE_FATAL=no # IPV6INIT=yes # IPV6_AUTOCONF=yes # IPV6_DEFROUTE=yes # IPV6_FAILURE_FATAL=no # IPV6_ADDR_GEN_MODE=stable-privacy # NAME=ens33 # UUID=3434b388-c4db-4130-bf90-3e34115fe2d3 # UUID，统一标识符DEVICE=ens33 # 设备名称ONBOOT=yes # 服务器启动时，自动启用IPADDR=192.168.46.200 # IP地址NETMASK=255.255.255.0 # 子网掩码GATEWAY=192.168.46.2 # 网关DNS1=192.168.46.2 # DNS DNS配置文件 /etc/resolv.conf 123# Generated by NetworkManagersearch localdomainnameserver 192.168.46.2 主机名配置文件 /etc/sysconfig/network 静态主机名配置文件 /etc/hosts 网络测试命令 测试网络连通性 ping 192.168.1.1 ping www.baidu.com 测试DNS解析 1yum install -y bind-utils host www.baidu.com 12345域名到IP地址的解析[root@dev ~]# host www.baidu.comwww.baidu.com is an alias for www.a.shifen.com.www.a.shifen.com has address 61.135.169.125www.a.shifen.com has address 61.135.169.121 dig www.baidu.com 显示路由表 ip route 123456789101112[root@aio1 ~]# ip routedefault via 192.168.46.2 dev ens33 proto static metric 100 #跟我不在一个网段的，发给defaultdefault via 192.168.46.2 dev ens34 proto static metric 101 default via 192.168.46.2 dev ens35 proto static metric 102 default via 192.168.46.2 dev ens36 proto dhcp metric 103 172.29.232.0/22 dev br-dbaas proto kernel scope link src 172.29.232.100 192.168.46.0/24 dev ens33 proto kernel scope link src 192.168.46.200 metric 100 192.168.46.0/24 dev ens36 proto kernel scope link src 192.168.46.137 metric 103 192.168.46.2 dev ens34 proto static scope link metric 101 192.168.46.2 dev ens35 proto static scope link metric 102 192.168.100.0/24 dev ens34 proto kernel scope link src 192.168.100.200 metric 101 192.168.200.0/24 dev ens35 proto kernel scope link src 192.168.200.200 metric 102 追踪到达目标地址的网络路径 1yum install -y traceroute traceroute www.baidu.com 使用mtr进行网络质量测试（结合了traceroute和ping） 1yum install -y mtr mtr www.baidu.com 修改主机名 实时修改主机名 1hostname train.linuxcast.net 永久性修改主机名 123/etc/sysconfig/networkHOSTNAME=train.linuxcast.net 故障排查网络故障排查遵循从底层到高层、从自身到外部的流程进行 先查看网络配置信息是否正确 IP地址 子网掩码 网关 DNS 查看到达网关是否连通 ping 网关IP地址 查看DNS解析是否正常 1host www.baidu.com traceroute www.baidu.com]]></content>
      <tags>
        <tag>linux网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 常用操作]]></title>
    <url>%2F2019%2F08%2F20%2Fgit-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[视频地址：https://www.youtube.com/channel/UCPhn2rCqhu0HdktsFjixahA 原文地址：https://github.com/twtrubiks/Git-Tutorials 本文档只作为学习笔记，方便自己速查，如果想系统学习请看沈老师的视频和github。 如何加速大型repo clone速度 –depth参数：只下载最新1次commits log，默认会加–single-branch（只clone单分支，无法checkout）。如果想clone所有分支需要加–no-single-branch（git clone https://github.com/ceph/ceph.git –depth 1 –no-single-branch），可以checkout。 1234567891011121314151617181920yujiangdeMacBook-Pro-13:test yujiang$ git clone https://github.com/ceph/ceph.git --depth 1Cloning into &apos;ceph&apos;...remote: Enumerating objects: 8395, done.remote: Counting objects: 100% (8395/8395), done.remote: Compressing objects: 100% (7508/7508), done.remote: Total 8395 (delta 1133), reused 2722 (delta 467), pack-reused 0Receiving objects: 100% (8395/8395), 21.01 MiB | 1.52 MiB/s, done.Resolving deltas: 100% (1133/1133), done.Checking out files: 100% (8847/8847), done.yujiangdeMacBook-Pro-13:test yujiang$ cd ceph/yujiangdeMacBook-Pro-13:ceph yujiang$ git logcommit 6b0ef5dc3c550cd8d17c830156541dd491e9a57a (grafted, HEAD -&gt; master, origin/master, origin/HEAD)Author: Alfredo Deza &lt;adeza@redhat.com&gt;Date: Tue Aug 20 09:32:05 2019 -0400 Merge pull request #29762 from alfredodeza/bz-1738379 ceph-volume: use the OSD identifier when reporting success Reviewed-by: Jan Fajerski &lt;jfajerski@suse.com&gt; git subtreegit submodule是link的概念 git subtree是copy的概念 https://github.com/git/git/blob/master/contrib/subtree/git-subtree.txt create git subtree12345678910111213141、首先clone主repogit clone --recurse-submodules -j8 git@github.com:lnsyyj/lnsyyj-ansible.git2、进入主repocd lnsyyj-ansible3、添加子repogit subtree add --prefix=roles/elasticsearch --squash git@github.com:lnsyyj/ansible-role-elasticsearch.git mastergit subtree add --prefix=roles/kibana --squash git@github.com:lnsyyj/ansible-role-kibana.git mastergit subtree add --prefix=roles/cloudalchemy.node-exporter --squash git@github.com:cloudalchemy/ansible-node-exporter.git mastergit subtree add --prefix=roles/cloudalchemy.prometheus --squash git@github.com:cloudalchemy/ansible-prometheus.git master--squash 合并子repo的git log--prefix= 指定copy到主repo的位置 push git subtree12 pull git subtree12 常见问题112345678fatal: early EOFfatal: the remote end hung up unexpectedlyfatal: index-pack failederror: RPC failed; curl 18 transfer closed with outstanding read data remaining解决办法：git config --global http.postBuffer 5242880000git clone https://github.com/ansible/ansible.git]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos创建内部mirror]]></title>
    <url>%2F2019%2F08%2F20%2Fcentos%E5%88%9B%E5%BB%BA%E5%86%85%E9%83%A8mirror%2F</url>
    <content type="text"><![CDATA[公司内部搭建mirror的好处：可以节省大量时间、控制版本、节省公司外网带宽（尤其是国内很多公司基于开源软件搞定制化二次开发），同时也可以纳入发布流程当中。 yum可以通过FTP或HTTP传输文件，这里只实验HTTP方式。 server端（搭建的mirror端）1、首先安装Nginx 1sudo yum install -y epel-release &amp;&amp; sudo yum install -y nginx 2、安装createrepo工具（负责将.rpm放到repomd repository） 1sudo yum install -y createrepo yum-utils 3、在本地创建目录，存储repositories 1sudo mkdir -p /usr/share/nginx/repos/ceph/rpm-nautilus/el7/&#123;SRPMS,aarch64,noarch,x86_64&#125; 4、修改centos源 12345678910111213141516cat /etc/yum.repos.d/ceph_163.repo # $basearch is x86_64, can be modified[ceph]baseurl = http://mirrors.163.com/ceph/rpm-nautilus/el7/$basearchgpgcheck = 0gpgkey = http://mirrors.163.com/ceph/keys/release.ascname = Ceph Stable $basearch repopriority = 2[noarch]baseurl = http://mirrors.163.com/ceph/rpm-nautilus/el7/noarchgpgcheck = 0gpgkey = http://mirrors.163.com/ceph/keys/release.ascname = Ceph Stable noarch repopriority = 2 5、导入GPG Key 1curl https://mirrors.163.com/ceph/keys/release.asc | gpg --import - 6、下载官方repositories到本地服务器 1234sudo reposync -g -l -d -m --repoid=ceph --newest-only --download-metadata --download_path=/usr/share/nginx/repos/ceph/rpm-nautilus/el7/x86_64/sudo reposync -g -l -d -m --repoid=noarch --newest-only --download-metadata --download_path=/usr/share/nginx/repos/ceph/rpm-nautilus/el7/noarch/同步后，目录层级会不同，在download_path后面会自动加一层repoid指定的名字，需要自行调整。 7、创建new repository 1234sudo createrepo /usr/share/nginx/repos/ceph/rpm-nautilus/el7/x86_64/sudo createrepo /usr/share/nginx/repos/ceph/rpm-nautilus/el7/noarch/这时会在相应的目录中生成repodata 8、修改Nginx配置 12345678server &#123; # 修改 autoindex on; root /usr/share/nginx/repos/;&#125;启动nginxsystemctl start nginx &amp;&amp; systemctl enable nginx client端（使用mirror端）在/etc/yum.repos.d/中增加新的源 1234567891011121314[root@dev ~]# cat /etc/yum.repos.d/ceph_stable.repo [ceph_stable]baseurl = http://10.121.9.103/ceph/rpm-nautilus/el7/$basearchgpgcheck = 1gpgkey = https://download.ceph.com/keys/release.ascname = Ceph Stable $basearch repopriority = 2[ceph_stable_noarch]baseurl = http://10.121.9.103/ceph/rpm-nautilus/el7/noarchgpgcheck = 1gpgkey = https://download.ceph.com/keys/release.ascname = Ceph Stable noarch repopriority = 2]]></content>
      <tags>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph社区为什么要使用ceph-volume代替ceph-disk]]></title>
    <url>%2F2019%2F08%2F16%2Fceph%E7%A4%BE%E5%8C%BA%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8ceph-volume%E4%BB%A3%E6%9B%BFceph-disk%2F</url>
    <content type="text"><![CDATA[原文地址 OVERVIEWceph-volume旨在成为一个单一用途的命令行工具，将logical volumes部署为OSDs，在preparing, activating, 和creating OSDs时，试图维护与ceph-disk类似的API。 因为它不依赖于为CEPH安装的UDEV规则且没有交互，因此它背离了ceph-disk。这些rules允许自动检测先前设置的devices，这些devices又被送入ceph-disk以activate它们。 REPLACING CEPH-DISKceph-disk工具是在项目需要支持许多不同类型的init系统（upstart, sysvinit, etc…）时创建的，同时能够发现devices。这导致工具最初（以及后来）专注于GPT分区上。特别是在GPT GUID上，它以独特的方式标记device，以回答以下问题： device是否是Journal？ 是否是encrypted data partition（加密数据分区）？ device是否已部分准备就绪？ 为了解决这些问题，它使用UDEV规则来匹配GUID，这些GUID将调用ceph-disk，最后在ceph-disk systemd unit和ceph-disk可执行文件之间来回切换。该过程非常不可靠且耗时（必须为每个OSD设置接近3小时的超时），并且会导致OSD 在节点的boot过程中根本不出现。 考虑到UDEV的异步行为，很难调试甚至复现这些问题。 由于ceph-disk的world-view必须是专门的GPT分区，这意味着它无法与其他技术（如LVM或类似的device mapper devices）一起使用。它最终决定创建一些模块化的东西，从LVM支持开始，并根据需要扩展其他技术。 GPT PARTITIONS ARE SIMPLE?尽管分区通常很容易理解，但ceph-disk分区并不简单。它需要大量特殊标志才能使它们与device discovery工作流程一起正常工作。以下是创建数据分区的示例调用： 1/sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:f0fc39fd-eeb2-49f1-b922-a11939cf8a0f --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb 创建这些不仅很难，而且这些分区要求device由 Ceph 独占。例如，在某些情况下，在device加密时将创建一个特殊分区，其中包含未加密的密钥。这是ceph-disk领域的知识，它不会转变对”GPT partitions are simple”的理解。下面是正在创建的特殊分区的示例： 1/sbin/sgdisk --new=5:0:+10M --change-name=5:ceph lockbox --partition-guid=5:None --typecode=5:fb3aabf9-d25f-47cc-bf5e-721d181642be --mbrtogpt -- /dev/sdad MODULARITYceph-volume被设计成一个模块化工具，因为我们预计人们会使用多种方式来配置硬件设备。目前已有两个情况：传统的ceph-disk devices仍然在使用并且有GPT分区（handled by simple）和lvm。 我们直接从用户空间管理NVMe devices的SPDK devices即将出现，LVM将无法在那里工作，因为根本不涉及内核。 CEPH-VOLUME LVM通过使用LVM tags，lvm子命令能够保存并随后重新发现和查询与OSD相关联的devices，以便可以激活（activate）它们。 这包括对基于LVM的技术（如dm-cache）的支持。 对于ceph-volume，dm-cache的使用是透明的，对于工具没有区别，它将dm-cache视为普通logical volume。 LVM PERFORMANCE PENALTY简而言之：我们未能注意到与LVM更改相关的任何重大性能损失。 通过与LVM合作，可以使用其他device mapper技术（例如dmcache）：处理任何位于Logical Volume以下的事情没有技术困难。]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prometheus alertmanager]]></title>
    <url>%2F2019%2F08%2F08%2FPrometheus-alertmanager%2F</url>
    <content type="text"><![CDATA[最近在研究Ceph的监控系统，Ceph 14.2.1的MGR中已经有Prometheus Plugin，可以替代原来的ceph_exporter项目，作为新的exporter。MGR中的Prometheus Plugin可以对Ceph的一些状态进行监控（大概390+个监控指标），基本涵盖了能想到的还有想不到的监控指标，打开了认识Ceph的另一个窗口。enable起这个Plugin并不是难点，难点在怎样从中捞取有价值的数据，这是需要长期积累持续挖掘的问题，你懂得。。。。。。说了一堆XXX，这里只做一个prometheus与alertmanager集成的入门实验。 prometheus与alertmanager是相互独立的组件，我们先来说一下它们的职责： prometheus端负责产生告警，并发送到alertmanager端 需要配置alertmanager服务在哪、告警产生的条件（alert rule） alertmanager端负责接收prometheus发来的告警，然后做一些后续处理（比如把告警信息发出到email、微信、钉钉……） 需要配置route、receivers 假设有这样一个场景：Prometheus已经收集到了Ceph的监控指标，如果集群中有OSD down超过1小时，那么我们需要发送邮件给相关人员，告知需要处理。]]></content>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible常用模块与操作]]></title>
    <url>%2F2019%2F07%2F20%2FAnsible%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97%E4%B8%8E%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[ansible命令模式如何查看模块帮助12345列出所有模块ansible-doc -l查看指定模块帮助ansible-doc -s MODULE_NAME ansible命令应用基础12345678语法（新版）ansible &lt;host-pattern&gt; [options]语法（旧版）ansible &lt;host-pattern&gt; [-f forks] [-m module_name] [-a args] -f forks：启动的并发线程数 -m module_name：要使用的模块 -a args：模块特有的参数 常见模块123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475command：命令模块，默认模块，用于在远程执行命令 例如： ansible all -a &apos;date&apos; ansible all -m command -a &apos;date&apos;cron： state： present：安装 absent：移除 例如： ansible webserver -m cron -a &apos;minute=&quot;*/10&quot; job=&quot;/bin/echo hello&quot; name=&quot;test cron job&quot;&apos; ansible webserver -a &apos;crontab -l&apos; ansible webserver -m cron -a &apos;minute=&quot;*/10&quot; job=&quot;/bin/echo hello&quot; name=&quot;test cron job&quot; state=absent&apos;user： name=：指明创建的用户的名字 例如： ansible all -m user -a &apos;name=&quot;user1&quot;&apos; ansible all -m user -a &apos;name=&quot;user1&quot; state=absent&apos;group： 例如： ansible webserver -m group -a &apos;name=mysql gid=306 system=yes&apos; ansible webserver -m user -a &apos;name=mysql uid=306 system=yes group=mysql&apos; copy： src=：定义本地源文件路径 dest=：定义远程目标文件路径 content=：取代src=，表示直接用此处指定的信息生成为目标文件内容 例如: ansible all -m copy -a &apos;src=/etc/fstab dest=/tmp/fstab.ansible owner=root mode=640&apos; ansible all -m copy -a &apos;content=&quot;Hello Ansible\nHi MageEdu&quot; dest=/tmp/test.ansible&apos;file：设定文件属性 path=：指定文件路径，可以使用name或dest来替换 创建文件的符号链接： src=：指明源文件 path=：指明符号链接文件路径 例如： ansible all -m file -a &apos;owner=mysql group=mysql mode=644 path=/tmp/fstab.ansible&apos; ansible all -m file -a &apos;path=/tmp/fstab.link src=/tmp/fstab.ansible state=link&apos;ping：测试指定主机是否能连接 例如： ansible all -m pingservice：指定服务运行状态 enabled=：是否开机自动启动，取值为true或者false name=：服务名称 state=：状态，取值有started，stopped，restarted 例如： ansible webserver -a &apos;service httpd status&apos; ansible webserver -a &apos;chkconfig --list httpd&apos; ansible webserver -m service -a &apos;enabled=true name=httpd state=started&apos; shell：在远程主机上运行命令，尤其是用到管道等功能的复杂命令 例如： ansible all -m user -a &apos;name=user1&apos; ansible all -m command -a &apos;echo mageedu | passwd --stdin user1&apos; （command模块|管道符无法送过去） ansible all -m shell -a &apos;echo mageedu | passwd --stdin user1&apos; （有管道或变量最好使用shell模块）script：将本地脚本复制到远程主机并运行 注意：要使用相对路径指定脚本 例如： ansible all -m script -a &quot;test.sh&quot;yum：安装程序包 name=：指明要安装的程序包，可以带上版本号 state=：present，latest表示安装，absent表示卸载 例如： ansible all -m yum -a &quot;name=vim&quot;setup：收集远程主机的facts 每个被管理节点在接收并运行管理命令之前，会将自己主机相关信息，如操作系统版本、IP地址等报告给远程的ansible主机 ansible all -m setup ansible-playbook预备知识YMAL语法123456789101112131415YAML的语法和其他高阶语言类似，并且可以简单表达清单、散列表、标量等数据结构。其结构（Structure）通过空格来展示，序列（Sequence）里的项用&quot;-&quot;来代表，Map里的键值对用&quot;:&quot;分隔。下面是一个示例。name: John Smithage: 41gender: Malespouse: name: Jane Smith age: 37 gender: Femalechildren: - name: Jimmy Smith age: 17 gender: Male - name: Jenny Smith age: 13 gender: Female YMAL常见的数据类型list 列表的所有元素均使用”-“开头，例如： 12345# A list of stasty fruits- Apple- Orange- Strawberry- Mango dictionary 字典通过key与value进行标识，例如： 123456789# An employee recordname: Example Developerjob: Developerskill: Elite也可以将key: value放置于&#123;&#125;中进行表示，例如：---# An employee record&#123;name: Example Developer, job: Developer, skill: Elite&#125; 基础元素变量 变量命名 变量名仅能由字母、数字和下划线组成，且只能以字母开头。 facts facts是由正在通信的远程目标主机发回的信息，这些信息被保存在ansible变量中，要获取指定的远程主机所支持的所有facts，可以使用命令： 1ansible hostname -m setup register 把任务的输出定义为变量，然后用于其他任务，示例如下： 1234tasks: - shell: /usr/bin/foo register: foo_result ignore_errors: True 通过命令行传递变量 在运行playbook的时候也可以传递一些变量供playbook使用，示例如下： 1ansible-playbook test.yml --extra-vars &quot;hosts=www user=mageedu&quot; 通过roles传递变量 当给一个主机应用角色的时候可以传递变量，然后在角色内使用这些变量，示例如下： 1234- hosts: webservers roles: - common - &#123; role: foo_app_instance, dir: &apos;/web/htdocs/a.com&apos;, port: 8080 &#125; Inventoryansible主要用在批量主机操作，为了便携的使用其中的部分主机，可以在inventory file中将其分组命名。默认的inventory file为/etc/ansible/hosts。 inventory file可以有多个，且也可以通过Dynamic Inventory来动态生成。 inventory文件格式 inventory文件遵循INI文件风格，中括号中的字符为组名。可以将同一个主机同时归并到多个不同的组中。此外，如果目标主机使用了非默认的SSH端口，还可以在主机名称之后使用冒号加端口号来标明。 123456789101112131415[webservers]www1.magedu.com:2222www2.magedu.com[dbservers]db1.magedu.comdb2.magedu.comdb3.magedu.com如果主机名称遵循相似的命名模式，还可以使用列表的方式标识各主机，例如：[webservers]www[01:50].example.com[databases]db-[a:f].example.com 主机变量 可以在inventory中定义主机时为其添加主机变量以便于在playbook中使用，例如： 123[webservers]www1.magedu.com http_port=80 maxRequestsPerChild=808www2.magedu.com http_port=8080 maxRequestsPerChild=909 组变量 组变量是赋予给指定组内所有主机上的在playbook中可用的变量。例如： 1234567[webservers]www1.magedu.comwww2.magedu.com[webservers:vars]ntp_server=ntp.magedu.comnfs_server=nfs.magedu.com 组嵌套 inventory中，组还可以包含其他的组，并且也可以向组中的主机指定变量。不过，这些变量只能在ansible-playbook中使用，而ansible不支持。例如： 1234567891011121314[apache]httpd1.magedu.comhttpd2.magedu.com[nginx]ngx1.magedu.comngx2.magedu.com[webservers:children]apachenginx[webservers:vars]ntp_server=ntp.magedu.com inventory参数 ansible基于ssh连接inventory中指定的远程主机时，还可以通过参数指定其交互方式，https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html ansible-playbookplaybook是由一个或多个play组成的列表，play的主要功能在于将事先归并为一组的主机装扮成事先通过ansible中的task定义好的角色。从根本上来讲，所谓task无非是调用ansible的一个module。将多个play组织在一个playbook中，即可以让它们连同起来按事先编排的机制同唱一台大戏。下面是一个简单示例。 12345678910111213- host: webnodes vars: http_port: 80 max_clients: 256 remote_user: root tasks: - name: ensure apache is at the latest version yum: name=httpd state=latest - name: ensure apache is running service: name=httpd state=started handlers: - name: restart apache service: name=httpd state=restarted]]></content>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos yum 详解]]></title>
    <url>%2F2019%2F07%2F10%2Fcentos-yum-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[NAMEyum - Yellowdog Updater Modified SYNOPSISyum [options] [command] [package …] DESCRIPTION yum是一个基于rpm的交互式package manager。 它可以自动执行系统更新，包括依赖分析和基于repository元数据的过时处理。 它还可以安装新软件包，删除旧软件包以及对已安装的或者可用软件包执行查询。yum类似于apt-get和smart等其他高级包管理器。 command是以下之一： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768* install package1 [package2] [...]* update [package1] [package2] [...]* update-to [package1] [package2] [...]* update-minimal [package1] [package2] [...]* check-update* upgrade [package1] [package2] [...]* upgrade-to [package1] [package2] [...]* distribution-synchronization [package1] [package2] [...]* remove | erase package1 [package2] [...]* autoremove [package1] [...]* list [...]* info [...]* provides | whatprovides feature1 [feature2] [...]* clean [ packages | metadata | expire-cache | rpmdb | plugins | all ]* makecache [fast]* groups [...]* search string1 [string2] [...]* shell [filename]* resolvedep dep1 [dep2] [...] (maintained for legacy reasons only - use repoquery or yum provides)* localinstall rpmfile1 [rpmfile2] [...] (maintained for legacy reasons only - use install)* localupdate rpmfile1 [rpmfile2] [...] (maintained for legacy reasons only - use update)* reinstall package1 [package2] [...]* downgrade package1 [package2] [...]* deplist package1 [package2] [...]* repolist [all|enabled|disabled]* repoinfo [all|enabled|disabled]* repository-packages &lt;enabled-repoid&gt; &lt;install|remove|remove-or-reinstall|remove-or-distribution-synchronization&gt; [package2] [...]* version [ all | installed | available | group-* | nogroups* | grouplist | groupinfo ]* history [info|list|packages-list|packages-info|summary|addon-info|redo|undo|rollback|new|sync|stats]* load-transaction [txfile]* updateinfo [summary | list | info | remove-pkgs-ts | exclude-updates | exclude-all | check-running-kernel]* fssnapshot [summary | list | have-space | create | delete]* fs [filters | refilter | refilter-cleanup | du]* check* help [command]除非给出--help或-h选项，否则必须存在上述命令之一。常用的命令如下：install 用于安装最新版本的软件包或软件包组，同时确保满足所有依赖项。update 如果在没有指定任何包的情况下，update将更新所有当前安装的包。如果指定了一个或多个包或包，则Yum将仅更新列出的包。在更新包时，yum将确保满足所有依赖关系。update-to 此命令的作用类似于“update”，但是要指定包的版本。check-update 检查是否有需要更新的包。remove or erase 用于从系统中删除指定的包。autoremove 用于从系统中删除指定的包以及依赖。list 用于列出有关可用包的各种信息。provides or whatprovides 用于找出哪个包提供某些功能或文件。 search 当您对包知道一些但不确定它的名称时，这用于查找包。 默认情况下，搜索将尝试仅搜索包名称和摘要，但如果“失败”，则会尝试描述和URL。]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用操作]]></title>
    <url>%2F2019%2F07%2F02%2FLinux%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Python pipPython pip安装 12345yum -y install python-setuptoolseasy install pip或者curl &quot;https://bootstrap.pypa.io/get-pip.py&quot; -o &quot;get-pip.py&quot;python get-pip.py Python pip更新包 12345pip install --upgrade setuptools -i https://pypi.tuna.tsinghua.edu.cn/simple更新包是遇到类似问题，需要重新装ERROR: Cannot uninstall &apos;pyparsing&apos;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.sudo pip install -I tox -i https://pypi.tuna.tsinghua.edu.cn/simple Python pip国内源指定 123456pip install setuptools -i https://pypi.tuna.tsinghua.edu.cn/simple或者在 ~/.pip/pip.conf 中添加[global]index-url=http://pypi.douban.com/simple/#index-url=https://pypi.tuna.tsinghua.edu.cn/simple Python开源项目编译安装 123451、下载开源项目源代码2、安装依赖pip install -r requirements.txt3、安装开源项目python setup.py install Python安装.whl文件 1pip install some-package.whl 安装python3 12345yum -y install python36 python36-tools安装python3的pipcurl &quot;https://bootstrap.pypa.io/get-pip.py&quot; -o &quot;get-pip.py&quot;python3.6 get-pip.py Centos 7升级GCC 7123456781、确定g++默认使用的C++标准：g++ -dM -E -x c++ /dev/null | grep -F __cplusplus2、安装devtoolset-7yum install -y centos-release-scl &amp;&amp; yum install -y devtoolset-7-gcc-c++3、使用GCC 7scl enable devtoolset-7 bash Centos安装扩展源1sudo yum install -y epel-release Linux批量创建嵌套目录1mkdir -pv roles/vdbench/&#123;tasks,templates,meta,defaults,vars,files,plugins,handler&#125; 压缩 tar.gz格式 12压缩（把ceph-14.2.1目录压缩为ceph-14.2.1.tar.gz）：tar zcf ceph-14.2.1.tar.gz ceph-14.2.1解压缩：tar zxf ceph-14.2.1.tar.gz iperf检查网络带宽123456sudo yum install -y epel-releasesudo yum install -y iperf服务端 iperf -s -p 12345 -i 1 -M客户端 iperf -c 192.168.1.10 -p 12345 -i 1 -t 600 -w 100M ubuntu 1804添加repository1234567sudo apt updatesudo apt install software-properties-commonsudo add-apt-repository --yes --update ppa:ansible/ansible-2.8 （ 或 sudo apt-add-repository --yes --update ppa:ansible/ansible ）sudo apt install ansiblerepository站点地址https://launchpad.net/~ansible/+archive/ubuntu/ansible-2.8 查看硬盘类型12cat /sys/block/sda/queue/rotational0为ssd，1为hdd]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph数据正确性与一致性守护者Scrub读书笔记]]></title>
    <url>%2F2019%2F06%2F03%2FCeph%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%B8%80%E8%87%B4%E6%80%A7%E5%AE%88%E6%8A%A4%E8%80%85Scrub%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Ceph关于Scrub相关配置 12345678910111213141516171819202122232425262728293031323334353637383940OPTION(mon_warn_not_scrubbed, OPT_INT) //OPTION(mon_warn_not_deep_scrubbed, OPT_INT) //OPTION(mon_scrub_interval, OPT_INT) // 默认值：一天一次OPTION(mon_scrub_timeout, OPT_INT) // 默认值：5分钟OPTION(mon_scrub_max_keys, OPT_INT) // 每次scrub的最大key数OPTION(mon_scrub_inject_crc_mismatch, OPT_DOUBLE) // 注入crc不匹配的概率[0.0,1.0]OPTION(mon_scrub_inject_missing_keys, OPT_DOUBLE) // 注入缺失key的概率[0.0,1.0]OPTION(mds_max_scrub_ops_in_progress, OPT_INT) // 并行执行的最大scrub操作数，允许同时scrub的数量OPTION(osd_op_queue_mclock_scrub_res, OPT_DOUBLE) // OPTION(osd_op_queue_mclock_scrub_wgt, OPT_DOUBLE) // OPTION(osd_op_queue_mclock_scrub_lim, OPT_DOUBLE) // OPTION(osd_scrub_invalid_stats, OPT_BOOL) // OPTION(osd_max_scrubs, OPT_INT) // 单个OSD最大并发scrub数OPTION(osd_scrub_during_recovery, OPT_BOOL) // 当OSD上的PG正在recovery时允许scrubOPTION(osd_scrub_begin_hour, OPT_INT) // 限制scrubbing在每天几点开始OPTION(osd_scrub_end_hour, OPT_INT) // 限制scrubbing在每天几点结束OPTION(osd_scrub_begin_week_day, OPT_INT) // 限制scrubbing在一周中的哪天开始（星期几开始，0 or 7 = Sunday, 1 = Monday, etc.）OPTION(osd_scrub_end_week_day, OPT_INT) // 限制scrubbing在一周中的哪天结束（星期几结束，0 or 7 = Sunday, 1 = Monday, etc.）OPTION(osd_scrub_load_threshold, OPT_FLOAT) // 允许scrubbing低于系统负载（system load）除以CPU数量的值OPTION(osd_scrub_min_interval, OPT_FLOAT) // 每隔一次PG Scrub频率不超过此间隔（如果负载低）OPTION(osd_scrub_max_interval, OPT_FLOAT) // 每隔一次PG Scrub频率不低于此间隔（无论负载如何）OPTION(osd_scrub_interval_randomize_ratio, OPT_FLOAT) // scrub间隔的比率随机变化，这可以通过随机改变scrub间隔来防止scrub 踩踏，以便它们很快在一周内均匀分布，在[min，min *（1 + randomize_ratio））的范围内随机化调度scrubOPTION(osd_scrub_backoff_ratio, OPT_DOUBLE) // scrub调度尝试失败后的退避率，the probability to back off the scheduled scrubOPTION(osd_scrub_chunk_min, OPT_INT) // 在单个chunk中scrub的最小对象数OPTION(osd_scrub_chunk_max, OPT_INT) // 在单个chunk中scrub的最大对象数OPTION(osd_scrub_sleep, OPT_FLOAT) // 持续scrub过程中注入延迟（在deep scrubbing操作之间sleep）OPTION(osd_scrub_auto_repair, OPT_BOOL) // 自动修复scrub过程中检测到的受损对象（是否在deep-scrubbing时自动修复不一致）OPTION(osd_scrub_auto_repair_num_errors, OPT_U32) // 自动修复的最大检测错误数（仅在错误数低于此阈值时自动修复）OPTION(osd_deep_scrub_interval, OPT_FLOAT) // Deep scrub每个PG（即，验证数据校验和）（每周一次）OPTION(osd_deep_scrub_randomize_ratio, OPT_FLOAT) // Deep scrub间隔比例随机变化，这可以通过随机改变scrub间隔来防止deep scrub 踩踏，当没有用户启动scrub时添加随机，scrub将随机变成这种速率的deep scrub（0.15 - &gt; 15％ deep scrub）OPTION(osd_deep_scrub_stride, OPT_INT) // Deep scrub期间一次从对象读取的字节数OPTION(osd_deep_scrub_keys, OPT_INT) // Deep scrub期间一次从对象读取的key数OPTION(osd_deep_scrub_update_digest_min_age, OPT_INT) // 仅当对象的上次修改时间超过此时间时才更新整体对象digest（摘要）OPTION(osd_deep_scrub_large_omap_object_key_threshold, OPT_U64) // 当我们遇到某个对象的omap key多于此阈值时，发出警告OPTION(osd_deep_scrub_large_omap_object_value_sum_threshold, OPT_U64) // 当我们遇到某个对象的omap key bytes大小多于此阈值时，发出警告OPTION(osd_debug_deep_scrub_sleep, OPT_FLOAT) // 在deep scrub IO期间注入sleep以使其更容易induce preemption（诱导抢占）OPTION(osd_scrub_priority, OPT_U32) // 工作队列中的scrub操作的优先级OPTION(osd_scrub_cost, OPT_U32) // 工作队列中的scrub操作的cost（设置默认cost等于50MB io）（cost翻译为成本，需要看代码到底指的是什么）OPTION(osd_requested_scrub_priority, OPT_U32) // 设置请求的scrub优先级高于scrub优先级，使请求的scrub跳过预定scrub的队列 临时结论： 1、目前无法避免静默错误，Scrub也无法修复严重的静默错误 2、Scrub是Ceph检测静默错误的机制 3、目前发现静默错误也没有好的方法修复（只能通知客户尽可能挽回数据） 想法： 1、借助Scrub作为测试正确性与一致性的工具（需要解决覆盖写问题）]]></content>
      <tags>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信抢票加速互助群]]></title>
    <url>%2F2019%2F05%2F29%2F%E5%BE%AE%E4%BF%A1%E6%8A%A2%E7%A5%A8%E5%8A%A0%E9%80%9F%E4%BA%92%E5%8A%A9%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[加群请加微信：yjtcok（注明：微信抢票） 本群只为抢票加速互助，不提供任何其他服务。不参与任何购物砍价，请大家知晓，谢谢。]]></content>
      <tags>
        <tag>微信</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[prometheus node_exporter 监控项]]></title>
    <url>%2F2019%2F05%2F27%2Fprometheus-node-exporter-%E7%9B%91%E6%8E%A7%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[截止2019-05-24 node_exporter监控项整理。 Collectors每个操作系统对collector的支持各不相同。 下表列出了所有现有collector和支持的系统。通过--collector.&lt;name&gt;标志来启用collector。默认情况下启用的collector可以通过--no-collector.&lt;name&gt;标志来禁用。 默认情况下启用 Name Description OS arp 来自/proc/net/arp的ARP统计信息 Linux bcache 来自/sys/fs/bcache/的bcache统计信息 Linux bonding Linux bonding interfaces已配置active slave的数量 Linux boottime 从kern.boottime sysctl派生的系统启动时间 Darwin, Dragonfly, FreeBSD, NetBSD, OpenBSD, Solaris conntrack 显示conntrack统计信息（如果没有/proc/sys/net/netfilter/，则不执行任何操作） Linux cpu CPU统计信息 Darwin, Dragonfly, FreeBSD, Linux, Solaris cpufreq CPU频率统计信息 Linux, Solaris diskstats 磁盘I/O统计信息 Darwin, Linux, OpenBSD edac 检错和纠错的统计数据 Linux entropy 可用的熵（entropy） Linux exec execution统计信息 Dragonfly, FreeBSD filefd 来自/proc/sys/fs/file-nr的文件描述符统计信息 Linux filesystem 文件系统统计信息，例如使用的磁盘空间 Darwin, Dragonfly, FreeBSD, Linux, OpenBSD hwmon 来自/sys/class/hwmon/的硬件监控和传感器数据 Linux infiniband 特定于InfiniBand和Intel OmniPath配置的网络统计信息 Linux ipvs 来自/proc/net/ip_vs的IPVS状态和来自/proc/net/ip_vs_stats的统计数据 Linux loadavg 负载平均值 Darwin, Dragonfly, FreeBSD, Linux, NetBSD, OpenBSD, Solaris mdadm 有关/proc/mdstat中设备的统计信息（如果没有/proc/mdstat，则不执行任何操作） Linux meminfo 内存统计信息 Darwin, Dragonfly, FreeBSD, Linux, OpenBSD netclass 来自/sys/class/net/的网络接口信息 Linux netdev 网络接口统计信息，如字节传输 Darwin, Dragonfly, FreeBSD, Linux, OpenBSD netstat 来自/proc/net/netstat的网络统计信息。 这与netstat -s的信息相同 Linux nfs 从/proc/net/rpc/nfs公开NFS客户端统计信息。 这与nfsstat -c的信息相同 Linux nfsd 从/proc/net/rpc/nfsd公开NFS内核服务器统计信息。 这与nfsstat -s的信息相同 Linux pressure 来自/proc/pressure/的压力失速统计 Linux (kernel 4.20+ and/or CONFIG_PSI) sockstat 公开来自/proc/net/sockstat的各种统计信息 Linux stat 来自/proc/stat的各种统计信息。这包括启动时间，forks和interrupts。 Linux textfile 从本地磁盘读取的统计信息。必须设置--collector.textfile.directory标志 any time 当前的系统时间 any timex selected adjtimex(2) system call统计信息 Linux uname uname系统调用提供的系统信息 FreeBSD, Linux vmstat 来自/proc/vmstat的统计信息 Linux xfs XFS运行时统计信息 Linux (kernel 4.4+) zfs ZFS性能统计数据 Linux, Solaris 默认情况下禁用由于内核配置和安全设置，默认情况下，所有Linux系统上的perf收集器可能无法正常工作。 要允许访问，请设置以下sysctl参数： 1sysctl -w kernel.perf_event_paranoid=X 2 仅允许用户空间度量（自Linux 4.6起默认）。 1 允许内核和用户度量（在Linux 4.6之前默认）。 0 允许访问特定CPU的数据，但不允许访问raw tracepoint samples。 -1 没有限制。 根据配置的值，将提供不同的度量标准，对于大多数情况，0将提供最完整的设置。 有关更多信息，请参阅man 2 perf_event_open。 Name Description OS buddyinfo /proc/buddyinfo报告的内存碎片统计信息 Linux devstat 设备统计信息 Dragonfly, FreeBSD drbd 分布式副本（Replicated）块设备统计信息（到版本8.4） Linux interrupts 详细的中断统计 Linux, OpenBSD ksmd 来自/sys/kernel/mm/ksm的内核和系统统计信息 Linux logind 会话计数来自logind Linux meminfo_numa 来自/proc/meminfo_numa的内存统计信息 Linux mountstats 来自/proc/self/mountstats的文件系统统计信息。详细的NFS客户端统计信息。 Linux ntp 本地NTP守护程序运行状况检查时间 any processes 来自/proc的聚合进程统计信息 Linux qdisc queuing discipline 统计 Linux runit 来自runit的服务状态统计 any supervisord 来自supervisord的服务状态统计 any systemd 来自systemd的服务和系统状态统计 Linux tcpstat 来自/proc/net/tcp和/proc/net/tcp6的TCP连接状态信息。（警告：当前版本在高负载情况下存在潜在的性能问题。） Linux wifi WiFi设备和station统计 Linux perf 基于perf的指标（警告：指标取决于内核配置和设置） Linux Textfile Collectortextfile collector类似于Pushgateway因为它允许从批处理job导出统计信息。它还可用于导出静态指标，例如计算机具有的role。Pushgateway应该用于服务级别指标。textfile模式用于绑定计算机的度量标准。 要使用它，请在Node exporter上设置--collector.textfile.directory标志。collector将使用文本格式解析该目录中与glob *.prom匹配的所有文件。 注意：不支持时间戳。 以原子方式推送cron job的完成时间： 12echo my_batch_job_completion_time $(date +%s) &gt; /path/to/directory/my_batch_job.prom.$$mv /path/to/directory/my_batch_job.prom.$$ /path/to/directory/my_batch_job.prom 使用标签静态设置计算机的roles： 12echo &apos;role&#123;role=&quot;application_server&quot;&#125; 1&apos; &gt; /path/to/directory/role.prom.$$mv /path/to/directory/role.prom.$$ /path/to/directory/role.prom Filtering enabled collectors默认情况下，node_exporter将从启用的collector中公开所有指标。 这是收集指标以避免在比较不同系列的指标时出错的建议方法。 高级使用node_exporter可以传递一个可选的收集器列表来过滤指标。 collect[]参数可以多次使用。 在Prometheus配置中，您可以在scrape config 1234params: collect[]: - foo - bar 这对于让不同的Prometheus服务器从节点收集特定指标非常有用。 部分监控项见下表 Metrics Chinese explanation English explanation node_arp_entries device的ARP表项 # HELP node_arp_entries ARP entries by device node_boot_time_seconds 节点启动时间，unixtime # HELP node_boot_time_seconds Node boot time, in unixtime. node_context_switches_total context switches（上下文切换）的总数 # HELP node_context_switches_total Total number of context switches. node_cpu_guest_seconds_total 每种模式在guests(VM)上花费CPU的秒数 # HELP node_cpu_guest_seconds_total Seconds the cpus spent in guests (VMs) for each mode. node_cpu_seconds_total 在每种模式下花费CPU的秒数 # HELP node_cpu_seconds_total Seconds the cpus spent in each mode. node_disk_io_now 当前正在进行的I/O数量 # HELP node_disk_io_now The number of I/Os currently in progress. node_disk_io_time_seconds_total 执行I/O所花费的总时间 # HELP node_disk_io_time_seconds_total Total seconds spent doing I/Os. node_disk_io_time_weighted_seconds_total 进行I/O所花费的加权秒数 # HELP node_disk_io_time_weighted_seconds_total The weighted # of seconds spent doing I/Os. node_disk_read_bytes_total 成功读取的总字节数 # HELP node_disk_read_bytes_total The total number of bytes read successfully. node_disk_read_time_seconds_total 所有读取花费的总秒数 # HELP node_disk_read_time_seconds_total The total number of seconds spent by all reads. node_disk_reads_completed_total 成功完成的读取总数 # HELP node_disk_reads_completed_total The total number of reads completed successfully. node_disk_reads_merged_total 读合并的次数 # HELP node_disk_reads_merged_total The total number of reads merged. node_disk_write_time_seconds_total 这是所有写入花费的总秒数 # HELP node_disk_write_time_seconds_total This is the total number of seconds spent by all writes. node_disk_writes_completed_total 成功完成的写入总数 # HELP node_disk_writes_completed_total The total number of writes completed successfully. node_disk_writes_merged_total 写合并的次数 # HELP node_disk_writes_merged_total The number of writes merged. node_disk_written_bytes_total 成功写入的总字节数 # HELP node_disk_written_bytes_total The total number of bytes written successfully. node_entropy_available_bits 可用entropy的Bits（比特） # HELP node_entropy_available_bits Bits of available entropy. node_exporter_build_info 构建node_exporter的版本，修订版，分支和goversion标记，具有常量值“1”的metric # HELP node_exporter_build_info A metric with a constant ‘1’ value labeled by version, revision, branch, and goversion from which node_exporter was built. node_filefd_allocated 文件描述符统计：已分配 # HELP node_filefd_allocated File descriptor statistics: allocated. node_filefd_maximum 文件描述符统计：最大值 # HELP node_filefd_maximum File descriptor statistics: maximum. node_filesystem_avail_bytes 非root用户可用的文件系统空间（以字节为单位） # HELP node_filesystem_avail_bytes Filesystem space available to non-root users in bytes. node_filesystem_device_error 获取指定设备的统计信息时是否发生错误 # HELP node_filesystem_device_error Whether an error occurred while getting statistics for the given device. node_filesystem_files 文件系统总的file nodes数（inode） # HELP node_filesystem_files Filesystem total file nodes. node_filesystem_files_free 文件系统空闲file node数（inode） # HELP node_filesystem_files_free Filesystem total free file nodes. node_filesystem_free_bytes 文件系统可用空间，以字节为单位 # HELP node_filesystem_free_bytes Filesystem free space in bytes. node_filesystem_readonly 文件系统只读状态 # HELP node_filesystem_readonly Filesystem read-only status. node_filesystem_size_bytes 文件系统大小（字节） # HELP node_filesystem_size_bytes Filesystem size in bytes. node_forks_total forks总数 # HELP node_forks_total Total number of forks. node_hwmon_chip_names 人类可读（可读性良好）的芯片名称的注释指标 # HELP node_hwmon_chip_names Annotation metric for human-readable chip names node_hwmon_sensor_label 给定芯片和传感器的标签 # HELP node_hwmon_sensor_label Label for given chip and sensor node_hwmon_temp_celsius 温度硬件监视器（输入） # HELP node_hwmon_temp_celsius Hardware monitor for temperature (input) node_hwmon_temp_crit_alarm_celsius 温度硬件监控器（crit_alarm） # HELP node_hwmon_temp_crit_alarm_celsius Hardware monitor for temperature (crit_alarm) node_hwmon_temp_crit_celsius 温度硬件监控器（暴击） # HELP node_hwmon_temp_crit_celsius Hardware monitor for temperature (crit) node_hwmon_temp_max_celsius 温度硬件监控器（最大） # HELP node_hwmon_temp_max_celsius Hardware monitor for temperature (max) node_intr_total 服务的中断总数 # HELP node_intr_total Total number of interrupts serviced. node_load1 1m负载平均值 # HELP node_load1 1m load average. node_load15 15m负载平均值 # HELP node_load15 15m load average. node_load5 5m负载平均值 # HELP node_load5 5m load average. node_memory_Active_anon_bytes 内存信息字段Active_anon_bytes # HELP node_memory_Active_anon_bytes Memory information field Active_anon_bytes. node_memory_Active_bytes 内存信息字段Active_bytes # HELP node_memory_Active_bytes Memory information field Active_bytes. node_memory_Active_file_bytes 内存信息字段Active_file_bytes # HELP node_memory_Active_file_bytes Memory information field Active_file_bytes. node_memory_AnonHugePages_bytes 内存信息字段AnonHugePages_bytes # HELP node_memory_AnonHugePages_bytes Memory information field AnonHugePages_bytes. node_memory_AnonPages_bytes 内存信息字段AnonPages_bytes # HELP node_memory_AnonPages_bytes Memory information field AnonPages_bytes. node_memory_Bounce_bytes 内存信息字段Bounce_bytes # HELP node_memory_Bounce_bytes Memory information field Bounce_bytes. node_memory_Buffers_bytes 内存信息字段Buffers_bytes # HELP node_memory_Buffers_bytes Memory information field Buffers_bytes. node_memory_Cached_bytes 内存信息字段Cached_bytes # HELP node_memory_Cached_bytes Memory information field Cached_bytes. node_memory_CmaFree_bytes 内存信息字段CmaFree_bytes # HELP node_memory_CmaFree_bytes Memory information field CmaFree_bytes. node_memory_CmaTotal_bytes 内存信息字段CmaTotal_bytes # HELP node_memory_CmaTotal_bytes Memory information field CmaTotal_bytes. node_memory_CommitLimit_bytes 内存信息字段CommitLimit_bytes # HELP node_memory_CommitLimit_bytes Memory information field CommitLimit_bytes. node_memory_Committed_AS_bytes 内存信息字段Committed_AS_bytes # HELP node_memory_Committed_AS_bytes Memory information field Committed_AS_bytes. node_memory_DirectMap1G_bytes 内存信息字段DirectMap1G_bytes # HELP node_memory_DirectMap1G_bytes Memory information field DirectMap1G_bytes. node_memory_DirectMap2M_bytes 内存信息字段DirectMap2M_bytes # HELP node_memory_DirectMap2M_bytes Memory information field DirectMap2M_bytes. node_memory_DirectMap4k_bytes 内存信息字段DirectMap4k_bytes # HELP node_memory_DirectMap4k_bytes Memory information field DirectMap4k_bytes. node_memory_Dirty_bytes 内存信息字段Dirty_bytes # HELP node_memory_Dirty_bytes Memory information field Dirty_bytes. node_memory_HardwareCorrupted_bytes 内存信息字段HardwareCorrupted_bytes # HELP node_memory_HardwareCorrupted_bytes Memory information field HardwareCorrupted_bytes. node_memory_HugePages_Free 内存信息字段HugePages_Free # HELP node_memory_HugePages_Free Memory information field HugePages_Free. node_memory_HugePages_Rsvd 内存信息字段HugePages_Rsvd # HELP node_memory_HugePages_Rsvd Memory information field HugePages_Rsvd. node_memory_HugePages_Surp 内存信息字段HugePages_Surp # HELP node_memory_HugePages_Surp Memory information field HugePages_Surp. node_memory_HugePages_Total 内存信息字段HugePages_Total # HELP node_memory_HugePages_Total Memory information field HugePages_Total. node_memory_Hugepagesize_bytes 内存信息字段Hugepagesize_bytes # HELP node_memory_Hugepagesize_bytes Memory information field Hugepagesize_bytes. node_memory_Inactive_anon_bytes 内存信息字段Inactive_anon_bytes # HELP node_memory_Inactive_anon_bytes Memory information field Inactive_anon_bytes. node_memory_Inactive_bytes 内存信息字段Inactive_bytes # HELP node_memory_Inactive_bytes Memory information field Inactive_bytes. node_memory_Inactive_file_bytes 内存信息字段Inactive_file_bytes # HELP node_memory_Inactive_file_bytes Memory information field Inactive_file_bytes. node_memory_KernelStack_bytes 内存信息字段KernelStack_bytes # HELP node_memory_KernelStack_bytes Memory information field KernelStack_bytes. node_memory_Mapped_bytes 内存信息字段Mapped_bytes # HELP node_memory_Mapped_bytes Memory information field Mapped_bytes. node_memory_MemAvailable_bytes 内存信息字段MemAvailable_bytes # HELP node_memory_MemAvailable_bytes Memory information field MemAvailable_bytes. node_memory_MemFree_bytes 内存信息字段MemFree_bytes # HELP node_memory_MemFree_bytes Memory information field MemFree_bytes. node_memory_MemTotal_bytes 内存信息字段MemTotal_bytes # HELP node_memory_MemTotal_bytes Memory information field MemTotal_bytes. node_memory_Mlocked_bytes 内存信息字段Mlocked_bytes # HELP node_memory_Mlocked_bytes Memory information field Mlocked_bytes. node_memory_NFS_Unstable_bytes 内存信息字段NFS_Unstable_bytes # HELP node_memory_NFS_Unstable_bytes Memory information field NFS_Unstable_bytes. node_memory_PageTables_bytes 内存信息字段PageTables_bytes # HELP node_memory_PageTables_bytes Memory information field PageTables_bytes. node_memory_SReclaimable_bytes 内存信息字段SReclaimable_bytes # HELP node_memory_SReclaimable_bytes Memory information field SReclaimable_bytes. node_memory_SUnreclaim_bytes 内存信息字段SUnreclaim_bytes # HELP node_memory_SUnreclaim_bytes Memory information field SUnreclaim_bytes. node_memory_Shmem_bytes 内存信息字段Shmem_bytes # HELP node_memory_Shmem_bytes Memory information field Shmem_bytes. node_memory_Slab_bytes 内存信息字段Slab_bytes # HELP node_memory_Slab_bytes Memory information field Slab_bytes. node_memory_SwapCached_bytes 内存信息字段SwapCached_bytes # HELP node_memory_SwapCached_bytes Memory information field SwapCached_bytes. node_memory_SwapFree_bytes 内存信息字段SwapFree_bytes # HELP node_memory_SwapFree_bytes Memory information field SwapFree_bytes. node_memory_SwapTotal_bytes 内存信息字段SwapTotal_bytes # HELP node_memory_SwapTotal_bytes Memory information field SwapTotal_bytes. node_memory_Unevictable_bytes 内存信息字段Unevictable_bytes # HELP node_memory_Unevictable_bytes Memory information field Unevictable_bytes. node_memory_VmallocChunk_bytes 内存信息字段VmallocChunk_bytes # HELP node_memory_VmallocChunk_bytes Memory information field VmallocChunk_bytes. node_memory_VmallocTotal_bytes 内存信息字段VmallocTotal_bytes # HELP node_memory_VmallocTotal_bytes Memory information field VmallocTotal_bytes. node_memory_VmallocUsed_bytes 内存信息字段VmallocUsed_bytes # HELP node_memory_VmallocUsed_bytes Memory information field VmallocUsed_bytes. node_memory_WritebackTmp_bytes 内存信息字段WritebackTmp_bytes # HELP node_memory_WritebackTmp_bytes Memory information field WritebackTmp_bytes. node_memory_Writeback_bytes 内存信息字段Writeback_bytes # HELP node_memory_Writeback_bytes Memory information field Writeback_bytes. node_netstat_Icmp6_InErrors 统计Icmp6InErrors # HELP node_netstat_Icmp6_InErrors Statistic Icmp6InErrors. node_netstat_Icmp6_InMsgs 统计Icmp6InMsgs # HELP node_netstat_Icmp6_InMsgs Statistic Icmp6InMsgs. node_netstat_Icmp6_OutMsgs 统计Icmp6OutMsgs # HELP node_netstat_Icmp6_OutMsgs Statistic Icmp6OutMsgs. node_netstat_Icmp_InErrors 统计IcmpInErrors # HELP node_netstat_Icmp_InErrors Statistic IcmpInErrors. node_netstat_Icmp_InMsgs 统计IcmpInMsgs # HELP node_netstat_Icmp_InMsgs Statistic IcmpInMsgs. node_netstat_Icmp_OutMsgs 统计IcmpOutMsgs # HELP node_netstat_Icmp_OutMsgs Statistic IcmpOutMsgs. node_netstat_Ip6_InOctets 统计Ip6InOctets # HELP node_netstat_Ip6_InOctets Statistic Ip6InOctets. node_netstat_Ip6_OutOctets 统计Ip6OutOctets # HELP node_netstat_Ip6_OutOctets Statistic Ip6OutOctets. node_netstat_IpExt_InOctets 统计IpExtInOctets # HELP node_netstat_IpExt_InOctets Statistic IpExtInOctets. node_netstat_IpExt_OutOctets 统计IpExtOutOctets # HELP node_netstat_IpExt_OutOctets Statistic IpExtOutOctets. node_netstat_Ip_Forwarding 统计IpForwarding # HELP node_netstat_Ip_Forwarding Statistic IpForwarding. node_netstat_TcpExt_ListenDrops 统计TcpExtListenDrops # HELP node_netstat_TcpExt_ListenDrops Statistic TcpExtListenDrops. node_netstat_TcpExt_ListenOverflows 统计TcpExtListenOverflows # HELP node_netstat_TcpExt_ListenOverflows Statistic TcpExtListenOverflows. node_netstat_TcpExt_SyncookiesFailed 统计TcpExtSyncookiesFailed # HELP node_netstat_TcpExt_SyncookiesFailed Statistic TcpExtSyncookiesFailed. node_netstat_TcpExt_SyncookiesRecv 统计TcpExtSyncookiesRecv # HELP node_netstat_TcpExt_SyncookiesRecv Statistic TcpExtSyncookiesRecv. node_netstat_TcpExt_SyncookiesSent 统计TcpExtSyncookiesSent # HELP node_netstat_TcpExt_SyncookiesSent Statistic TcpExtSyncookiesSent. node_netstat_TcpExt_TCPSynRetrans 统计TcpExtTCPSynRetrans # HELP node_netstat_TcpExt_TCPSynRetrans Statistic TcpExtTCPSynRetrans. node_netstat_Tcp_ActiveOpens 统计TcpActiveOpens # HELP node_netstat_Tcp_ActiveOpens Statistic TcpActiveOpens. node_netstat_Tcp_CurrEstab 统计TcpCurrEstab # HELP node_netstat_Tcp_CurrEstab Statistic TcpCurrEstab. node_netstat_Tcp_InErrs 统计TcpInErrs # HELP node_netstat_Tcp_InErrs Statistic TcpInErrs. node_netstat_Tcp_InSegs 统计TcpInSegs # HELP node_netstat_Tcp_InSegs Statistic TcpInSegs. node_netstat_Tcp_OutSegs 统计TcpOutSegs # HELP node_netstat_Tcp_OutSegs Statistic TcpOutSegs. node_netstat_Tcp_PassiveOpens 统计TcpPassiveOpens # HELP node_netstat_Tcp_PassiveOpens Statistic TcpPassiveOpens. node_netstat_Tcp_RetransSegs 统计TcpRetransSegs # HELP node_netstat_Tcp_RetransSegs Statistic TcpRetransSegs. node_netstat_Udp6_InDatagrams 统计Udp6InDatagrams # HELP node_netstat_Udp6_InDatagrams Statistic Udp6InDatagrams. node_netstat_Udp6_InErrors 统计Udp6InErrors # HELP node_netstat_Udp6_InErrors Statistic Udp6InErrors. node_netstat_Udp6_NoPorts 统计Udp6NoPorts # HELP node_netstat_Udp6_NoPorts Statistic Udp6NoPorts. node_netstat_Udp6_OutDatagrams 统计Udp6OutDatagrams # HELP node_netstat_Udp6_OutDatagrams Statistic Udp6OutDatagrams. node_netstat_UdpLite6_InErrors 统计UdpLite6InErrors # HELP node_netstat_UdpLite6_InErrors Statistic UdpLite6InErrors. node_netstat_UdpLite_InErrors 统计UdpLiteInErrors # HELP node_netstat_UdpLite_InErrors Statistic UdpLiteInErrors. node_netstat_Udp_InDatagrams 统计UdpInDatagrams # HELP node_netstat_Udp_InDatagrams Statistic UdpInDatagrams. node_netstat_Udp_InErrors 统计UdpInErrors # HELP node_netstat_Udp_InErrors Statistic UdpInErrors. node_netstat_Udp_NoPorts 统计UdpNoPorts # HELP node_netstat_Udp_NoPorts Statistic UdpNoPorts. node_netstat_Udp_OutDatagrams 统计UdpOutDatagrams # HELP node_netstat_Udp_OutDatagrams Statistic UdpOutDatagrams. node_network_address_assign_type /sys/class/net/ address_assign_type值 # HELP node_network_address_assign_type address_assign_type value of /sys/class/net/. node_network_carrier /sys/class/net/ carrier值 # HELP node_network_carrier carrier value of /sys/class/net/. node_network_carrier_changes_total /sys/class/net/ carrier_changes_total值 # HELP node_network_carrier_changes_total carrier_changes_total value of /sys/class/net/. node_network_device_id /sys/class/net/ device_id值 # HELP node_network_device_id device_id value of /sys/class/net/. node_network_dormant /sys/class/net/ dormant（休眠）值 # HELP node_network_dormant dormant value of /sys/class/net/. node_network_flags /sys/class/net/ flags值 # HELP node_network_flags flags value of /sys/class/net/. node_network_iface_id /sys/class/net/ iface_id值 # HELP node_network_iface_id iface_id value of /sys/class/net/. node_network_iface_link /sys/class/net/ iface_link值 # HELP node_network_iface_link iface_link value of /sys/class/net/. node_network_iface_link_mode /sys/class/net/ iface_link_mode值 # HELP node_network_iface_link_mode iface_link_mode value of /sys/class/net/. node_network_info 来自/sys/class/net/的非数字数据，值始终为1 # HELP node_network_info Non-numeric data from /sys/class/net/, value is always 1. node_network_mtu_bytes /sys/class/net/ mtu_bytes值 # HELP node_network_mtu_bytes mtu_bytes value of /sys/class/net/. node_network_net_dev_group /sys/class/net/ net_dev_group值 # HELP node_network_net_dev_group net_dev_group value of /sys/class/net/. node_network_protocol_type /sys/class/net/ protocol_type值 # HELP node_network_protocol_type protocol_type value of /sys/class/net/. node_network_receive_bytes_total 网络设备统计信息receive_bytes # HELP node_network_receive_bytes_total Network device statistic receive_bytes. node_network_receive_compressed_total 网络设备统计信息receive_compressed # HELP node_network_receive_compressed_total Network device statistic receive_compressed. node_network_receive_drop_total 网络设备统计信息receive_drop # HELP node_network_receive_drop_total Network device statistic receive_drop. node_network_receive_errs_total 网络设备统计信息receive_errs # HELP node_network_receive_errs_total Network device statistic receive_errs. node_network_receive_fifo_total 网络设备统计信息receive_fifo # HELP node_network_receive_fifo_total Network device statistic receive_fifo. node_network_receive_frame_total 网络设备统计信息receive_frame # HELP node_network_receive_frame_total Network device statistic receive_frame. node_network_receive_multicast_total 网络设备统计信息receive_multicast # HELP node_network_receive_multicast_total Network device statistic receive_multicast. node_network_receive_packets_total 网络设备统计信息receive_packets # HELP node_network_receive_packets_total Network device statistic receive_packets. node_network_speed_bytes /sys/class/net/ speed_bytes值 # HELP node_network_speed_bytes speed_bytes value of /sys/class/net/. node_network_transmit_bytes_total 网络设备统计信息transmit_bytes # HELP node_network_transmit_bytes_total Network device statistic transmit_bytes. node_network_transmit_carrier_total 网络设备统计信息transmit_carrier # HELP node_network_transmit_carrier_total Network device statistic transmit_carrier. node_network_transmit_colls_total 网络设备统计信息transmit_colls # HELP node_network_transmit_colls_total Network device statistic transmit_colls. node_network_transmit_compressed_total 网络设备统计信息transmit_compressed # HELP node_network_transmit_compressed_total Network device statistic transmit_compressed. node_network_transmit_drop_total 网络设备统计信息transmit_drop # HELP node_network_transmit_drop_total Network device statistic transmit_drop. node_network_transmit_errs_total 网络设备统计信息transmit_errs # HELP node_network_transmit_errs_total Network device statistic transmit_errs. node_network_transmit_fifo_total 网络设备统计信息transmit_fifo # HELP node_network_transmit_fifo_total Network device statistic transmit_fifo. node_network_transmit_packets_total 网络设备统计信息transmit_packets # HELP node_network_transmit_packets_total Network device statistic transmit_packets. node_network_transmit_queue_length /sys/class/net/ transmit_queue_length值 # HELP node_network_transmit_queue_length transmit_queue_length value of /sys/class/net/. node_network_up 如果operstate为’up’，则值为1，否则为0 # HELP node_network_up Value is 1 if operstate is ‘up’, 0 otherwise. node_procs_blocked 阻塞等待I/O完成的进程数 # HELP node_procs_blocked Number of processes blocked waiting for I/O to complete. node_procs_running 处于可运行状态的进程数 # HELP node_procs_running Number of processes in runnable state. node_scrape_collector_duration_seconds node_exporter：collector scrape持续时间 # HELP node_scrape_collector_duration_seconds node_exporter: Duration of a collector scrape. node_scrape_collector_success node_exporter：collector 是否成功 # HELP node_scrape_collector_success node_exporter: Whether a collector succeeded. node_sockstat_FRAG_inuse state inuse中的FRAG sockets数量 # HELP node_sockstat_FRAG_inuse Number of FRAG sockets in state inuse. node_sockstat_FRAG_memory state memory（状态存储器）中的FRAG sockets数量 # HELP node_sockstat_FRAG_memory Number of FRAG sockets in state memory. node_sockstat_RAW_inuse state inuse中的RAW sockets数 # HELP node_sockstat_RAW_inuse Number of RAW sockets in state inuse. node_sockstat_TCP_alloc state alloc中的TCP sockets数 # HELP node_sockstat_TCP_alloc Number of TCP sockets in state alloc. node_sockstat_TCP_inuse state inuse中的TCP sockets数 # HELP node_sockstat_TCP_inuse Number of TCP sockets in state inuse. node_sockstat_TCP_mem state mem中的TCP sockets数 # HELP node_sockstat_TCP_mem Number of TCP sockets in state mem. node_sockstat_TCP_mem_bytes state mem_bytes中的TCP sockets数 # HELP node_sockstat_TCP_mem_bytes Number of TCP sockets in state mem_bytes. node_sockstat_TCP_orphan state orphan中的TCP sockets数 # HELP node_sockstat_TCP_orphan Number of TCP sockets in state orphan. node_sockstat_TCP_tw state tw中的TCP sockets数 # HELP node_sockstat_TCP_tw Number of TCP sockets in state tw. node_sockstat_UDPLITE_inuse state inuse中的UDPLITE UDP sockets数 # HELP node_sockstat_UDPLITE_inuse Number of UDPLITE sockets in state inuse. node_sockstat_UDP_inuse state inuse中的UDP sockets数 # HELP node_sockstat_UDP_inuse Number of UDP sockets in state inuse. node_sockstat_UDP_mem state mem中的UDP sockets数 # HELP node_sockstat_UDP_mem Number of UDP sockets in state mem. node_sockstat_UDP_mem_bytes state mem_bytes中的UDP sockets数 # HELP node_sockstat_UDP_mem_bytes Number of UDP sockets in state mem_bytes. node_sockstat_sockets_used state used中的sockets sockets数 # HELP node_sockstat_sockets_used Number of sockets sockets in state used. node_textfile_scrape_error 如果打开或读取文件时出错时为1，否则为0 # HELP node_textfile_scrape_error 1 if there was an error opening or reading a file, 0 otherwise node_time_seconds 以纪元（1970年）开始的系统时间 # HELP node_time_seconds System time in seconds since epoch (1970). node_timex_estimated_error_seconds 估计误差（秒） # HELP node_timex_estimated_error_seconds Estimated error in seconds. node_timex_frequency_adjustment_ratio 本地时钟频率调整 # HELP node_timex_frequency_adjustment_ratio Local clock frequency adjustment. node_timex_loop_time_constant 锁相回路时间常数 # HELP node_timex_loop_time_constant Phase-locked loop time constant. node_timex_maxerror_seconds 最大误差（秒） # HELP node_timex_maxerror_seconds Maximum error in seconds. node_timex_offset_seconds 本地系统和参考时钟之间的时间偏移 # HELP node_timex_offset_seconds Time offset in between local system and reference clock. node_timex_pps_calibration_total 每秒脉冲校准间隔计数 # HELP node_timex_pps_calibration_total Pulse per second count of calibration intervals. node_timex_pps_error_total 每秒脉冲校准错误计数 # HELP node_timex_pps_error_total Pulse per second count of calibration errors. node_timex_pps_frequency_hertz 脉冲每秒频率 # HELP node_timex_pps_frequency_hertz Pulse per second frequency. node_timex_pps_jitter_seconds 脉冲每秒抖动 # HELP node_timex_pps_jitter_seconds Pulse per second jitter. node_timex_pps_jitter_total 每秒脉冲数超出抖动限制的事件数 # HELP node_timex_pps_jitter_total Pulse per second count of jitter limit exceeded events. node_timex_pps_shift_seconds 每秒脉冲数超出稳定极限的事件数 # HELP node_timex_pps_shift_seconds Pulse per second interval duration. node_timex_pps_stability_exceeded_total 每秒脉冲数超出稳定极限事件 # HELP node_timex_pps_stability_exceeded_total Pulse per second count of stability limit exceeded events. node_timex_pps_stability_hertz 脉冲每秒稳定性，最近频率变化的平均值 # HELP node_timex_pps_stability_hertz Pulse per second stability, average of recent frequency changes. node_timex_status status array bits的值 # HELP node_timex_status Value of the status array bits. node_timex_sync_status 时钟与可靠服务器同步(1 = yes, 0 = no) # HELP node_timex_sync_status Is clock synchronized to a reliable server (1 = yes, 0 = no). node_timex_tai_offset_seconds 国际原子时间（TAI）偏移量 # HELP node_timex_tai_offset_seconds International Atomic Time (TAI) offset. node_timex_tick_seconds 时钟周期之间的秒数 # HELP node_timex_tick_seconds Seconds between clock ticks. node_uname_info 由uname系统调用提供的标记系统信息 # HELP node_uname_info Labeled system information as provided by the uname system call. node_vmstat_pgfault /proc/vmstat信息字段pgfault # HELP node_vmstat_pgfault /proc/vmstat information field pgfault. node_vmstat_pgmajfault /proc/vmstat信息字段pgmajfault # HELP node_vmstat_pgmajfault /proc/vmstat information field pgmajfault. node_vmstat_pgpgin /proc/vmstat信息字段pgpgin # HELP node_vmstat_pgpgin /proc/vmstat information field pgpgin. node_vmstat_pgpgout /proc/vmstat信息字段pgpgout # HELP node_vmstat_pgpgout /proc/vmstat information field pgpgout. node_vmstat_pswpin /proc/vmstat信息字段pswpin # HELP node_vmstat_pswpin /proc/vmstat information field pswpin. node_vmstat_pswpout /proc/vmstat信息字段pswpout # HELP node_vmstat_pswpout /proc/vmstat information field pswpout. node_xfs_allocation_btree_compares_total 文件系统的分配B-tree比较数 # HELP node_xfs_allocation_btree_compares_total Number of allocation B-tree compares for a filesystem. node_xfs_allocation_btree_lookups_total 文件系统的分配B-tree查找数 # HELP node_xfs_allocation_btree_lookups_total Number of allocation B-tree lookups for a filesystem. node_xfs_allocation_btree_records_deleted_total 为文件系统删除的分配B-tree记录数 # HELP node_xfs_allocation_btree_records_deleted_total Number of allocation B-tree records deleted for a filesystem. node_xfs_allocation_btree_records_inserted_total 为文件系统插入的分配B-tree记录数 # HELP node_xfs_allocation_btree_records_inserted_total Number of allocation B-tree records inserted for a filesystem. node_xfs_block_map_btree_compares_total 对于文件系统block映射B-tree比较的数量 # HELP node_xfs_block_map_btree_compares_total Number of block map B-tree compares for a filesystem. node_xfs_block_map_btree_lookups_total 对于文件系统block映射B-tree查找的数量 # HELP node_xfs_block_map_btree_lookups_total Number of block map B-tree lookups for a filesystem. node_xfs_block_map_btree_records_deleted_total 文件系统删除block映射B-tree记录的数量 # HELP node_xfs_block_map_btree_records_deleted_total Number of block map B-tree records deleted for a filesystem. node_xfs_block_map_btree_records_inserted_total 文件系统插入block映射B-tree记录的数量 # HELP node_xfs_block_map_btree_records_inserted_total Number of block map B-tree records inserted for a filesystem. node_xfs_block_mapping_extent_list_compares_total 文件系统的extent列表比较数 # HELP node_xfs_block_mapping_extent_list_compares_total Number of extent list compares for a filesystem. node_xfs_block_mapping_extent_list_deletions_total 文件系统的extent列表删除数 # HELP node_xfs_block_mapping_extent_list_deletions_total Number of extent list deletions for a filesystem. node_xfs_block_mapping_extent_list_insertions_total 文件系统的extent列表插入数 # HELP node_xfs_block_mapping_extent_list_insertions_total Number of extent list insertions for a filesystem. node_xfs_block_mapping_extent_list_lookups_total 文件系统的extent列表查找次数 # HELP node_xfs_block_mapping_extent_list_lookups_total Number of extent list lookups for a filesystem. node_xfs_block_mapping_reads_total 文件系统读取操作的block映射数 # HELP node_xfs_block_mapping_reads_total Number of block map for read operations for a filesystem. node_xfs_block_mapping_unmaps_total 文件系统的block取消映射（删除）的数量 # HELP node_xfs_block_mapping_unmaps_total Number of block unmaps (deletes) for a filesystem. node_xfs_block_mapping_writes_total 文件系统写操作的block映射数 # HELP node_xfs_block_mapping_writes_total Number of block map for write operations for a filesystem. node_xfs_extent_allocation_blocks_allocated_total 文件系统分配的block数 # HELP node_xfs_extent_allocation_blocks_allocated_total Number of blocks allocated for a filesystem. node_xfs_extent_allocation_blocks_freed_total 文件系统释放的block数 # HELP node_xfs_extent_allocation_blocks_freed_total Number of blocks freed for a filesystem. node_xfs_extent_allocation_extents_allocated_total 为文件系统分配的extent数 在电脑文件系统中，一个 Extent (在中国大陆某些文献中翻译为“区块”[1])，是指一段连续的存储空间。一般来说，一个文件的物理大小一定是一个 extent 容量的整数倍。当一个进程创建一个文件的时候，文件系统管理程序会将整个 extent 分配给这个文件。当再次向该文件写入数据时 (有可能是在其他写入操作之后)，数据会从上次写入的数据末尾处追加数据。这样可以减少甚至消除文件碎片。 参考：http://www.wikiwand.com/zh-sg/Extent_(%E6%AA%94%E6%A1%88%E7%B3%BB%E7%B5%B1) # HELP node_xfs_extent_allocation_extents_allocated_total Number of extents allocated for a filesystem. node_xfs_extent_allocation_extents_freed_total 文件系统释放的extent数 # HELP node_xfs_extent_allocation_extents_freed_total Number of extents freed for a filesystem.]]></content>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[prometheus ceph_exporter 监控项]]></title>
    <url>%2F2019%2F05%2F24%2Fprometheus-ceph-exporter-%E7%9B%91%E6%8E%A7%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[截止2019-05-24 ceph_exporter监控项整理 Metrics Chinese explanation English explanation ceph_active_pgs 处于active状态的PG数 # HELP ceph_active_pgs No. of active PGs in the cluster ceph_backfill_wait_pgs 处于backfill_wait状态的PG数 # HELP ceph_backfill_wait_pgs No. of PGs in the cluster with backfill_wait state ceph_backfilling_pgs 处于backfilling状态的PG数 # HELP ceph_backfilling_pgs No. of backfilling PGs in the cluster ceph_cache_evict_io_bytes 每秒从cache pool（缓存池）中evicted（逐出）的字节数 # HELP ceph_cache_evict_io_bytes Rate of bytes being evicted from the cache pool per second ceph_cache_flush_io_bytes 每秒从cache pool（缓存池）中flushed（刷新）的字节数 # HELP ceph_cache_flush_io_bytes Rate of bytes being flushed from the cache pool per second ceph_cache_promote_io_ops 每秒cache promote操作的总数 # HELP ceph_cache_promote_io_ops Total cache promote operations measured per second ceph_client_io_ops 客户端每秒操作总数 # HELP ceph_client_io_ops Total client ops on the cluster measured per second ceph_client_io_read_bytes 客户端每秒read字节数 # HELP ceph_client_io_read_bytes Rate of bytes being read by all clients per second ceph_client_io_read_ops 客户端每秒read总I/O操作数 # HELP ceph_client_io_read_ops Total client read I/O ops on the cluster measured per second ceph_client_io_write_bytes 客户端每秒write字节数 # HELP ceph_client_io_write_bytes Rate of bytes being written by all clients per second ceph_client_io_write_ops 客户端每秒write总I/O操作数 # HELP ceph_client_io_write_ops Total client write I/O ops on the cluster measured per second ceph_cluster_available_bytes 群集中的可用空间 # HELP ceph_cluster_available_bytes Available space within the cluster ceph_cluster_capacity_bytes 群集的总容量 # HELP ceph_cluster_capacity_bytes Total capacity of the cluster ceph_cluster_objects 集群中的rados object数 # HELP ceph_cluster_objects No. of rados objects within the cluster ceph_cluster_used_bytes 集群已使用的容量 # HELP ceph_cluster_used_bytes Capacity of the cluster currently in use ceph_deep_scrubbing_pgs deep scrubbing状态的PG数量 # HELP ceph_deep_scrubbing_pgs No. of deep scrubbing PGs in the cluster ceph_degraded_objects 所有PG中degraded objects的数量，包括副本 # HELP ceph_degraded_objects No. of degraded objects across all PGs, includes replicas ceph_degraded_pgs 处于degraded状态的PG数 # HELP ceph_degraded_pgs No. of PGs in a degraded state ceph_down_pgs 处于down状态的PG数 # HELP ceph_down_pgs No. of PGs in the cluster in down state ceph_forced_backfill_pgs 处于forced_backfill状态的PG数 # HELP ceph_forced_backfill_pgs No. of PGs in the cluster with forced_backfill state ceph_forced_recovery_pgs 处于forced_recovery状态的PG数 # HELP ceph_forced_recovery_pgs No. of PGs in the cluster with forced_recovery state ceph_health_status 群集的health状态，只能在3种状态之间变化(err:2, warn:1, ok:0) # HELP ceph_health_status Health status of Cluster, can vary only between 3 states (err:2, warn:1, ok:0) ceph_misplaced_objects 所有PG中misplaced object的数量，包括副本 # HELP ceph_misplaced_objects No. of misplaced objects across all PGs, includes replicas ceph_monitor_quorum_count monitor quorum的总数 # HELP ceph_monitor_quorum_count The total size of the monitor quorum ceph_osd_avail_bytes OSD可用存储字节数 # HELP ceph_osd_avail_bytes OSD Available Storage in Bytes ceph_osd_average_utilization OSD平均利用率 # HELP ceph_osd_average_utilization OSD Average Utilization ceph_osd_bytes OSD总字节数 # HELP ceph_osd_bytes OSD Total Bytes ceph_osd_crush_weight OSD Crush Weight # HELP ceph_osd_crush_weight OSD Crush Weight ceph_osd_depth OSD 深度 # HELP ceph_osd_depth OSD Depth ceph_osd_in OSD In状态 # HELP ceph_osd_in OSD In Status ceph_osd_perf_apply_latency_seconds OSD Perf Apply延迟 # HELP ceph_osd_perf_apply_latency_seconds OSD Perf Apply Latency ceph_osd_perf_commit_latency_seconds OSD Perf Commit延迟 # HELP ceph_osd_perf_commit_latency_seconds OSD Perf Commit Latency ceph_osd_pgs OSD Placement Group计数 # HELP ceph_osd_pgs OSD Placement Group Count ceph_osd_reweight OSD Reweight # HELP ceph_osd_reweight OSD Reweight ceph_osd_total_avail_bytes OSD可用存储总字节数 # HELP ceph_osd_total_avail_bytes OSD Total Available Storage Bytes ceph_osd_total_bytes OSD总存储字节数 # HELP ceph_osd_total_bytes OSD Total Storage Bytes ceph_osd_total_used_bytes OSD已使用总的存储字节数 # HELP ceph_osd_total_used_bytes OSD Total Used Storage Bytes ceph_osd_up OSD Up状态 # HELP ceph_osd_up OSD Up Status ceph_osd_used_bytes OSD已使用的存储空间字节 # HELP ceph_osd_used_bytes OSD Used Storage in Bytes ceph_osd_utilization OSD利用率 # HELP ceph_osd_utilization OSD Utilization ceph_osd_variance OSD方差 # HELP ceph_osd_variance OSD Variance ceph_osdmap_flag_full 群集标记为已满，无法提供写入服务 # HELP ceph_osdmap_flag_full The cluster is flagged as full and cannot service writes ceph_osdmap_flag_nobackfill OSD不会被backfill # HELP ceph_osdmap_flag_nobackfill OSDs will not be backfilled ceph_osdmap_flag_nodeep_scrub 禁用Deep scrubbing # HELP ceph_osdmap_flag_nodeep_scrub Deep scrubbing is disabled ceph_osdmap_flag_nodown 忽略OSD失败报告，OSD不会被标记为down # HELP ceph_osdmap_flag_nodown OSD failure reports are ignored, OSDs will not be marked as down ceph_osdmap_flag_noin 不会自动标记out的OSD # HELP ceph_osdmap_flag_noin OSDs that are out will not be automatically marked in ceph_osdmap_flag_noout 在配置的间隔后，OSD不会自动标记out # HELP ceph_osdmap_flag_noout OSDs will not be automatically marked out after the configured interval ceph_osdmap_flag_norebalance 数据rebalancing暂停 # HELP ceph_osdmap_flag_norebalance Data rebalancing is suspended ceph_osdmap_flag_norecover Recovery暂停 # HELP ceph_osdmap_flag_norecover Recovery is suspended ceph_osdmap_flag_noscrub 禁用Scrubbing # HELP ceph_osdmap_flag_noscrub Scrubbing is disabled ceph_osdmap_flag_notieragent Cache tiering activity已暂停 # HELP ceph_osdmap_flag_notieragent Cache tiering activity is suspended ceph_osdmap_flag_noup 不允许OSD start # HELP ceph_osdmap_flag_noup OSDs are not allowed to start ceph_osdmap_flag_pauserd Reads暂停 # HELP ceph_osdmap_flag_pauserd Reads are paused ceph_osdmap_flag_pausewr Writes暂停 # HELP ceph_osdmap_flag_pausewr Writes are paused ceph_osds 群集中总OSD数量 # HELP ceph_osds Count of total OSDs in the cluster ceph_osds_down 处于DOWN状态的OSD数量 # HELP ceph_osds_down Count of OSDs that are in DOWN state ceph_osds_in 处于IN状态并可用于处理请求的OSD数量 # HELP ceph_osds_in Count of OSDs that are in IN state and available to serve requests ceph_osds_up 处于UP状态的OSD数量 # HELP ceph_osds_up Count of OSDs that are in UP state ceph_peering_pgs 群集中peering状态的PG数量 # HELP ceph_peering_pgs No. of peering PGs in the cluster ceph_pgs_remapped remapped并引起cluster-wide（群集范围）移动的PG数量 # HELP ceph_pgs_remapped No. of PGs that are remapped and incurring cluster-wide movement ceph_recovering_pgs 群集中recovering状态的PG数量 # HELP ceph_recovering_pgs No. of recovering PGs in the cluster ceph_recovery_io_bytes 每秒recovery的字节数 # HELP ceph_recovery_io_bytes Rate of bytes being recovered in cluster per second ceph_recovery_io_keys 每秒恢复的keys数率 # HELP ceph_recovery_io_keys Rate of keys being recovered in cluster per second ceph_recovery_io_objects 每秒恢复的object的速率 # HELP ceph_recovery_io_objects Rate of objects being recovered in cluster per second ceph_recovery_wait_pgs 处于recovery_wait状态的PG数 # HELP ceph_recovery_wait_pgs No. of PGs in the cluster with recovery_wait state ceph_scrubbing_pgs 处于scrubbing状态的PG数 # HELP ceph_scrubbing_pgs No. of scrubbing PGs in the cluster ceph_slow_requests 慢速请求数 # HELP ceph_slow_requests No. of slow requests ceph_stale_pgs 处于stale状态的PG数 # HELP ceph_stale_pgs No. of stale PGs in the cluster ceph_stuck_degraded_pgs 处于degraded状态的PG数 # HELP ceph_stuck_degraded_pgs No. of PGs stuck in a degraded state ceph_stuck_requests stuck（卡住）的请求数 # HELP ceph_stuck_requests No. of stuck requests ceph_stuck_stale_pgs 处于stale状态的PG数 # HELP ceph_stuck_stale_pgs No. of stuck stale PGs in the cluster ceph_stuck_unclean_pgs 处于unclean状态的PG数 # HELP ceph_stuck_unclean_pgs No. of PGs stuck in an unclean state ceph_stuck_undersized_pgs 处于undersized状态的PG数 # HELP ceph_stuck_undersized_pgs No. of stuck undersized PGs in the cluster ceph_total_pgs 集群中总的PG数 # HELP ceph_total_pgs Total no. of PGs in the cluster ceph_unclean_pgs 处于unclean状态的PG数 # HELP ceph_unclean_pgs No. of PGs in an unclean state ceph_undersized_pgs 处于undersized状态的PG数 # HELP ceph_undersized_pgs No. of undersized PGs in the cluster]]></content>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVM逻辑卷基本概念]]></title>
    <url>%2F2019%2F04%2F08%2FLVM%E9%80%BB%E8%BE%91%E5%8D%B7%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[LinuxCast视频教程笔记 传统磁盘管理的问题当分区大小不够用时无法扩展其大小，只能通过添加硬盘、创建新的分区来扩展空间，但是新添加进来的硬盘是作为独立文件系统存在的，原有的文件系统并未得到扩充，上层应用很多时候只能访问一个文件系统。只能让现有磁盘下线，换上新的磁盘之后，再将原始数据导入。 LVMLVM（Logical volume Manager）逻辑卷管理通过将底层物理硬盘抽象封装起来，以逻辑卷的形式表现给上层系统，逻辑卷的大小可以动态调整，而且不会丢失现有数据。新加入的硬盘也不会改变现有上层的逻辑卷。 作为一种动态磁盘管理机制，逻辑卷技术大大提高了磁盘管理的灵活性。 上图黄色为：VG 橙色为：LV （1）首先把物理硬盘格式化（存储里面叫条带化）为物理卷（PV），格式化为物理卷的过程实际上是把硬盘空间化成一个一个的PE（PE是逻辑卷空间管理的最基本单位，默认4M） （2）第二步我们要创建一个VG，VG的作用是用来装PE的，就像一个空间池。我们可以把一个或者多个PV加到VG当中。加入多少个PV，我们的VG容量就是这些PV大小之和。（当创建VG以后，在/dev目录下会多出一个目录） （3）最后创建LV（每个LV的空间可能来自不同的物理硬盘） 创建LVM 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465[root@teuthology ~]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTvda 253:0 0 200G 0 disk ├─vda1 253:1 0 1G 0 part /boot├─vda2 253:2 0 4G 0 part [SWAP]└─vda3 253:3 0 195G 0 part /vdb 253:16 0 100G 0 disk vdc 253:32 0 100G 0 disk[root@teuthology ~]# pvcreate /dev/vdb /dev/vdc Physical volume &quot;/dev/vdb&quot; successfully created. Physical volume &quot;/dev/vdc&quot; successfully created.[root@teuthology ~]# pvs PV VG Fmt Attr PSize PFree /dev/vdb lvm2 --- 100.00g 100.00g /dev/vdc lvm2 --- 100.00g 100.00g[root@teuthology ~]# vgcreate linuxcast /dev/vdb /dev/vdc Volume group &quot;linuxcast&quot; successfully created[root@teuthology ~]# vgs VG #PV #LV #SN Attr VSize VFree linuxcast 2 0 0 wz--n- 199.99g 199.99g[root@teuthology ~]# lvcreate -n mylv -L 2G linuxcast Logical volume &quot;mylv&quot; created.[root@teuthology ~]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert mylv linuxcast -wi-a----- 2.00g[root@teuthology ~]# lvcreate -n mynewlv -L 2G linuxcast Logical volume &quot;mynewlv&quot; created.[root@teuthology ~]# ll /dev/linuxcast/total 0lrwxrwxrwx 1 root root 7 Apr 8 14:59 mylv -&gt; ../dm-0lrwxrwxrwx 1 root root 7 Apr 8 15:03 mynewlv -&gt; ../dm-1[root@teuthology ~]# mkfs.ext4 /dev/linuxcast/mylv[root@teuthology ~]# mount /dev/linuxcast/mylv /mnt/[root@teuthology ~]# df -ThFilesystem Type Size Used Avail Use% Mounted on/dev/vda3 xfs 195G 2.6G 193G 2% /devtmpfs devtmpfs 3.9G 0 3.9G 0% /devtmpfs tmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs tmpfs 3.9G 8.6M 3.9G 1% /runtmpfs tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/vda1 xfs 1014M 172M 843M 17% /boottmpfs tmpfs 783M 0 783M 0% /run/user/0/dev/mapper/linuxcast-mylv ext4 2.0G 6.0M 1.8G 1% /mnt#删除LVM[root@teuthology ~]# umount /mnt/[root@teuthology ~]# lvremove /dev/linuxcast/mylv Do you really want to remove active logical volume linuxcast/mylv? [y/n]: y Logical volume &quot;mylv&quot; successfully removed[root@teuthology ~]# lvremove /dev/linuxcast/mynewlv Do you really want to remove active logical volume linuxcast/mynewlv? [y/n]: y Logical volume &quot;mynewlv&quot; successfully removed[root@teuthology ~]# lvs[root@teuthology ~]# vgremove linuxcast Volume group &quot;linuxcast&quot; successfully removed[root@teuthology ~]# vgs[root@teuthology ~]# pvremove /dev/vdb Labels on physical volume &quot;/dev/vdb&quot; successfully wiped.[root@teuthology ~]# pvremove /dev/vdc Labels on physical volume &quot;/dev/vdc&quot; successfully wiped. LVM逻辑卷的拉伸与缩小 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798[root@teuthology ~]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTvda 253:0 0 200G 0 disk ├─vda1 253:1 0 1G 0 part /boot├─vda2 253:2 0 4G 0 part [SWAP]└─vda3 253:3 0 195G 0 part /vdb 253:16 0 100G 0 disk vdc 253:32 0 100G 0 disk [root@teuthology ~]# pvcreate /dev/vdb /dev/vdc Physical volume &quot;/dev/vdb&quot; successfully created. Physical volume &quot;/dev/vdc&quot; successfully created.[root@teuthology ~]# pvs PV VG Fmt Attr PSize PFree /dev/vdb lvm2 --- 100.00g 100.00g /dev/vdc lvm2 --- 100.00g 100.00g[root@teuthology ~]# vgcreate linuxcast /dev/vdb /dev/vdc Volume group &quot;linuxcast&quot; successfully created[root@teuthology ~]# lvcreate -n mylv -L 2G linuxcastWARNING: ext4 signature detected on /dev/linuxcast/mylv at offset 1080. Wipe it? [y/n]: y Wiping ext4 signature on /dev/linuxcast/mylv. Logical volume &quot;mylv&quot; created.[root@teuthology ~]# mkfs.ext4 /dev/linuxcast/mylvmke2fs 1.42.9 (28-Dec-2013)文件系统标签=OS type: Linux块大小=4096 (log=2)分块大小=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks131072 inodes, 524288 blocks26214 blocks (5.00%) reserved for the super user第一个数据块=0Maximum filesystem blocks=53687091216 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912Allocating group tables: 完成 正在写入inode表: 完成 Creating journal (16384 blocks): 完成Writing superblocks and filesystem accounting information: 完成 [root@teuthology ~]# mount /dev/linuxcast/mylv /mnt/[root@teuthology ~]# df -TH文件系统 类型 容量 已用 可用 已用% 挂载点/dev/vda3 xfs 210G 2.8G 207G 2% /devtmpfs devtmpfs 4.1G 0 4.1G 0% /devtmpfs tmpfs 4.2G 0 4.2G 0% /dev/shmtmpfs tmpfs 4.2G 9.0M 4.1G 1% /runtmpfs tmpfs 4.2G 0 4.2G 0% /sys/fs/cgroup/dev/vda1 xfs 1.1G 180M 884M 17% /boottmpfs tmpfs 821M 0 821M 0% /run/user/0/dev/mapper/linuxcast-mylv ext4 2.1G 6.3M 2.0G 1% /mnt[root@teuthology ~]# vgs VG #PV #LV #SN Attr VSize VFree linuxcast 2 1 0 wz--n- 199.99g 197.99g[root@teuthology ~]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert mylv linuxcast -wi-ao---- 2.00g[root@teuthology ~]# lvextend -L +1G /dev/linuxcast/mylv Size of logical volume linuxcast/mylv changed from 2.00 GiB (512 extents) to 3.00 GiB (768 extents). Logical volume linuxcast/mylv successfully resized.[root@teuthology ~]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert mylv linuxcast -wi-ao---- 3.00g[root@teuthology ~]# df -Th文件系统 类型 容量 已用 可用 已用% 挂载点/dev/vda3 xfs 195G 2.6G 193G 2% /devtmpfs devtmpfs 3.9G 0 3.9G 0% /devtmpfs tmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs tmpfs 3.9G 8.6M 3.9G 1% /runtmpfs tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/vda1 xfs 1014M 172M 843M 17% /boottmpfs tmpfs 783M 0 783M 0% /run/user/0/dev/mapper/linuxcast-mylv ext4 2.0G 6.0M 1.8G 1% /mnt[root@teuthology ~]# resize2fs /dev/linuxcast/mylv resize2fs 1.42.9 (28-Dec-2013)Filesystem at /dev/linuxcast/mylv is mounted on /mnt; on-line resizing requiredold_desc_blocks = 1, new_desc_blocks = 1The filesystem on /dev/linuxcast/mylv is now 786432 blocks long.[root@teuthology ~]# df -Th文件系统 类型 容量 已用 可用 已用% 挂载点/dev/vda3 xfs 195G 2.6G 193G 2% /devtmpfs devtmpfs 3.9G 0 3.9G 0% /devtmpfs tmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs tmpfs 3.9G 8.6M 3.9G 1% /runtmpfs tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/vda1 xfs 1014M 172M 843M 17% /boottmpfs tmpfs 783M 0 783M 0% /run/user/0/dev/mapper/linuxcast-mylv ext4 2.9G 6.0M 2.8G 1% /mnt 12345678910111213141516171819202122[root@teuthology ~]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTvda 253:0 0 200G 0 disk ├─vda1 253:1 0 1G 0 part /boot├─vda2 253:2 0 4G 0 part [SWAP]└─vda3 253:3 0 195G 0 part /vdb 253:16 0 100G 0 disk └─linuxcast-mylv 252:0 0 3G 0 lvm /mntvdc 253:32 0 100G 0 disk vdd 253:48 0 100G 0 disk [root@teuthology ~]# pvcreate /dev/vdvda vda1 vda2 vda3 vdb vdc vdd [root@teuthology ~]# pvcreate /dev/vdd Physical volume &quot;/dev/vdd&quot; successfully created.[root@teuthology ~]# vgs VG #PV #LV #SN Attr VSize VFree linuxcast 2 1 0 wz--n- 199.99g 196.99g[root@teuthology ~]# vgextend linuxcast /dev/vdd Volume group &quot;linuxcast&quot; successfully extended[root@teuthology ~]# vgs VG #PV #LV #SN Attr VSize VFree linuxcast 3 1 0 wz--n- &lt;299.99g &lt;296.99g 123456789101112131415161718192021222324252627282930313233343536373839[root@teuthology ~]# umount /mnt/[root@teuthology ~]# df -Th文件系统 类型 容量 已用 可用 已用% 挂载点/dev/vda3 xfs 195G 2.6G 193G 2% /devtmpfs devtmpfs 3.9G 0 3.9G 0% /devtmpfs tmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs tmpfs 3.9G 8.6M 3.9G 1% /runtmpfs tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/vda1 xfs 1014M 172M 843M 17% /boottmpfs tmpfs 783M 0 783M 0% /run/user/0[root@teuthology ~]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert mylv linuxcast -wi-a----- 3.00g [root@teuthology ~]# resize2fs /dev/linuxcast/mylv 2Gresize2fs 1.42.9 (28-Dec-2013)请先运行 &apos;e2fsck -f /dev/linuxcast/mylv&apos;.[root@teuthology ~]# e2fsck -f /dev/linuxcast/mylve2fsck 1.42.9 (28-Dec-2013)第一步: 检查inode,块,和大小第二步: 检查目录结构第3步: 检查目录连接性Pass 4: Checking reference counts第5步: 检查簇概要信息/dev/linuxcast/mylv: 11/196608 files (0.0% non-contiguous), 30268/786432 blocks[root@teuthology ~]# resize2fs /dev/linuxcast/mylv 2Gresize2fs 1.42.9 (28-Dec-2013)Resizing the filesystem on /dev/linuxcast/mylv to 524288 (4k) blocks.The filesystem on /dev/linuxcast/mylv is now 524288 blocks long.[root@teuthology ~]# lvreduce -L -1G /dev/linuxcast/mylv WARNING: Reducing active logical volume to 2.00 GiB. THIS MAY DESTROY YOUR DATA (filesystem etc.)Do you really want to reduce linuxcast/mylv? [y/n]: y Size of logical volume linuxcast/mylv changed from 3.00 GiB (768 extents) to 2.00 GiB (512 extents). Logical volume linuxcast/mylv successfully resized.[root@teuthology ~]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert mylv linuxcast -wi-a----- 2.00g 12345678910111213141516171819[root@teuthology ~]# vgs VG #PV #LV #SN Attr VSize VFree linuxcast 3 1 0 wz--n- &lt;299.99g &lt;296.99g[root@teuthology ~]# vgreduce linuxcast /dev/vdd Removed &quot;/dev/vdd&quot; from volume group &quot;linuxcast&quot;[root@teuthology ~]# vgs VG #PV #LV #SN Attr VSize VFree linuxcast 2 1 0 wz--n- 199.99g 197.99g[root@teuthology ~]# pvs PV VG Fmt Attr PSize PFree /dev/vdb linuxcast lvm2 a-- &lt;100.00g &lt;98.00g /dev/vdc linuxcast lvm2 a-- &lt;100.00g &lt;100.00g /dev/vdd lvm2 --- 100.00g 100.00g[root@teuthology ~]# pvremove /dev/vdd Labels on physical volume &quot;/dev/vdd&quot; successfully wiped.[root@teuthology ~]# pvs PV VG Fmt Attr PSize PFree /dev/vdb linuxcast lvm2 a-- &lt;100.00g &lt;98.00g /dev/vdc linuxcast lvm2 a-- &lt;100.00g &lt;100.00g]]></content>
      <tags>
        <tag>LVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LTP使用]]></title>
    <url>%2F2019%2F04%2F01%2FLTP%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788[root@centos7 ltp-install]# ./runltp -h usage: runltp [ -a EMAIL_TO ] [ -c NUM_PROCS ] [ -C FAILCMDFILE ] [ -T TCONFCMDFILE ] [ -d TMPDIR ] [ -D NUM_PROCS,NUM_FILES,NUM_BYTES,CLEAN_FLAG ] -e [ -f CMDFILES(,...) ] [ -g HTMLFILE] [ -i NUM_PROCS ] [ -l LOGFILE ] [ -m NUM_PROCS,CHUNKS,BYTES,HANGUP_FLAG ] -N -n [ -o OUTPUTFILE ] -p -q -Q [ -r LTPROOT ] [ -s PATTERN ] [ -t DURATION ] -v [ -w CMDFILEADDR ] [ -x INSTANCES ] [ -b DEVICE ] [-B LTP_DEV_FS_TYPE][ -F LOOPS,PERCENTAGE ] [ -z BIG_DEVICE ] [-Z LTP_BIG_DEV_FS_TYPE] # 将所有报告通过EMAIL方式发送到指定E-mail Address -a EMAIL_TO EMAIL all your Reports to this E-mail Address # 在后台额外增加CPU负载的情况下运行LTP -c NUM_PROCS Run LTP under additional background CPU load [NUM_PROCS = no. of processes creating the CPU Load by spinning over sqrt() (Defaults to 1 when value)] -C FAILCMDFILE Command file with all failed test cases. -T TCONFCMDFILE Command file with all test cases that are not fully tested. -d TMPDIR Directory where temporary files will be created. -D NUM_PROCS,NUM_FILES,NUM_BYTES,CLEAN_FLAG Run LTP under additional background Load on Secondary Storage (Seperate by comma) [NUM_PROCS = no. of processes creating Storage Load by spinning over write()] [NUM_FILES = Write() to these many files (Defaults to 1 when value 0 or undefined)] [NUM_BYTES = write these many bytes (defaults to 1GB, when value 0 or undefined)] [CLEAN_FLAG = unlink file to which random data written, when value 1] -e Prints the date of the current LTP release -f CMDFILES Execute user defined list of testcases (separate with &apos;,&apos;)-F LOOPS,PERCENTAGE Induce PERCENTAGE Fault in the Kernel Subsystems, and, run each test for LOOPS loop -g HTMLFILE Create an additional HTML output format -h Help. Prints all available options. -i NUM_PROCS Run LTP under additional background Load on IO Bus [NUM_PROCS = no. of processes creating IO Bus Load by spinning over sync()] -K DMESG_LOG_DIRLog Kernel messages generated for each test cases inside this directory -l LOGFILE Log results of test in a logfile. -m NUM_PROCS,CHUNKS,BYTES,HANGUP_FLAG Run LTP under additional background Load on Main memory (Seperate by comma) [NUM_PROCS = no. of processes creating main Memory Load by spinning over malloc()] [CHUNKS = malloc these many chunks (default is 1 when value 0 or undefined)] [BYTES = malloc CHUNKS of BYTES bytes (default is 256MB when value 0 or undefined) ] [HANGUP_FLAG = hang in a sleep loop after memory allocated, when value 1]-M CHECK_TYPE[CHECK_TYPE=1 =&gt; Full Memory Leak Check tracing children as well][CHECK_TYPE=2 =&gt; Thread Concurrency Check tracing children as well][CHECK_TYPE=3 =&gt; Full Memory Leak &amp; Thread Concurrency Check tracing children as well] # 运行所有网络测试 -N Run all the networking tests. # 在后台额外增加网络流量的方式运行LTP -n Run LTP with network traffic in background. # 将测试输出重定向到文件 -o OUTPUTFILE Redirect test output to a file. # 友好可读格式日志文件。 -p Human readable format logfiles. # 打印较少的详细输出到屏幕。 这意味着不在kernel log中记录测试的开始信息 -q Print less verbose output to screen. This implies not logging start of the test in kernel log. # 不在kernel log中记录测试开始信息 -Q Don&apos;t log start of test in kernel log. # 安装testsuite的绝对路径 -r LTPROOT Fully qualified path where testsuite is installed. # 随机化测试顺序 -R Randomize test order. # 只运行与模式匹配的测试用例 -s PATTERN Only run test cases which match PATTERN. # 跳过SKIPFILE中指定的测试 -S SKIPFILE Skip tests specified in SKIPFILE # 在给定的时间内持续执行testsuite。例如： -t DURATION Execute the testsuite for given duration. Examples: -t 60s = 60 seconds -t 45m = 45 minutes -t 24h = 24 hours -t 2d = 2 days # 执行testsuite迭代次数 -I ITERATIONS Execute the testsuite ITERATIONS times. # 使用wget获取用户的测试用例列表 -w CMDFILEADDR Uses wget to get the user&apos;s list of testcases. # 运行此testsuite的多个实例 -x INSTANCES Run multiple instances of this testsuite. # 有些测试需要一个unmounted的块设备才能正常运行 -b DEVICE Some tests require an unmounted block device to run correctly. # 测试块设备的文件系统 -B LTP_DEV_FS_TYPE The file system of test block devices. # 某些测试需要大型未安装的块设备才能正常运行 -z BIG_DEVICE Some tests require a big unmounted block device to run correctly. # 大设备的文件系统 -Z LTP_BIG_DEV_FS_TYPE The file system of the big device example: runltp -c 2 -i 2 -m 2,4,10240,1 -D 2,10,10240,1 -p -q -l /tmp/result-log.3140 -o /tmp/result-output.3140 -C /tmp/result-failed.3140 -d /root/ltp-install]]></content>
      <tags>
        <tag>LTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Visual Studio Code 常用插件]]></title>
    <url>%2F2019%2F03%2F03%2FVisual-Studio-Code-%E5%B8%B8%E7%94%A8%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Mac下常用插件 123456789101、vscode-icons：根据文件不同类型，显示不同图标(1)安装后激活：shift + command + p(2)激活vscode-icons2、Project Manager：管理多个项目(1)首先save项目：shift + command + p --&gt; Project Manager Save Project(2)可以编辑：shift + command + p --&gt; Project Manager Edit Project(3)列出所有项目，在不同的项目之间切换：alt + command + p3、Code Runner：快速运行测试代码的插件]]></content>
      <tags>
        <tag>Visual Studio Code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis笔记]]></title>
    <url>%2F2019%2F01%2F15%2Fredis%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[对于redis的用处不必多说，互联网上有很多资料。文章只做为笔记使用，记录redis操作与学习过程。 redis安装安装redis很简单，https://redis.io/download官方文档有教程。 1[root@centos ~]# wget http://download.redis.io/releases/redis-5.0.3.tar.gz &amp;&amp; tar xzf redis-5.0.3.tar.gz &amp;&amp; cd redis-5.0.3 &amp;&amp; make &amp;&amp; make install redis启动方式非守护进程启动方式 1234[root@centos ~]# redis-server或者传递一些配置参数[root@centos ~]# redis-server --port 6380 守护进程启动方式 12345678910111213配置文件中开启daemonize yes[root@centos ~]# redis-server &lt;configPath&gt;[root@centos ~]# mkdir -p /var/log/redis[root@centos redis-5.0.3]# vim myredis.conf port 6379daemonize yeslogfile &quot;redis.log&quot;dir &quot;/var/log/redis&quot;[root@centos redis-5.0.3]# redis-server ./myredis.conf [root@centos ~]# ps -ef | grep redisroot 2447 1 0 11:01 ? 00:00:00 redis-server *:6379 redis常用配置123456daemonize 是否是守护进程(no|yes)port redis对外端口号logfile redis系统日志dir redis工作目录redis默认端口是6379 从默认配置文件中导出常用的配置项 12去掉&quot;#&quot;，去掉空格[root@centos redis-5.0.3]# cat redis.conf | grep -v &quot;#&quot; | grep -v &quot;^$&quot; &gt; myredis.conf redis API通过redis-cli命令进入客户端，测试API 通用命令通用命令keys [pattern]（一般不在生产环境使用，O(n)，redis单线程会阻塞其他命令） 1234567891011121314151617181920212223242526127.0.0.1:6379&gt; set hello worldOK127.0.0.1:6379&gt; set php goodOK127.0.0.1:6379&gt; set java bestOK127.0.0.1:6379&gt; keys *1) &quot;hello&quot;2) &quot;java&quot;3) &quot;php&quot;127.0.0.1:6379&gt; mset 1 1 2 2 3 3 4 4 5 5 12 12 13 13 123 123 1234 1234OK127.0.0.1:6379&gt; keys 1*1) &quot;1&quot;2) &quot;12&quot;3) &quot;13&quot;4) &quot;123&quot;5) &quot;1234&quot;127.0.0.1:6379&gt; keys 1[2-3]*1) &quot;12&quot;2) &quot;13&quot;3) &quot;123&quot;4) &quot;1234&quot;127.0.0.1:6379&gt; keys 12?1) &quot;123&quot; dbsize 123456789101112127.0.0.1:6379&gt; set hello worldOK127.0.0.1:6379&gt; set php goodOK127.0.0.1:6379&gt; set java bestOK127.0.0.1:6379&gt; keys *1) &quot;hello&quot;2) &quot;java&quot;3) &quot;php&quot;127.0.0.1:6379&gt; dbsize(integer) 3 exists key 12 del key [key …] 12 expire key seconds 12 type key 12 数据结构和内部编码单线程架构字符串类型 12 哈希类型 12 列表类型 12 集合类型 12 有序集合类型 12]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux NFS文件共享]]></title>
    <url>%2F2019%2F01%2F09%2FLinux-NFS%E6%96%87%E4%BB%B6%E5%85%B1%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[NFSNFS（Network File System）是Linux系统之间（类Unix系统之间）使用最为广泛的文件共享协议，不同于ftp或http，需要将文件下载后使用，NFS方式的共享是可以直接使用而不需要下载的。 NFS协议参考Wiki NFSv1只在SUN公司内部用作实验目的 NFSv2 基于UDP 由于 32-bit 的限制，NFSv2 只允读写文件起始2G大小的内容 NFSv3其主要目的是解决NFSv2进行同步写操作的性能问题。 支持 64 bit 文件大小和偏移量，即突破 2GB 文件大小的限制 支持服务端的异步写操作，提升写入性能 在许多响应报文中额外增加文件属性，避免用到这些属性时重新获取 增加READDIRPLUS调用，用于在遍历目录时获取文件描述符和文件属性 增加了将TCP作为传输层的支持 NFSv4协议的实现/开发工作不再是由SUN公司主导开发，而是改为由互联网工程任务组（IETF）开发。 性能提升 强制安全策略 引入有状态的协议 NFSv4.1 为并行访问可横向扩展的集群服务（pNFS扩展）提供协议支持 NFSv4.2NFSv4.2 当前正在开发中 如何使用假设一个Unix风格的场景，其中一台计算机（客户端）需要访问存储在其他机器上的数据（NFS 服务器）： 1、服务端实现 NFS 守护进程，默认运行 nfsd，用来使得数据可以被客户端访问。2、服务端系统管理员可以决定哪些资源可以被访问，导出目录的名字和参数，通常使用 /etc/exports 配置文件 和 exportfs 命令。3、服务端安全-管理员保证它可以组织和认证合法的客户端。4、服务端网络配置保证可以跟客户端透过防火墙进行协商。5、客户端请求导出的数据，通常调用一个 mount 命令。6、如果一切顺利，客户端的用户就可以通过已经挂载的文件系统查看和访问服务端的文件了。提醒：NFS自动挂载可以通过—可能是 /etc/fstab 或者自动安装管理进程。 环境信息NFS服务端IP地址为：192.168.56.101 NFS客户端IP地址为：192.168.56.240 NFS服务端1、在Centos7上安装软件包 1[root@centos ~]# yum install -y nfs-utils rpcbind 2、创建共享目录并更改权限 12345[root@centos ~]# mkdir /var/nfsshare &amp;&amp; chmod -R 755 /var/nfsshare[root@centos ~]# chown nfsnobody:nfsnobody /var/nfsshare[root@centos ~]# ll -l /var/drwxr-xr-x. 2 nfsnobody nfsnobody 6 1月 9 10:41 nfsshare 3、启动服务并设置开机自启 1234567[root@centos ~]# systemctl enable rpcbind &amp;&amp; systemctl enable nfs-server &amp;&amp; systemctl enable nfs-lock &amp;&amp; systemctl enable nfs-idmap &amp;&amp; systemctl start rpcbind &amp;&amp; systemctl start nfs-server &amp;&amp; systemctl start nfs-lock &amp;&amp; systemctl start nfs-idmap重启所有服务命令[root@centos ~]# systemctl enable rpcbind &amp;&amp; systemctl enable nfs-server &amp;&amp; systemctl enable nfs-lock &amp;&amp; systemctl enable nfs-idmap &amp;&amp; systemctl restart rpcbind &amp;&amp; systemctl restart nfs-server &amp;&amp; systemctl restart nfs-lock &amp;&amp; systemctl restart nfs-idmap无需重启NFS服务，使exports文件生效[root@centos ~]# exportfs -ra 4、编辑/etc/exports文件，通过网络共享NFS目录 1注意：192.168.56.240是NFS客户端IP 123456789101112131415161718192021222324252627[root@centos ~]# vi /etc/exports/var/nfsshare 192.168.56.240(rw,sync,no_root_squash,no_all_squash)/home 192.168.56.240(rw,sync,no_root_squash,no_all_squash)# 配置文件分三段# 第一段是将要共享出去的本地目录# 第二段是允许访问的主机(可以是一个IP也可以是一个IP段192.168.56.0/24)# 第三段是权限客户端 示例使用IP地址指定单一主机 10.20.30.40使用IP地址指定范围主机 172.16.0.0/16使用IP地址指定范围主机 192.168.1.*使用域名指定单一主机 Test.ice.apple使用域名指定范围主机 *.ice.apple使用通配符指定所有主机 *参数 说明ro 设置共享权限为只读rw 设置共享权限为读写root_squash 共享目录的使用者是root时，将被映射为匿名账号（nobody）no_root_squash 当使用NFS服务器共享目录的使用者是root时，将不被映射为匿名账号all_squash 将所有使用NFS服务器共享目录的使用者都映射为匿名账号anonuid 设置匿名账号的UIDanongid 设置匿名账号的GIDsync 将数据同步写入内存和硬盘。这可能导致效率降低async 先将数据保存在内存中，而不是直接保存在硬盘 12345678910111213141516171819选项含义： rw 允许对共享目录进行读写 sync 实时同步共享目录 no_root_squash 允许root访问 no_all_squash 允许用户授权默认选项如下： ro 只读共享 sync 所有操作返回前必须已经写入磁盘 wdelay 延迟写操作，等待更多的写操作一起执行 root_squash 防止远程root用户使用root权限对共享进行访问。NFS客户端连接服务端时如果使用的是root的话，那么对服务端分享的目录来说，拥有匿名用户权限，通常他将使用nobody或nfsnobody身份。 常用选项： rw 读写共享 async 操作可以延迟写入硬盘，立即响应，可提高速度 no_wdelay 关闭写延迟，需要与sync选项同时使用 no_root_squash 关闭root_squash，NFS客户端连接服务端时如果使用的是root的话，那么对服务端分享的目录来说，也拥有root权限。显然开启这项是不安全的。 secure 此选项要求请求源自小于IPPORT_RESERVED（1024）的Internet端口。限制客户端只能从小于1024的tcp/ip端口连接nfs服务器（默认设置） insecure 此选项接受所有端口，允许客户端从大于1024的tcp/ip端口连接服务器 5、重启服务 1[root@centos ~]# systemctl restart nfs-server 6、修改防火墙 1234[root@centos ~]# firewall-cmd --permanent --zone=public --add-service=nfs[root@centos ~]# firewall-cmd --permanent --zone=public --add-service=mountd[root@centos ~]# firewall-cmd --permanent --zone=public --add-service=rpc-bind[root@centos ~]# firewall-cmd --reload 1234挂载选项： ro 只读挂载 rw 读写挂载 vers=4 指定使用NFSv4方式挂载 NFS客户端1、在Centos7上安装软件包 1[root@docker ~]# yum install -y nfs-utils 2、创建NFS挂载点 12[root@docker ~]# mkdir -p /mnt/nfs/home[root@docker ~]# mkdir -p /mnt/nfs/var/nfsshare 3、挂载服务端的NFS共享目录到客户端 12[root@docker ~]# mount -t nfs 192.168.56.101:/home /mnt/nfs/home/[root@docker ~]# mount -t nfs 192.168.56.101:/var/nfsshare /mnt/nfs/var/nfsshare/ 4、检查 1234[root@docker ~]# df -Tkh文件系统 类型 容量 已用 可用 已用% 挂载点192.168.56.101:/home nfs4 17G 4.6G 13G 27% /mnt/nfs/home192.168.56.101:/var/nfsshare nfs4 17G 4.6G 13G 27% /mnt/nfs/var/nfsshare 5、读写文件 在客户端向挂载点写入文件 1234[root@docker ~]# touch /mnt/nfs/var/nfsshare/test_nfs[root@docker ~]# ll /mnt/nfs/var/nfsshare/总用量 0-rw-r--r--. 1 root root 0 1月 9 11:01 test_nfs 在服务端检查写入的文件 123[root@centos ~]# ll /var/nfsshare/总用量 0-rw-r--r--. 1 root root 0 1月 9 11:01 test_nfs 参考文章1【1】https://www.linuxidc.com/Linux/2013-10/91137.htm]]></content>
      <tags>
        <tag>NFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph中的单元测试]]></title>
    <url>%2F2019%2F01%2F07%2Fceph%E4%B8%AD%E7%9A%84%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[Ceph中的单元测试使用的是Google Test，嵌入到Ceph源码中。Google Test的使用这里有一篇入门文章，大体思路讲的很清晰。我们主要重心是如何在Ceph源码中添加单元测试。 在添加之前我们先读一下已经存在的单元测试，在代码中不同的单元测试实现方式不同，可能按照以下Demo无法完全复制，但是慢慢看代码总会理解的。我们挑一个最简单的，不讲解详细测试内容（因为涉及知识较多，目前我还无法完全理解）。 下面我们以ceph_test_filejournal为例，它存在于ceph-test*.rpm包中，在打社区版Ceph rpm包时（根据ceph.spec）会生成出来《ceph build rpm》，在我们的被测Ceph节点上安装该rpm，会在/usr/bin/目录中找到ceph_test_filejournal（其实会有很多类似ceph_test_的二进制文件，这些都是Ceph的单元测试）。 CMakeLists单元测试总CMakeLists文件位置在src/test/CMakeLists.txt，如果想自己编写测试代码添加到测试集中，需要修改这个CMakeLists。在社区Ceph我们编译出来可执行测试二进制ceph_test_*，都是在这里通过CMakeLists add_executable定义的。 单元测试代码位置123add_executable(ceph_test_filejournal test_filejournal.cc ) 在add_executable中同时定义了测试代码源文件test_filejournal.cc。 1::testing::InitGoogleTest(&amp;argc, argv); 参考【1】http://www.yeolar.com/note/2014/12/21/gtest/#id15 【2】https://www.ibm.com/developerworks/cn/aix/library/au-googletestingframework.html]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7 使用ceph-ansible部署ceph L版]]></title>
    <url>%2F2019%2F01%2F06%2Fcentos7-%E4%BD%BF%E7%94%A8ceph-ansible%E9%83%A8%E7%BD%B2ceph-L%E7%89%88%2F</url>
    <content type="text"><![CDATA[安装ansible 2.4.2使用yum安装指定版本的ansible 1234567891011121314151617181920212223242526[root@cephJ ~]# yum -h | grep show --showduplicates 在 list/search 命令下，显示源里重复的条目[root@cephJ ~]# yum --showduplicates list ansible已加载插件：fastestmirror, prioritiesLoading mirror speeds from cached hostfile * base: mirrors.cn99.com * epel: www.ftp.ne.jp * extras: mirrors.nwsuaf.edu.cn * nux-dextop: li.nux.ro * updates: mirrors.nwsuaf.edu.cn12 packages excluded due to repository priority protections可安装的软件包ansible.noarch 2.4.2.0-2.el7 extrasansible.noarch 2.7.5-1.el7 epel ### sudo yum install &lt;package name&gt;-&lt;version info&gt;[root@cephJ ~]# sudo yum install -y ansible-2.4.2.0[root@cephJ ~]# ansible --versionansible 2.4.2.0 config file = /etc/ansible/ansible.cfg configured module search path = [u&apos;/root/.ansible/plugins/modules&apos;, u&apos;/usr/share/ansible/plugins/modules&apos;] ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /usr/bin/ansible python version = 2.7.5 (default, Apr 11 2018, 07:36:10) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] 配置ceph-ansible准备四台机器 1234567ansible节点 ansible-masterceph节点 ansible-ceph-1 ansible-ceph-2 ansible-ceph-3 配置ansible 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101ansible节点执行1、首先clone代码[root@ansible-master ~]# git clone https://github.com/ceph/ceph-ansible.git &amp;&amp; cd ceph-ansible/2、创建本地分支并切换分支[root@ansible-master ceph-ansible]# git checkout -b myv3.2.0 v3.2.0 3、copy模板文件[root@ansible-master ceph-ansible]# cp site.yml.sample site.yml4、修改ansible机器清单(inventory)[root@ansible-master ~]# vim /etc/ansible/hosts[mons]ansible-ceph-[1:3] ansible_ssh_pass=yujiang2[osds]ansible-ceph-[1:3] ansible_ssh_pass=yujiang2[rgws]ansible-ceph-[1:3] ansible_ssh_pass=yujiang2[mgrs]ansible-ceph-[1:3] ansible_ssh_pass=yujiang25、批量推送sshkey[root@ansible-master ~]# ssh-keygen -t rsa[root@ansible-master ~]# cat push-ssh.yaml - hosts: all user: root tasks: - name: ssh-key-copy authorized_key: user=root key=&quot;&#123;&#123; lookup(&apos;file&apos;,&apos;/root/.ssh/id_rsa.pub&apos;)&#125;&#125;&quot; tags: - sshkey[root@ansible-master ~]# ansible-playbook push-ssh.yamlansible-ceph-1 : ok=2 changed=1 unreachable=0 failed=0 ansible-ceph-2 : ok=2 changed=1 unreachable=0 failed=0 ansible-ceph-3 : ok=2 changed=1 unreachable=0 failed=0 6、安装pip并安装ceph-ansible依赖[root@ansible-master ~]# curl &quot;https://bootstrap.pypa.io/get-pip.py&quot; -o &quot;get-pip.py&quot; &amp;&amp; python get-pip.py &amp;&amp; pip install --upgrade setuptools[root@ansible-master ceph-ansible]# pip install -r requirements.txt7、修改ansible变量[root@ansible-master group_vars]# pwd/root/ceph-ansible/group_vars[root@ansible-master group_vars]# cp all.yml.sample all.yml &amp;&amp; cp osds.yml.sample osds.yml &amp;&amp; cp mgrs.yml.sample mgrs.ymlall.yml修改如下：mon_group_name: monsosd_group_name: osdsntp_daemon_type: chronydceph_origin: repositoryceph_repository: communityceph_mirror: http://mirrors.163.com/cephceph_stable_key: http://mirrors.163.com/ceph/keys/release.ascceph_stable_release: luminousceph_stable_repo: &quot;&#123;&#123; ceph_mirror &#125;&#125;/rpm-&#123;&#123; ceph_stable_release &#125;&#125;/el7/x86_64&quot;monitor_interface: eth0public_network: 192.168.0.0/24cluster_network: 192.168.0.0/24radosgw_interface: eth0radosgw_address: 0.0.0.0osds.yml修改如下：copy_admin_key: truedevices: - /dev/vdbosd_scenario: collocatedmgrs.yml修改如下：ceph_mgr_modules: [status,dashboard]8、执行ansible开始部署ceph，部署哪些模块会在/etc/ansible/hosts中定义（下面是我们之前定义的），如果在该文件中没有对应模块定义，ansible会忽略该模块的部署。[mons]ansible-ceph-[1:3] ansible_ssh_pass=yujiang2[osds]ansible-ceph-[1:3] ansible_ssh_pass=yujiang2[rgws]ansible-ceph-[1:3] ansible_ssh_pass=yujiang2[mgrs]ansible-ceph-[1:3] ansible_ssh_pass=yujiang29、修改ansible host_key_checking[root@ansible-master ~]# vim /etc/ansible/ansible.cfg[defaults]host_key_checking = False[root@ansible-master ceph-ansible]# pwd/root/ceph-ansible[root@ansible-master ceph-ansible]# ansible-playbook site.ymlPLAY RECAP *******************************************************************************************************************************************************************************************************************************************************************ansible-ceph-1 : ok=310 changed=17 unreachable=0 failed=0 ansible-ceph-2 : ok=287 changed=17 unreachable=0 failed=0 ansible-ceph-3 : ok=289 changed=20 unreachable=0 failed=0 INSTALLER STATUS *************************************************************************************************************************************************************************************************************************************************************Install Ceph Monitor : Complete (0:00:37)Install Ceph Manager : Complete (0:01:05)Install Ceph OSD : Complete (0:00:38)Install Ceph RGW : Complete (0:00:30)]]></content>
      <tags>
        <tag>ceph-ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在CentOS 7上将BIND配置为专用网络DNS服务器]]></title>
    <url>%2F2019%2F01%2F06%2F%E5%A6%82%E4%BD%95%E5%9C%A8CentOS-7%E4%B8%8A%E5%B0%86BIND%E9%85%8D%E7%BD%AE%E4%B8%BA%E4%B8%93%E7%94%A8%E7%BD%91%E7%BB%9CDNS%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Ceph的对象存储服务中有网站托管服务，需要结合DNS来做配合。所以想借此机会了解一下DNS服务，DNS开源软件中最有名的当属BIND。那么如果搭建该服务？如何与Ceph的对象存储服务radosgateway对接？这两个课题是需要来实验的。这篇文章不包括DNS理论，因为在网络上有很多，可以自己google一下。本篇文章主要是：如果搭建BIND服务，以实践为主，我在网络上找到一篇英文文章，是关于BIND在centos7上搭建的，如果英文可以请看原文。 下面我们就来验证一下BIND的搭建： Introduction 介绍通过设置适当的域名系统（DNS, Domain Name System），可以按名称查找network interfaces和IP地址，这使得管理server configuration和infrastructure变得更容易。在本教程中，我们将使用CentOS 7上的BIND name servers软件（BIND9）来讨论如何设置内部DNS服务器，Virtual Private Servers (VPS) 可以使用它来解析private host names和private IP地址。这是管理内部多主机hostnames和私有IP地址时必不可少的。 可以在此处找到本教程的Ubuntu版本。 Prerequisites 先决条件要完成本教程，您需要以下内容： 同一datacenter，并运行在同一private networking下的多台server（可以是虚拟机） Primary DNS server，ns1 Secondary DNS server，ns2 Root访问以上所有内容（steps 1-4 here） 如果您不熟悉DNS概念，建议您阅读Introduction to Managing DNS。 Example Hosts 示例主机出于示例目的，我们将假设以下内容： 我们有两个现有的VPS叫做”host1”和”host2” 两个VPS都存在于nyc3 datacenter 两个VPS都启用了private networking（并且位于10.128.0.0/16子网上） 两个VPS都与我们在”example.com”上运行的Web应用程序有某种关联 有了这些假设，我们使用”nyc3.example.com”来参考我们的private subnet或zone的命名方案。因此，host1的private Fully-Qualified Domain Name (FQDN) 将为”host1.nyc3.example.com”。 请参阅下表中的相关详细信息： Host Role Private FQDN Private IP Address host1 Generic Host 1 host1.nyc3.example.com 10.128.100.101 host2 Generic Host 2 host2.nyc3.example.com 10.128.200.102 注意：自己的实验环境将有所不同，示例中的名称和IP地址只是用于演示如何配置DNS server以提供内部DNS服务。您应该设置适合自己环境的host names和private IP addresses。没有必要在命名方案中使用datacenter的region名称，但我们在此处使用它来表示这些主机属于特定datacenter的private network。如果您使用多个datacenters，则可以在每个相应的datacenter内设置内部DNS。 Our Goal 我们的目标在本教程结束时，我们将有一个primary DNS server（ns1），以及可选的secondary DNS server（ns2，它将用作备份）。 这是一个包含示例名称和IP地址的表： Host Role Private FQDN Private IP Address ns1 Primary DNS Server ns1.nyc3.example.com 10.128.10.11 ns2 Secondary DNS Server ns2.nyc3.example.com 10.128.20.12 让我们开始安装我们的Primary DNS server，ns1。 Install BIND on DNS Servers 在DNS服务器上安装BIND注意：以红色突出显示的文本很重要！ 它通常用于表示需要用您自己的设置替换的内容，或者应该修改或添加到配置文件中的内容。（红色字体我会标注red，全文搜索red就可以）例如，如果您看到类似host1.nyc3.example.com（red）的内容，请将其替换为您自己服务器的FQDN。同样，如果您看到host1_private_IP（red），请将其替换为您自己服务器的private IP地址。 在两个DNS服务器ns1和ns2上，使用yum安装BIND： 1sudo yum install -y bind bind-utils 现在已经安装了BIND，让我们配置primary DNS server。 Configure Primary DNS Server 配置主DNS服务器BIND的配置由多个文件组成，这些文件包含在主配置文件named. conf中。 这些文件名以“named”开头，因为这是BIND运行时进程的名称。 我们将从配置options文件开始。 Configure Bind 配置BindBIND的进程名为named，因此，许多文件以”named”命名，而不是”BIND”。 在ns1上，打开named.conf文件进行编辑： 1sudo vi /etc/named.conf 在现有options block上方，创建一个名为”trusted”的新ACL block。在这里我们将定义允许递归查询DNS的客户端列表（即与ns1位于同一datacenter的服务器）。使用我们的示例private IP地址，我们将ns1，ns2，host1和host2的IP添加到可信客户端列表中： 12345678/etc/named.conf — 1 of 4acl &quot;trusted&quot; &#123; 10.128.10.11; # ns1 - can be set to localhost 10.128.20.12; # ns2 10.128.100.101; # host1 10.128.200.102; # host2&#125;; 我们有了可信DNS客户端列表，我们将要编辑options block。将ns1的private IP添加到listen-on port 53指令中，并注释掉listen-on-v6行： 123456/etc/named.conf — 2 of 4options &#123; listen-on port 53 &#123; 127.0.0.1; 10.128.10.11; &#125;;# listen-on-v6 port 53 &#123; ::1; &#125;;... 在这些条目下面，将allow-transfer指令从”none”更改为ns2的private IP地址。另外，将allow-query指令从”localhost”更改为”trusted”： 123456789/etc/named.conf — 3 of 4...options &#123;... allow-transfer &#123; 10.128.20.12; &#125;; # disable zone transfers by default... allow-query &#123; trusted; &#125;; # allows queries from &quot;trusted&quot; clients... 在文件末尾，添加以下行： 123/etc/named.conf — 4 of 4include &quot;/etc/named/named.conf.local&quot;; 现在保存并退出named.conf。上述配置指定只有您自己的服务器（”trusted”服务器）才能查询DNS服务器。 接下来，我们将配置local文件，以指定我们的DNS zones。 Configure Local File 配置Local文件在ns1上，打开named.conf.local文件以进行编辑： 1sudo vi /etc/named/named.conf.local 该文件应为空。 在这里，我们将指定forward和reverse zones。 使用以下行添加forward zone（替换您自己的zone名称）： 123456/etc/named/named.conf.local — 1 of 2zone &quot;nyc3.example.com&quot; &#123; type master; file &quot;/etc/named/zones/db.nyc3.example.com&quot;; # zone file path&#125;; 假设我们的private subnet是10.128.0.0/16，请使用以下行添加reverse zone（请注意，我们的reverse zone名称以”128.10”开头，这是”10.128”的八位字节反转，octet reversal of “10.128”） 123456/etc/named/named.conf.local — 2 of 2zone &quot;128.10.in-addr.arpa&quot; &#123; type master; file &quot;/etc/named/zones/db.10.128&quot;; # 10.128.0.0/16 subnet &#125;; 如果您的服务器跨多个private subnets但位于同一datacenter，请确保为每个不同的subnet指定一个额外的zone和zone文件。添加完所有所需zone后，保存并退出named.conf.local文件。 既然我们的zone是在BIND中指定的，我们需要创建相应的forward和reverse zone文件。 Create Forward Zone File 创建Forward Zone文件forward zone文件中，我们为正向DNS查找定义了DNS records（记录）。也就是说，当DNS收到名称查询（例如”host1.nyc3.example.com”）时，它将查找forward zone文件以解析host1的相应private IP地址。 创建我们的zone文件。根据我们的named.conf.local配置，该位置应为/etc/named/zones： 12sudo chmod 755 /etc/namedsudo mkdir /etc/named/zones 现在让我们编辑我们的forward zone文件： 1sudo vi /etc/named/zones/db.nyc3.example.com 首先，您需要添加SOA record（记录）。将突出显示的ns1 FQDN替换为您自己的FQDN，然后将第二个”nyc3.example.com”替换为您自己的domain。 每次编辑zone文件时，都应该在重新启动named进程之前增加serial值 —— 我们将它增加到”3”。它应该看起来像这样： 12345678/etc/named/zones/db.nyc3.example.com — 1 of 3@ IN SOA ns1.nyc3.example.com. admin.nyc3.example.com. ( 3 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL 之后，使用以下行添加您的nameserver records（替换自己的名称nyc3.example.com.）。请注意，第二列指定这些是”NS” records（记录）： 12345/etc/named/zones/db.nyc3.example.com — 2 of 3; name servers - NS records IN NS ns1.nyc3.example.com. IN NS ns2.nyc3.example.com. 然后为属于此zone的主机添加A记录。 这包括我们希望以”.nyc3.example.com” 结尾的名称的任何服务器（替换名称和private IP地址）。使用我们的示例名称和private IP地址，我们将为ns1，ns2，host1和host2添加A记录，如下所示： 123456789/etc/named/zones/db.nyc3.example.com — 3 of 3; name servers - A recordsns1.nyc3.example.com. IN A 10.128.10.11ns2.nyc3.example.com. IN A 10.128.20.12; 10.128.0.0/16 - A recordshost1.nyc3.example.com. IN A 10.128.100.101host2.nyc3.example.com. IN A 10.128.200.102 保存并退出db.nyc3.example.com文件。 我们的最终示例forward zone文件如下所示： 123456789101112131415161718192021/etc/named/zones/db.nyc3.example.com — complete$TTL 604800@ IN SOA ns1.nyc3.example.com. admin.nyc3.example.com. ( 3 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL;; name servers - NS records IN NS ns1.nyc3.example.com. IN NS ns2.nyc3.example.com.; name servers - A recordsns1.nyc3.example.com. IN A 10.128.10.11ns2.nyc3.example.com. IN A 10.128.20.12; 10.128.0.0/16 - A recordshost1.nyc3.example.com. IN A 10.128.100.101host2.nyc3.example.com. IN A 10.128.200.102 现在让我们转到reverse zone文件。 Create Reverse Zone File(s) 创建Reverse Zone文件Reverse zone文件是我们为反向DNS查找定义DNS PTR记录的地方。也就是说，当DNS通过IP地址（例如”10.128.100.101”）接收查询时，它将查看reverse zone文件以解析相应的FQDN，在这种情况下查询结果为”host1.nyc3.example.com”。 在ns1上，对于named.conf.local文件中指定的每个reverse zone，都创建一个reverse zone文件。 编辑与named.conf.local中定义的reverse zone对应的reverse zone文件： 1sudo vi /etc/named/zones/db.10.128 与forward zone文件相同的方式，将突出显示的ns1 FQDN替换为您自己的FQDN，然后将第二个”nyc3.example.com”替换为您自己的domain。每次编辑zone文件时，都应该在重新启动named进程之前递增serial值 —— 我们将其增加到”3”。 它应该看起来像这样： 12345678/etc/named/zones/db.10.128 — 1 of 3@ IN SOA ns1.nyc3.example.com. admin.nyc3.example.com. ( 3 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL 之后，使用以下行添加您的nameserver records（用您自己的名称替换）。 请注意，第二列指定这些是”NS”记录： 12345/etc/named/zones/db.10.128 — 2 of 3; name servers - NS records IN NS ns1.nyc3.example.com. IN NS ns2.nyc3.example.com. 然后为所有服务器添加PTR记录，这些服务器的IP地址位于您正在编辑的zone文件的subnet上。在我们的示例中，这包括我们所有的主机，因为它们都在10.128.0.0/16 subnet上。请注意，第一列包含服务器private IP地址的最后两个八位字节，顺序相反。请务必替换名称和private IP地址以匹配您的服务器： 1234567/etc/named/zones/db.10.128 — 3 of 3; PTR Records11.10 IN PTR ns1.nyc3.example.com. ; 10.128.10.1112.20 IN PTR ns2.nyc3.example.com. ; 10.128.20.12101.100 IN PTR host1.nyc3.example.com. ; 10.128.100.101102.200 IN PTR host2.nyc3.example.com. ; 10.128.200.102 保存并退出reverse zone文件（如果需要添加更多reverse zone文件，请重复此部分）。 我们的最终示例反向区域文件如下所示： 123456789101112131415161718/etc/named/zones/db.10.128 — complete$TTL 604800@ IN SOA nyc3.example.com. admin.nyc3.example.com. ( 3 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL; name servers IN NS ns1.nyc3.example.com. IN NS ns2.nyc3.example.com.; PTR Records11.10 IN PTR ns1.nyc3.example.com. ; 10.128.10.1112.20 IN PTR ns2.nyc3.example.com. ; 10.128.20.12101.100 IN PTR host1.nyc3.example.com. ; 10.128.100.101102.200 IN PTR host2.nyc3.example.com. ; 10.128.200.102 Check BIND Configuration Syntax 检查BIND配置语法运行以下命令以检查named.conf *文件的语法： 1sudo named-checkconf 如果您的named配置文件没有语法错误，您将返回shell提示符并且看不到任何错误消息。 如果配置文件有问题，请检查error message和Configure Primary DNS Server section，然后再次尝试named-checkconf。 named-checkzone命令可用于检查zone文件的正确性。 其第一个参数指定zone名称，第二个参数指定相应的zone文件，这两个文件都在named.conf.local中定义。 例如，要检查”nyc3.example.com”的forward zone配置，请运行以下命令（更改名称以匹配你自己的forward zone和文件）： 1sudo named-checkzone nyc3.example.com /etc/named/zones/db.nyc3.example.com 要检查”128.10.in-addr.arpa”的reverse zone配置，请运行以下命令（更改数字以匹配你自己的reverse zone和文件）： 1sudo named-checkzone 128.10.in-addr.arpa /etc/named/zones/db.10.128 当所有配置和zone文件都没有错误时，您应该重新启动BIND服务。 Start BIND 启动BIND启动BIND： 1sudo systemctl start named 如果你想在OS启动时自动启动BIND服务，请执行： 1sudo systemctl enable named 您的primary DNS服务器现已设置好并可以响应DNS查询了。 让我们继续创建secondary DNS服务器。 Configure Secondary DNS Server 配置备用DNS服务器在大多数环境中，最好设置secondary DNS服务器，以便在primary服务器不可用时响应请求。幸运的是，secondary DNS服务器更容易配置。 在ns2上，编辑named.conf文件： 1sudo vi /etc/named.conf 1注意：如果您希望跳过这些说明，可以复制ns1的named.conf文件并对其进行修改以侦听ns2的private IP地址，and not allow transfers。 在现有options block上方，创建一个名为”trusted”的新ACL block。在这里我们将定义允许递归DNS查询的客户端列表（即与ns1位于同一datacenter的服务器）。使用我们的示例private IP地址，我们将ns1，ns2，host1和host2添加到trusted（可信）客户端列表中： 12345678/etc/named.conf — 1 of 4acl &quot;trusted&quot; &#123; 10.128.10.11; # ns1 - can be set to localhost 10.128.20.12; # ns2 10.128.100.101; # host1 10.128.200.102; # host2&#125;; 现在我们有了trusted（可信）DNS客户端列表，我们将要编辑options block。将ns1的private IP地址添加到listen-on port 53指令，并注释掉listen-on-v6行： 123456/etc/named.conf — 2 of 4options &#123; listen-on port 53 &#123; 127.0.0.1; 10.128.20.12; &#125;;# listen-on-v6 port 53 &#123; ::1; &#125;;... 将allow-query指令从”localhost”更改为”trusted”： 1234567/etc/named.conf — 3 of 4...options &#123;... allow-query &#123; trusted; &#125;; # allows queries from &quot;trusted&quot; clients... 在文件末尾，添加以下行： 123/etc/named.conf — 4 of 4include &quot;/etc/named/named.conf.local&quot;; 现在保存并退出named.conf。 上述配置指定只有您自己的服务器（”trusted”服务器）才能查询您的DNS服务。 接下来，我们将配置local文件，以指定我们的DNS zones。 保存并退出named.conf。 现在编辑named.conf.local文件： 12sudo chmod 755 /etc/namedsudo vi /etc/named/named.conf.local 定义与primary DNS服务器上的master zones对应的slave zones。请注意，类型是”slave”，文件不包含路径，并且有一个masters指令应该设置为primary DNS服务器的private IP。如果您在primary DNS服务器中定义了多个reverse zones，请确保在此处全部添加它们： 12345678910111213/etc/named/named.conf.localzone &quot;nyc3.example.com&quot; &#123; type slave; file &quot;slaves/db.nyc3.example.com&quot;; masters &#123; 10.128.10.11; &#125;; # ns1 private IP&#125;;zone &quot;128.10.in-addr.arpa&quot; &#123; type slave; file &quot;slaves/db.10.128&quot;; masters &#123; 10.128.10.11; &#125;; # ns1 private IP&#125;; 现在保存并退出named.conf.local。 运行以下命令以检查配置文件的有效性： 1sudo named-checkconf 检查后，启动BIND： 1sudo systemctl start named 如果你想在OS启动时自动启动BIND服务，请执行： 1sudo systemctl enable named 现在，您拥有用于private network名称和IP地址解析的primary DNS服务器和secondary DNS服务器。 现在，您必须配置客户端服务器以使用您的private DNS服务器。 Configure DNS Clients 配置DNS客户端在”trusted” ACL中的所有服务器都可以查询DNS服务器之前，必须将客户端服务器配置为使用ns1和ns2作为nameservers。 此过程因操作系统而异，但对于大多数Linux发行版，它涉及将name servers添加到/etc/resolv.conf文件中。 CentOS Clients在CentOS，RedHat和Fedora Linux VPS上，只需编辑resolv.conf文件： 1sudo vi /etc/resolv.conf 然后将以下行添加到文件的顶部（替换您的private domain，以及ns1和ns2 private IP地址）： 12345/etc/resolv.confsearch nyc3.example.com # your private domainnameserver 10.128.10.11 # ns1 private IP addressnameserver 10.128.20.12 # ns2 private IP address 现在保存并退出。 客户端现在已经配置为使用您自己的DNS服务器。 Ubuntu Clients在Ubuntu和Debian Linux VPS上，您可以编辑head文件，该文件在启动时附加到resolv.conf： 1sudo vi /etc/resolvconf/resolv.conf.d/head 将以下行添加到文件中（替换您的private domain，以及ns1和ns2 private IP地址）： 12345/etc/resolvconf/resolv.conf.d/headsearch nyc3.example.com # your private domainnameserver 10.128.10.11 # ns1 private IP addressnameserver 10.128.20.12 # ns2 private IP address 现在运行resolvconf以生成新的resolv.conf文件： 1sudo resolvconf -u 客户端现在已经配置为使用您自己的DNS服务器。 Test Clients使用nslookup —— 包含在”bind-utils”包中 —— 测试您的客户端是否可以查询您的name servers。您应该能够在已配置在”trusted” ACL中的所有客户端上执行此操作。 Forward Lookup例如，我们可以通过运行以下命令来执行forward（正向）查找以检索host1.nyc3.example.com的IP地址： 1nslookup host1 查询”host1”会扩展为”host1.nyc3.example.com”，因为搜索选项设置为您的private subdomain，在查找其他DNS server之前，DNS查询将尝试查看该subdomain。上面命令的输出如下所示： 123456Output:Server: 10.128.10.11Address: 10.128.10.11#53Name: host1.nyc3.example.comAddress: 10.128.100.101 Reverse Lookup要测试reverse（反向）查找，请使用host1的private IP地址查询DNS服务器： 1nslookup 10.128.100.101 您应该看到如下所示的输出： 12345Output:Server: 10.128.10.11Address: 10.128.10.11#5311.10.128.10.in-addr.arpa name = host1.nyc3.example.com. 如果所有名称和IP地址都解析为正确的值，则表示您的zone文件已正确配置。如果收到异常值，请务必查看primary DNS服务器上的zone文件（例如db.nyc3.example.com和db.10.128）。 恭喜！ 您的内部DNS服务器现已正确设置！ 现在我们将介绍如何维护您的zone records。 Maintaining DNS Records 维护DNS记录现在您有了一个正常工作的内部DNS，您需要维护DNS records，以便它们准确地响应您的server environment。 Adding Host to DNS 将主机添加到DNS每当您向environment添加主机（在同一datacenter中）时，都需要将其添加到DNS中。以下是您需要执行的步骤： Primary Nameserver（主Nameserver） Forward zone文件：为新主机添加”A”记录，增加”Serial”的值 Reverse zone文件：为新主机添加”PTR”记录，增加”Serial”的值 将新主机的private IP地址添加到”trusted” ACL（named.conf.options） 然后重新加载BIND： 1sudo systemctl reload named Secondary Nameserver（辅助Nameserver） 将新主机的private IP地址添加到”trusted” ACL（named.conf.options） 然后重新加载BIND： 1sudo systemctl reload named 配置新主机以使用您自己的DNS 配置resolv.conf以使用您自己的DNS服务器 使用nslookup进行测试 Removing Host from DNS 从DNS中删除主机如果您从environment中删除主机或想要将其从DNS中删除，只需删除将服务器添加到DNS时添加的所有内容（即上述步骤的相反步骤）。 Conclusion 结论现在，您可以按名称而不是IP地址来访问服务器的private network interfaces。这使得服务和应用程序的配置更加容易，因为您不再需要记住private IP地址，并且文件将更易于阅读和理解。此外，现在您可以更改配置，以便在单个位置指向新服务器，即primary DNS服务器，而不必编辑各种分布式配置文件，从而简化了维护。 设置内部DNS并且配置文件使用private FQDN指定网络连接后，维护DNS服务器至关重要。如果它们都不可用，那么依赖它们的服务和应用程序将无法正常运行。这就是为什么建议使用至少一个secondary服务器设置DNS，并维护所有这些服务器的工作备份原因。 验证 Host Role Private FQDN Private IP Address ns1 Primary DNS Server ns1.nyc3.example.com 192.168.0.22 ns2 Secondary DNS Server ns2.nyc3.example.com 192.168.0.23 dns-client DNS client 192.168.0.24 1、安装BIND在Primary DNS Server 1sudo yum install -y bind bind-utils 2、修改/etc/named.conf 123456789101112131415161718sudo vi /etc/named.confacl &quot;trusted&quot; &#123; 192.168.0.22; # ns1 - can be set to localhost 192.168.0.23; # ns2 192.168.0.24; # dns-client&#125;;options &#123; # options start # ... ... listen-on port 53 &#123; 127.0.0.1; 192.168.0.22; &#125;; # listen-on-v6 port 53 &#123; ::1; &#125;; # ... ... allow-transfer &#123; 192.168.0.23; &#125;; # disable zone transfers by default allow-query &#123; trusted; &#125;;&#125; # options endinclude &quot;/etc/named/named.conf.local&quot;; 12345678910111213sudo vi /etc/named/named.conf.localsudo chmod 755 /etc/namedsudo mkdir /etc/named/zonessudo vi /etc/named/zones/db.nyc3.example.comsudo vi /etc/named/zones/db.192.168sudo named-checkconfsudo named-checkzone nyc3.example.com /etc/named/zones/db.nyc3.example.comsudo named-checkzone 192.168.in-addr.arpa /etc/named/zones/db.192.168sudo systemctl start namedsudo systemctl enable named]]></content>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS bind搭建]]></title>
    <url>%2F2019%2F01%2F05%2FDNS-bind%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[bind centos7 搭建【图片预留】 首先安装bind rpm 1[root@centos ~]# yum install -y bind bind-chroot bind-utils 【图片预留】 拷贝配置文件模板 1234567891011121314[root@centos ~]# cp -rv /usr/share/doc/bind-9.9.4/sample/etc/* /var/named/chroot/etc/&quot;/usr/share/doc/bind-9.9.4/sample/etc/named.conf&quot; -&gt; &quot;/var/named/chroot/etc/named.conf&quot;&quot;/usr/share/doc/bind-9.9.4/sample/etc/named.rfc1912.zones&quot; -&gt; &quot;/var/named/chroot/etc/named.rfc1912.zones&quot;[root@centos ~]# cp -rv /usr/share/doc/bind-9.9.4/sample/var/* /var/named/chroot/var/&quot;/usr/share/doc/bind-9.9.4/sample/var/named/data&quot; -&gt; &quot;/var/named/chroot/var/named/data&quot;&quot;/usr/share/doc/bind-9.9.4/sample/var/named/my.external.zone.db&quot; -&gt; &quot;/var/named/chroot/var/named/my.external.zone.db&quot;&quot;/usr/share/doc/bind-9.9.4/sample/var/named/my.internal.zone.db&quot; -&gt; &quot;/var/named/chroot/var/named/my.internal.zone.db&quot;&quot;/usr/share/doc/bind-9.9.4/sample/var/named/named.ca&quot; -&gt; &quot;/var/named/chroot/var/named/named.ca&quot;&quot;/usr/share/doc/bind-9.9.4/sample/var/named/named.empty&quot; -&gt; &quot;/var/named/chroot/var/named/named.empty&quot;&quot;/usr/share/doc/bind-9.9.4/sample/var/named/named.localhost&quot; -&gt; &quot;/var/named/chroot/var/named/named.localhost&quot;&quot;/usr/share/doc/bind-9.9.4/sample/var/named/named.loopback&quot; -&gt; &quot;/var/named/chroot/var/named/named.loopback&quot;&quot;/usr/share/doc/bind-9.9.4/sample/var/named/slaves&quot; -&gt; &quot;/var/named/chroot/var/named/slaves&quot;&quot;/usr/share/doc/bind-9.9.4/sample/var/named/slaves/my.ddns.internal.zone.db&quot; -&gt; &quot;/var/named/chroot/var/named/slaves/my.ddns.internal.zone.db&quot;&quot;/usr/share/doc/bind-9.9.4/sample/var/named/slaves/my.slave.internal.zone.db&quot; -&gt; &quot;/var/named/chroot/var/named/slaves/my.slave.internal.zone.db&quot; 修改配置文件 123456789101112131415161718192021222324252627282930[root@centos ~]# vim /var/named/chroot/etc/named.conf /* Sample named.conf BIND DNS server &apos;named&apos; configuration file for the Red Hat BIND distribution. See the BIND Administrator&apos;s Reference Manual (ARM) for details about the configuration located in /usr/share/doc/bind-&#123;version&#125;/Bv9ARM.html*/options&#123; // Put files that named is allowed to write in the data/ directory: directory &quot;/var/named&quot;; // &quot;Working&quot; directory /* Specify listenning interfaces. You can use list of addresses (&apos;;&apos; is delimiter) or keywords &quot;any&quot;/&quot;none&quot; */ //listen-on port 53 &#123; any; &#125;; listen-on port 53 &#123; 127.0.0.1; &#125;; //listen-on-v6 port 53 &#123; any; &#125;; listen-on-v6 port 53 &#123; ::1; &#125;;&#125;;zone &quot;linuxcast.net&quot; &#123; type master; file &quot;linuxcast.net.zone&quot;; /* bind会自动到/var/named/chroot/var/named/中找linuxcast.net.zone文件 */&#125;; 123456789101112131415161718[root@centos ~]# cp -rv /var/named/chroot/var/named/named.localhost /var/named/chroot/var/named/linuxcast.net.zone&quot;/var/named/chroot/var/named/named.localhost&quot; -&gt; &quot;/var/named/chroot/var/named/linuxcast.net.zone&quot;[root@centos ~]# vim /var/named/chroot/var/named/linuxcast.net.zone$TTL 1D@ IN SOA @ rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS @ A 127.0.0.1 AAAA ::1www IN A 192.168.56.101mail IN A 192.168.56.101 IN MX 10 mail.linuxcast.net. 启动bind服务 1[root@centos ~]# systemctl start named.service]]></content>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS基础及域名系统架构 [LinuxCast视频教程]]]></title>
    <url>%2F2019%2F01%2F04%2FDNS%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%9F%9F%E5%90%8D%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84-LinuxCast%E8%A7%86%E9%A2%91%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[域名和IP地址是一一对应的，方便我们记忆。 DNS服务 参考： 【1】https://www.youtube.com/channel/UCNxDA1Ip2e_SKsvhKddfQ1w 【2】https://zh.wikipedia.org/wiki/%E5%9F%9F%E5%90%8D]]></content>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker-5 docker compose]]></title>
    <url>%2F2018%2F12%2F18%2Fdocker-5-docker-compose%2F</url>
    <content type="text"><![CDATA[搭建wordpress123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[vagrant@docker-node1 ~]$ docker run -d --name mysql -v mysql-data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=workpress mysql:5.7.24[vagrant@docker-node1 ~]$ docker run -d --name wordpress -e WORDPRESS_DB_HOST=mysql:3306 --link mysql -p 80:80 wordpressdocker-compose.yml示例version: &apos;3&apos;services: wordpress: image: wordpress ports: - 80:80 environment: WORDPRESS_DB_HOST: mysql WORDPRESS_DB_PASSWORD: root networks: - my-bridge mysql: image: mysql:5.7.24 environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: wordpress volumes: - mysql-data:/var/lib/mysql networks: - my-bridgevolumes: mysql-data:networks: my-bridge: driver: bridge创建docker组合服务[vagrant@docker-node1 ~]$ docker-compose -f docker-compose.yml up -d清除环境[vagrant@docker-node1 ~]$ docker-compose downDockerfile示例[vagrant@docker-node1 ~]$ cat docker-compose-build.yml version: &quot;3&quot;services: hello-docker: build: context: . dockerfile: Dockerfile[vagrant@docker-node1 ~]$ cat Dockerfile FROM centosRUN yum install vim -y[vagrant@docker-node1 ~]$ docker-compose -f docker-compose-build.yml up Load balancing示例lb: image: dockercloud/haproxy links: - webserver ports: - 80:80 volumes: - /var/run/docker.sock:/var/run/docker.sock Docker Compose如果搭建的APP有多个Container组成，部署APP的时候会非常繁琐。 1、要从Dockerfile build image或者Dockerhub拉取image 2、要创建多个Container 3、要管理这些Container（启动停止删除） Docker Compose –&gt; 批处理 Docker Compose是一个工具 这个工具可以通过一个yml文件定义多容器的docker应用 通过一条命令就可以根据yml文件的定义去创建或者管理这多个容器 docker-compose.yml1三大概念：services、networks、volumes demo 12345678910111213141516171819202122一个service代表一个Container，这个Container可以从dockerhub的image来创建，或者自己build出来的images来创建 service的启动类似docker run，我们可以给其指定network和volume，所以可以给service指定network和volume的引用services: db: image: postgres:9.4 volume: - &quot;db-data:/var/lib/postgresql/data&quot; networks: - back-tier就像：docker run -d --network back-tier -v db-data:/var/lib/postgresql/data postgres:9.4service: worker: build: ./worker #指定Docker的location links: - db - redis networks: - back-tier 安装docker composeLinux 12345sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.23.1/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-composedocker-compose --version]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker-4 docker data persistence]]></title>
    <url>%2F2018%2F12%2F17%2Fdocker-4-docker-data-persistence%2F</url>
    <content type="text"><![CDATA[Docker的持久化存储和数据共享 基于本地文件系统的volume。可以在执行docker create或docker run时，通过-v参数将主机的目录作为容器的数据卷。这部分功能便是基于本地文件系统的volume管理。 基于plugin的volume，支持第三方的存储方案，比如NAS，aws。 volume的类型 受管理的data volume，由docker后台自动创建。 绑定挂载的volume，具体挂载位置可以由用户指定。 data volume启动mysql Container，会在宿主机/var/lib/docker/volumes/目录创建持久化目录。在mysql的Dockerfile中有VOLUME /var/lib/mysql，对应的是Container里面的路径。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667[vagrant@docker-node1 ~]$ sudo docker run -d --name mysql1 -e MYSQL_ALLOW_EMPTY_PASSWORD=true mysql9f423f85765546cb5f2f748b99e221c371794e9ae2ee910aabc685462506a23e[vagrant@docker-node1 ~]$ docker volume lsDRIVER VOLUME NAMElocal 5625ef5f9b225dfab10577dbf0521c4244e6ebc60bc1951befac708b196082db[vagrant@docker-node1 ~]$ docker volume inspect 5625ef5f9b225dfab10577dbf0521c4244e6ebc60bc1951befac708b196082db[ &#123; &quot;CreatedAt&quot;: &quot;2018-12-17T16:21:09Z&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: null, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/5625ef5f9b225dfab10577dbf0521c4244e6ebc60bc1951befac708b196082db/_data&quot;, &quot;Name&quot;: &quot;5625ef5f9b225dfab10577dbf0521c4244e6ebc60bc1951befac708b196082db&quot;, &quot;Options&quot;: null, &quot;Scope&quot;: &quot;local&quot; &#125;][vagrant@docker-node1 ~]$ sudo docker run -d --name mysql2 -e MYSQL_ALLOW_EMPTY_PASSWORD=true mysql12dc76117789421e6d9fbe43b6ceeb38791b94848baa0506a1f90bbccd9050fc[vagrant@docker-node1 ~]$ docker volume lsDRIVER VOLUME NAMElocal 595c8cee9d6483ae8a4e2006e18103298bf02f940ef05d43261071af74aa6a42local 5625ef5f9b225dfab10577dbf0521c4244e6ebc60bc1951befac708b196082db[vagrant@docker-node1 ~]$ docker volume inspect 595c8cee9d6483ae8a4e2006e18103298bf02f940ef05d43261071af74aa6a42[ &#123; &quot;CreatedAt&quot;: &quot;2018-12-17T16:24:15Z&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: null, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/595c8cee9d6483ae8a4e2006e18103298bf02f940ef05d43261071af74aa6a42/_data&quot;, &quot;Name&quot;: &quot;595c8cee9d6483ae8a4e2006e18103298bf02f940ef05d43261071af74aa6a42&quot;, &quot;Options&quot;: null, &quot;Scope&quot;: &quot;local&quot; &#125;]以上的方式volume不能很好的区分volume对应哪个应用，可以在启动Container时指定-v参数。[vagrant@docker-node1 ~]$ sudo docker run -d -v mysql:/var/lib/mysql --name mysql1 -e MYSQL_ALLOW_EMPTY_PASSWORD=true mysql6b768f033c3f2c17c2da569ed3a9d4490ac7f3bd17e551a24e0fd7c9f7522ed2[vagrant@docker-node1 ~]$ docker volume lsDRIVER VOLUME NAMElocal mysql创建mysql1 Container，并在Container中的mysql创建docker database，然后停止并删除Container，这时mysql的volume依然存在。[vagrant@docker-node1 ~]$ docker exec -it mysql1 /bin/bashroot@6b768f033c3f:/# mysql -urootmysql&gt; create database docker;mysql&gt; exitroot@6b768f033c3f:/# exit[vagrant@docker-node1 ~]$ docker stop mysql1[vagrant@docker-node1 ~]$ docker rm $(docker ps -qa)创建mysql2 Container，并挂载之前名为mysql的volume，发现docker database依然存在，说明volume起作用了。[vagrant@docker-node1 ~]$ sudo docker run -d -v mysql:/var/lib/mysql --name mysql2 -e MYSQL_ALLOW_EMPTY_PASSWORD=true mysqlf574294e26818667095b78a7141e8eb5309b7aa27d4b027e6811e4cca3368fdf[vagrant@docker-node1 ~]$ docker exec -it mysql2 /bin/bashroot@f574294e2681:/# mysql -u rootmysql&gt; show databases;+--------------------+| Database |+--------------------+| docker || information_schema || mysql || performance_schema || sys |+--------------------+5 rows in set (0.05 sec) Bind Moutingdata volume方式需要在Dockerfile中定义VOLUME，bind mouting方式则不需要。在docker run时，指定宿主机目录与Container内目录的对应关系即可（docker run -v /home/aaa:/root/aaa）。]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker-3 docker network]]></title>
    <url>%2F2018%2F12%2F16%2Fdocker-3-docker-network%2F</url>
    <content type="text"><![CDATA[学习视频地址：https://coding.imooc.com/class/189.html 123456单机 Bridge Network Host Network None 多机 Overlay Network 抓包工具 1https://www.wireshark.org Vagrant创建两台主机12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152yujiangdeMBP-13:docker-network yujiang$ cat Vagrantfile # -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.require_version &quot;&gt;= 1.6.0&quot;boxes = [ &#123; :name =&gt; &quot;docker-node1&quot;, :eth1 =&gt; &quot;192.168.56.61&quot;, :mem =&gt; &quot;1024&quot;, :cpu =&gt; &quot;1&quot; &#125;, &#123; :name =&gt; &quot;docker-node2&quot;, :eth1 =&gt; &quot;192.168.56.62&quot;, :mem =&gt; &quot;1024&quot;, :cpu =&gt; &quot;1&quot; &#125;,]Vagrant.configure(2) do |config| config.vm.box = &quot;centos/centos7&quot; boxes.each do |opts| config.vm.define opts[:name] do |config| config.vm.hostname = opts[:name] config.vm.provider &quot;vmware_fusion&quot; do |v| v.vmx[&quot;memsize&quot;] = opts[:mem] v.vmx[&quot;numvcpus&quot;] = opts[:cpu] end config.vm.provider &quot;virtualbox&quot; do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--memory&quot;, opts[:mem]] v.customize [&quot;modifyvm&quot;, :id, &quot;--cpus&quot;, opts[:cpu]] end config.vm.network :private_network, ip:opts[:eth1] end end config.vm.provision &quot;shell&quot;, inline: &lt;&lt;-SHELL sudo yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine sudo yum install -y yum-utils device-mapper-persistent-data lvm2 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce -y sudo systemctl start docker sudo systemctl enable docker sudo groupadd docker sudo gpasswd -a vagrant docker SHELL # 问题 mount: unnown filesystem type &apos;vboxsf&apos; #config.vm.synced_folder &quot;./share&quot;, &quot;/Users/yujiang/Vagrant/docker-network&quot; #config.vm.provision &quot;shell&quot;, privileged: true, path: &quot;./setup.sh&quot;end linux network namespacehttp://cizixs.com/2017/02/10/network-virtualization-network-namespace/ https://tonybai.com/2017/01/11/understanding-linux-network-namespace-for-docker-network/ 1234[vagrant@docker-node1 ~]$ docker run -d --name test1 busybox /bin/sh -c &quot;while true;do sleep 3600; done&quot;[vagrant@docker-node1 ~]$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf9a344634aab busybox &quot;/bin/sh -c &apos;while t…&quot; 11 seconds ago Up 10 seconds test1 查看宿主机的network namespace 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879[vagrant@docker-node1 ~]$ sudo ip netns list[vagrant@docker-node1 ~]$ sudo ip netns add test1[vagrant@docker-node1 ~]$ sudo ip netns add test2[vagrant@docker-node1 ~]$ sudo ip netns listtest2test1[vagrant@docker-node1 ~]$ sudo ip netns exec test1 ip a1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00[vagrant@docker-node1 ~]$ sudo ip netns exec test1 ip link set dev lo up[vagrant@docker-node1 ~]$ sudo ip netns exec test1 ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever[vagrant@docker-node1 ~]$ sudo ip link add veth-test1 type veth peer name veth-test2[vagrant@docker-node1 ~]$ ip link5: veth-test2@veth-test1: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 96:92:53:64:58:17 brd ff:ff:ff:ff:ff:ff6: veth-test1@veth-test2: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 3e:20:9c:b8:9c:2b brd ff:ff:ff:ff:ff:ff[vagrant@docker-node1 ~]$ sudo ip link set veth-test1 netns test1[vagrant@docker-node1 ~]$ sudo ip netns exec test1 ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:006: veth-test1@if5: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 3e:20:9c:b8:9c:2b brd ff:ff:ff:ff:ff:ff link-netnsid 0[vagrant@docker-node1 ~]$ ip link5: veth-test2@if6: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 96:92:53:64:58:17 brd ff:ff:ff:ff:ff:ff link-netnsid 0[vagrant@docker-node1 ~]$ sudo ip link set veth-test2 netns test2[vagrant@docker-node1 ~]$ ip link5: veth-test2@if6也不见了[vagrant@docker-node1 ~]$ sudo ip netns exec test2 ip link1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:005: veth-test2@if6: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 96:92:53:64:58:17 brd ff:ff:ff:ff:ff:ff link-netnsid 0分配IP地址[vagrant@docker-node1 ~]$ sudo ip netns exec test1 ip addr add 192.168.1.2/24 dev veth-test1[vagrant@docker-node1 ~]$ sudo ip netns exec test2 ip addr add 192.168.1.3/24 dev veth-test2[vagrant@docker-node1 ~]$ sudo ip netns exec test1 ip link set dev veth-test1 up[vagrant@docker-node1 ~]$ sudo ip netns exec test2 ip link set dev veth-test2 up[vagrant@docker-node1 ~]$ sudo ip netns exec test1 ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever6: veth-test1@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 3e:20:9c:b8:9c:2b brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet 192.168.1.2/24 scope global veth-test1 valid_lft forever preferred_lft forever inet6 fe80::3c20:9cff:feb8:9c2b/64 scope link valid_lft forever preferred_lft forever[vagrant@docker-node1 ~]$ sudo ip netns exec test2 ip a1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:005: veth-test2@if6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 96:92:53:64:58:17 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.1.3/24 scope global veth-test2 valid_lft forever preferred_lft forever inet6 fe80::9492:53ff:fe64:5817/64 scope link valid_lft forever preferred_lft forever互ping[vagrant@docker-node1 ~]$ sudo ip netns exec test1 ping 192.168.1.3PING 192.168.1.3 (192.168.1.3) 56(84) bytes of data.64 bytes from 192.168.1.3: icmp_seq=1 ttl=64 time=0.051 ms[vagrant@docker-node1 ~]$ sudo ip netns exec test2 ping 192.168.1.2PING 192.168.1.2 (192.168.1.2) 56(84) bytes of data.64 bytes from 192.168.1.2: icmp_seq=1 ttl=64 time=0.035 ms Bridge01234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677[vagrant@docker-node1 ~]$ docker network lsNETWORK ID NAME DRIVER SCOPE36c9fe545daf bridge bridge local39385556d8cd host host local7828c2433efd none null local[vagrant@docker-node1 ~]$ docker network inspect 36c9fe545daf[ &#123; &quot;Name&quot;: &quot;bridge&quot;, &quot;Id&quot;: &quot;36c9fe545daf2ab0917fce9b1a8edee5ebdd4cb375b1083439212506f3fe179c&quot;, &quot;Created&quot;: &quot;2018-12-15T19:24:36.985544361Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: null, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; # test1这个container连到了bridge这个网络 &quot;f6c6d9b55defc79cf5a39feff1dde1da336f3aa29e68a7577428aefab0196f6b&quot;: &#123; &quot;Name&quot;: &quot;test1&quot;, &quot;EndpointID&quot;: &quot;12a9fa5786c42c486e46bb058e44eb811f8d293ca4a89e1203df134c393d0254&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;IPv4Address&quot;: &quot;172.17.0.2/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123; &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;, &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;, &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot; &#125;, &quot;Labels&quot;: &#123;&#125; &#125;][vagrant@docker-node1 ~]$ ip a3: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:42:8b:4b:10 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:42ff:fe8b:4b10/64 scope link valid_lft forever preferred_lft forever8: veth2657408@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default link/ether a2:0b:d6:4a:77:e5 brd ff:ff:ff:ff:ff:ff link-netnsid 2 inet6 fe80::a00b:d6ff:fe4a:77e5/64 scope link valid_lft forever preferred_lft forever[vagrant@docker-node1 ~]$ docker exec test1 ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever7: eth0@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft foreverveth2657408负责连到docker0，test1 container中有一个接口，test1 container中的eth0与外面的veth2657408是一对。[vagrant@docker-node1 ~]$ sudo yum install bridge-utils -y[vagrant@docker-node1 ~]$ brctl showbridge name bridge id STP enabled interfacesdocker0 8000.0242428b4b10 no veth2657408同一台宿主机上的container是通过docker0相互通信的。docker0通过NAT来使container访问Internet。 自建Bridge123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194[vagrant@docker-node1 ~]$ docker network create -d bridge my-bridge7cbd844d6e4cf262eba12277a3e8b4c475a1fb6aa73b9ea312d113d6c519284a[vagrant@docker-node1 ~]$ docker network lsNETWORK ID NAME DRIVER SCOPE36c9fe545daf bridge bridge local39385556d8cd host host local7cbd844d6e4c my-bridge bridge local7828c2433efd none null local[vagrant@docker-node1 ~]$ brctl showbridge name bridge id STP enabled interfacesbr-7cbd844d6e4c 8000.02425dd71cb3 no docker0 8000.0242428b4b10 no veth2657408 veth7687c87[vagrant@docker-node1 ~]$ docker run -d --name test3 --network my-bridge busybox /bin/sh -c &quot;while true; do sleep 3600; done&quot;[vagrant@docker-node1 ~]$ brctl showbridge name bridge id STP enabled interfacesbr-7cbd844d6e4c 8000.02425dd71cb3 no vetha530f6bdocker0 8000.0242428b4b10 no veth2657408 veth7687c87[vagrant@docker-node1 ~]$ docker network lsNETWORK ID NAME DRIVER SCOPE36c9fe545daf bridge bridge local39385556d8cd host host local7cbd844d6e4c my-bridge bridge local7828c2433efd none null local[vagrant@docker-node1 ~]$ docker network inspect 7cbd844d6e4c[ &#123; &quot;Name&quot;: &quot;my-bridge&quot;, &quot;Id&quot;: &quot;7cbd844d6e4cf262eba12277a3e8b4c475a1fb6aa73b9ea312d113d6c519284a&quot;, &quot;Created&quot;: &quot;2018-12-16T13:17:25.765613424Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.18.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.18.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; &quot;5adef860247e056b8f8d2fab89892e10573daff5ea598482f53cefe3152700e0&quot;: &#123; &quot;Name&quot;: &quot;test3&quot;, &quot;EndpointID&quot;: &quot;19f5d575ad3ff8602c7342f036f024e88c4b97c33a68efa206bd810128b82ddc&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;, &quot;IPv4Address&quot;: &quot;172.18.0.2/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123;&#125;, &quot;Labels&quot;: &#123;&#125; &#125;]连接到test2上[vagrant@docker-node1 ~]$ docker network connect my-bridge test2[vagrant@docker-node1 ~]$ docker network inspect 7cbd844d6e4c[ &#123; &quot;Name&quot;: &quot;my-bridge&quot;, &quot;Id&quot;: &quot;7cbd844d6e4cf262eba12277a3e8b4c475a1fb6aa73b9ea312d113d6c519284a&quot;, &quot;Created&quot;: &quot;2018-12-16T13:17:25.765613424Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.18.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.18.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; &quot;4b7db28e2a457ea6102fd8f06b512041e502dcf54c6d7c362fc5e39f7a0cdf23&quot;: &#123; &quot;Name&quot;: &quot;test2&quot;, &quot;EndpointID&quot;: &quot;cf4a07fff8a8684cd179ae747630ceeef32ebd38b2fb5eb9db4d2d35759970fb&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:03&quot;, &quot;IPv4Address&quot;: &quot;172.18.0.3/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125;, &quot;5adef860247e056b8f8d2fab89892e10573daff5ea598482f53cefe3152700e0&quot;: &#123; &quot;Name&quot;: &quot;test3&quot;, &quot;EndpointID&quot;: &quot;19f5d575ad3ff8602c7342f036f024e88c4b97c33a68efa206bd810128b82ddc&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;, &quot;IPv4Address&quot;: &quot;172.18.0.2/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123;&#125;, &quot;Labels&quot;: &#123;&#125; &#125;]查看默认的bridge，都有test2，说明test2 Container及连到了默认的bridge又连到了my-bridge。[vagrant@docker-node1 ~]$ docker network lsNETWORK ID NAME DRIVER SCOPE36c9fe545daf bridge bridge local39385556d8cd host host local7cbd844d6e4c my-bridge bridge local7828c2433efd none null local[vagrant@docker-node1 ~]$ docker network inspect 36c9fe545daf[ &#123; &quot;Name&quot;: &quot;bridge&quot;, &quot;Id&quot;: &quot;36c9fe545daf2ab0917fce9b1a8edee5ebdd4cb375b1083439212506f3fe179c&quot;, &quot;Created&quot;: &quot;2018-12-15T19:24:36.985544361Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: null, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; &quot;4b7db28e2a457ea6102fd8f06b512041e502dcf54c6d7c362fc5e39f7a0cdf23&quot;: &#123; &quot;Name&quot;: &quot;test2&quot;, &quot;EndpointID&quot;: &quot;4ba2bbe1ee63a797b528e25533fd09dda8921cf5f688d4a5a24db4efed7410e2&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:03&quot;, &quot;IPv4Address&quot;: &quot;172.17.0.3/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125;, &quot;f6c6d9b55defc79cf5a39feff1dde1da336f3aa29e68a7577428aefab0196f6b&quot;: &#123; &quot;Name&quot;: &quot;test1&quot;, &quot;EndpointID&quot;: &quot;12a9fa5786c42c486e46bb058e44eb811f8d293ca4a89e1203df134c393d0254&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;IPv4Address&quot;: &quot;172.17.0.2/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123; &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;, &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;, &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot; &#125;, &quot;Labels&quot;: &#123;&#125; &#125;][vagrant@docker-node1 ~]$ docker exec test2 ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever9: eth0@if10: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever14: eth1@if15: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:12:00:03 brd ff:ff:ff:ff:ff:ff inet 172.18.0.3/16 brd 172.18.255.255 scope global eth1 valid_lft forever preferred_lft forever[vagrant@docker-node1 ~]$ docker exec -it test3 /bin/sh/ # ping 172.18.0.3PING 172.18.0.3 (172.18.0.3): 56 data bytes64 bytes from 172.18.0.3: seq=0 ttl=64 time=0.727 ms/ # ping test2PING test2 (172.18.0.3): 56 data bytes64 bytes from 172.18.0.3: seq=0 ttl=64 time=0.058 msContainer连接到用户自己创建的bridge上，默认是link好的。所以使用Container name也能通。如果连接到docker0则不是。 Container之间link1234567891011121314151617181920212223242526272829[vagrant@docker-node1 ~]$ docker run -d --name test1 busybox /bin/sh -c &quot;while true;do sleep 3600; done&quot;[vagrant@docker-node1 ~]$ sudo docker run -d --name test2 --link test1 busybox /bin/sh -c &quot;while true; do sleep 3600; done&quot;[vagrant@docker-node1 ~]$ docker exec test1 ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever7: eth0@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever[vagrant@docker-node1 ~]$ docker exec test2 ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever9: eth0@if10: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever[vagrant@docker-node1 ~]$ docker exec -it test2 /bin/sh/ # ping 172.17.0.2PING 172.17.0.2 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.419 ms/ # ping test1 (ping Container名字也是通的)PING test1 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.082 ms docker 端口映射12345678910111213141516171819202122232425262728293031docker run --name nginx -d -p &lt;容器中端口&gt;:&lt;宿主机端口&gt; nginx[vagrant@docker-node1 ~]$ docker run --name nginx -d -p 80:80 nginx[vagrant@docker-node1 ~]$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES82d01fa9a547 nginx &quot;nginx -g &apos;daemon of…&quot; 14 seconds ago Up 13 seconds 0.0.0.0:80-&gt;80/tcp nginx[vagrant@docker-node1 ~]$ curl 127.0.0.1&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; HOST与NONE网络NONE，只能使用docker exec来访问Container。 1234567891011121314151617181920212223242526272829303132333435363738394041[vagrant@docker-node1 ~]$ docker run -d --name test1 --network none busybox /bin/sh -c &quot;while true;do sleep 3600;done&quot;005cd306dc9e51a50a9bff5a2f2a9c54297c2dfe786443787d54df1d0a10e8bb[vagrant@docker-node1 ~]$ docker network inspect none[ &#123; &quot;Name&quot;: &quot;none&quot;, &quot;Id&quot;: &quot;7828c2433efde412a4e8d4ffa9a92e121fb6391a37687fda9e2b09f8304ff12b&quot;, &quot;Created&quot;: &quot;2018-12-15T18:17:04.614441655Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;null&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: null, &quot;Config&quot;: [] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; &quot;005cd306dc9e51a50a9bff5a2f2a9c54297c2dfe786443787d54df1d0a10e8bb&quot;: &#123; &quot;Name&quot;: &quot;test1&quot;, &quot;EndpointID&quot;: &quot;46a077f2a809d0073a6c9251897e4dd1e47dd785c6f13f32056dfb13e32f0ded&quot;, &quot;MacAddress&quot;: &quot;&quot;, &quot;IPv4Address&quot;: &quot;&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123;&#125;, &quot;Labels&quot;: &#123;&#125; &#125;][vagrant@docker-node1 ~]$ docker exec test1 ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever HOST，与宿主机IP相同。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[vagrant@docker-node1 ~]$ docker run -d --name test1 --network host busybox /bin/sh -c &quot;while true;do sleep 3600;done&quot;0bf2514b4ce329a56491409d6470b0bc8374824be805bec6c5e387ecbb10713f[vagrant@docker-node1 ~]$ docker network inspect host[ &#123; &quot;Name&quot;: &quot;host&quot;, &quot;Id&quot;: &quot;39385556d8cda0864b8911f06c7c0b79d1858f2d16b6b31fd34e5e87cb8ac08b&quot;, &quot;Created&quot;: &quot;2018-12-15T18:17:04.625179187Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;host&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: null, &quot;Config&quot;: [] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; &quot;0bf2514b4ce329a56491409d6470b0bc8374824be805bec6c5e387ecbb10713f&quot;: &#123; &quot;Name&quot;: &quot;test1&quot;, &quot;EndpointID&quot;: &quot;adfa701db6a153c3fafd4b9fb4d10b35b47872afd9f7595f0b5ee7b9089d59df&quot;, &quot;MacAddress&quot;: &quot;&quot;, &quot;IPv4Address&quot;: &quot;&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123;&#125;, &quot;Labels&quot;: &#123;&#125; &#125;][vagrant@docker-node1 ~]$ docker exec test1 ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast qlen 1000 link/ether 52:54:00:47:2c:0d brd ff:ff:ff:ff:ff:ff inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0 valid_lft 84405sec preferred_lft 84405sec inet6 fe80::5054:ff:fe47:2c0d/64 scope link valid_lft forever preferred_lft forever3: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue link/ether 02:42:20:4d:38:8a brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:20ff:fe4d:388a/64 scope link valid_lft forever preferred_lft forever4: br-7cbd844d6e4c: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue link/ether 02:42:be:6a:02:b8 brd ff:ff:ff:ff:ff:ff inet 172.18.0.1/16 brd 172.18.255.255 scope global br-7cbd844d6e4c valid_lft forever preferred_lft forever5: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast qlen 1000 link/ether 08:00:27:2a:89:41 brd ff:ff:ff:ff:ff:ff inet 192.168.56.61/24 brd 192.168.56.255 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::a00:27ff:fe2a:8941/64 scope link valid_lft forever preferred_lft forever7: vethd963769@if6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue master docker0 link/ether ee:f6:51:e6:91:e3 brd ff:ff:ff:ff:ff:ff inet6 fe80::ecf6:51ff:fee6:91e3/64 scope link valid_lft forever preferred_lft forever 多机通信 Overlay参考资料：https://github.com/docker/labs/blob/master/networking/concepts/06-overlay-networks.md 12345Overlay +VXLAN(VXLAN，https://cizixs.com/2017/09/25/vxlan-protocol-introduction/) +隧道(Ethernet、IPV4、UDP) 需要借用etcd，记录多机上的Container没有重复IP。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197搭建etcd集群[vagrant@docker-node1 ~]$ sudo yum install wget -y &amp;&amp; wget https://github.com/etcd-io/etcd/releases/download/v3.3.10/etcd-v3.3.10-linux-amd64.tar.gz &amp;&amp; tar zxvf etcd-v3.3.10-linux-amd64.tar.gz &amp;&amp; cd etcd-v3.3.10-linux-amd64/[vagrant@docker-node2 ~]$ sudo yum install wget -y &amp;&amp; wget https://github.com/etcd-io/etcd/releases/download/v3.3.10/etcd-v3.3.10-linux-amd64.tar.gz &amp;&amp; tar zxvf etcd-v3.3.10-linux-amd64.tar.gz &amp;&amp; cd etcd-v3.3.10-linux-amd64/[vagrant@docker-node1 etcd-v3.3.10-linux-amd64]$ nohup ./etcd --name docker-node1 \--initial-advertise-peer-urls http://192.168.56.61:2380 \--listen-peer-urls http://192.168.56.61:2380 \--listen-client-urls http://192.168.56.61:2379,http://127.0.0.1:2379 \--advertise-client-urls http://192.168.56.61:2379 \--initial-cluster-token etcd-cluster \--initial-cluster docker-node1=http://192.168.56.61:2380,docker-node2=http://192.168.56.62:2380 \--initial-cluster-state new&amp;[vagrant@docker-node2 etcd-v3.3.10-linux-amd64]$ nohup ./etcd --name docker-node2 \--initial-advertise-peer-urls http://192.168.56.62:2380 \--listen-peer-urls http://192.168.56.62:2380 \--listen-client-urls http://192.168.56.62:2379,http://127.0.0.1:2379 \--advertise-client-urls http://192.168.56.62:2379 \--initial-cluster-token etcd-cluster \--initial-cluster docker-node1=http://192.168.56.61:2380,docker-node2=http://192.168.56.62:2380 \--initial-cluster-state new&amp;[vagrant@docker-node1 etcd-v3.3.10-linux-amd64]$ ./etcdctl cluster-healthmember 14192bed1b668a6 is healthy: got healthy result from http://192.168.56.61:2379member 80c395b734da48f6 is healthy: got healthy result from http://192.168.56.62:2379cluster is healthy[vagrant@docker-node2 etcd-v3.3.10-linux-amd64]$ ./etcdctl cluster-healthmember 14192bed1b668a6 is healthy: got healthy result from http://192.168.56.61:2379member 80c395b734da48f6 is healthy: got healthy result from http://192.168.56.62:2379cluster is healthy[vagrant@docker-node1 etcd-v3.3.10-linux-amd64]$ sudo service docker stop[vagrant@docker-node1 etcd-v3.3.10-linux-amd64]$ sudo /usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --cluster-store=etcd://192.168.56.61:2379 --cluster-advertise=192.168.56.61:2375&amp;[vagrant@docker-node1 etcd-v3.3.10-linux-amd64]$ sudo docker version[vagrant@docker-node2 etcd-v3.3.10-linux-amd64]$ sudo service docker stop[vagrant@docker-node2 etcd-v3.3.10-linux-amd64]$ sudo /usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --cluster-store=etcd://192.168.56.62:2379 --cluster-advertise=192.168.56.62:2375&amp;[vagrant@docker-node2 etcd-v3.3.10-linux-amd64]$ sudo docker version[vagrant@docker-node1 ~]$ sudo docker network lsNETWORK ID NAME DRIVER SCOPE6a9dcf7a7d0a bridge bridge local39385556d8cd host host local7828c2433efd none null local[vagrant@docker-node2 ~]$ sudo docker network lsNETWORK ID NAME DRIVER SCOPEe93dec4d4089 bridge bridge local199f9936292d host host local4a55973792c6 none null local在docker-node1机器上创建overlay网络，名字为demo。会自动同步到docker-node2机器上，这个就是etcd做的。[vagrant@docker-node1 ~]$ sudo docker network create -d overlay demod0cf99b69499e384b52854e852b0c734ebae3f039fdacd762e5347de18454fda[vagrant@docker-node1 ~]$ sudo docker network lsNETWORK ID NAME DRIVER SCOPE6a9dcf7a7d0a bridge bridge locald0cf99b69499 demo overlay global39385556d8cd host host local7828c2433efd none null local[vagrant@docker-node2 ~]$ sudo docker network lsNETWORK ID NAME DRIVER SCOPEe93dec4d4089 bridge bridge locald0cf99b69499 demo overlay global199f9936292d host host local4a55973792c6 none null local[vagrant@docker-node1 etcd-v3.3.10-linux-amd64]$ ./etcdctl ls /docker/network/v1.0/network/docker/network/v1.0/network/d0cf99b69499e384b52854e852b0c734ebae3f039fdacd762e5347de18454fda[vagrant@docker-node1 ~]$ docker network inspect demo[ &#123; &quot;Name&quot;: &quot;demo&quot;, &quot;Id&quot;: &quot;d0cf99b69499e384b52854e852b0c734ebae3f039fdacd762e5347de18454fda&quot;, &quot;Created&quot;: &quot;2018-12-17T15:00:39.903457425Z&quot;, &quot;Scope&quot;: &quot;global&quot;, &quot;Driver&quot;: &quot;overlay&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;10.0.0.0/24&quot;, &quot;Gateway&quot;: &quot;10.0.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123;&#125;, &quot;Options&quot;: &#123;&#125;, &quot;Labels&quot;: &#123;&#125; &#125;]创建Container测试，在不同的机器上创建相同名字的Container会有报错，当在第二个机器上创建test1 Container时，会去etcd中查找。[vagrant@docker-node1 ~]$ sudo docker run -d --name test1 --net demo busybox sh -c &quot;while true; do sleep 3600; done&quot;fabfd6aab82b692d0d59ba5ca61d0315fced0fa459510ebe1d17fe8eacd71882[vagrant@docker-node2 ~]$ sudo docker run -d --name test1 --net demo busybox sh -c &quot;while true; do sleep 3600; done&quot;docker: Error response from daemon: Conflict. The container name &quot;/test1&quot; is already in use by container &quot;74d40e80850ca8b95f6c43b7c78f755811e71892116428b1108a1f62553dbf7f&quot;. You have to remove (or rename) that container to be able to reuse that name.See &apos;docker run --help&apos;.[vagrant@docker-node2 ~]$ sudo docker run -d --name test2 --net demo busybox sh -c &quot;while true; do sleep 3600; done&quot;09ab0bd6131b7bebbadf7183a4ab6179fd4fea2fe1322c38437e2043bf7b0b43再次查看网络信息[vagrant@docker-node1 ~]$ docker network inspect demo[ &#123; &quot;Name&quot;: &quot;demo&quot;, &quot;Id&quot;: &quot;d0cf99b69499e384b52854e852b0c734ebae3f039fdacd762e5347de18454fda&quot;, &quot;Created&quot;: &quot;2018-12-17T15:00:39.903457425Z&quot;, &quot;Scope&quot;: &quot;global&quot;, &quot;Driver&quot;: &quot;overlay&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;10.0.0.0/24&quot;, &quot;Gateway&quot;: &quot;10.0.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; &quot;ep-596086a2ecffec95d85529df2e24016ac62c8448466f60454e61567482faf02f&quot;: &#123; &quot;Name&quot;: &quot;test2&quot;, &quot;EndpointID&quot;: &quot;596086a2ecffec95d85529df2e24016ac62c8448466f60454e61567482faf02f&quot;, &quot;MacAddress&quot;: &quot;02:42:0a:00:00:03&quot;, &quot;IPv4Address&quot;: &quot;10.0.0.3/24&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125;, &quot;fabfd6aab82b692d0d59ba5ca61d0315fced0fa459510ebe1d17fe8eacd71882&quot;: &#123; &quot;Name&quot;: &quot;test1&quot;, &quot;EndpointID&quot;: &quot;36027fb8100351e24db8e5691a70420413a5e02d6f397af3794cb2eabdc71fb8&quot;, &quot;MacAddress&quot;: &quot;02:42:0a:00:00:02&quot;, &quot;IPv4Address&quot;: &quot;10.0.0.2/24&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123;&#125;, &quot;Labels&quot;: &#123;&#125; &#125;][vagrant@docker-node1 ~]$ docker exec test1 ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever12: eth0@if13: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1450 qdisc noqueue link/ether 02:42:0a:00:00:02 brd ff:ff:ff:ff:ff:ff inet 10.0.0.2/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever15: eth1@if16: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:13:00:02 brd ff:ff:ff:ff:ff:ff inet 172.19.0.2/16 brd 172.19.255.255 scope global eth1 valid_lft forever preferred_lft forever[vagrant@docker-node2 ~]$ docker exec test2 ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever7: eth0@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1450 qdisc noqueue link/ether 02:42:0a:00:00:03 brd ff:ff:ff:ff:ff:ff inet 10.0.0.3/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever10: eth1@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff inet 172.18.0.2/16 brd 172.18.255.255 scope global eth1 valid_lft forever preferred_lft forever每个Container中两个接口，这时docker network会多出一个docker_gwbridge，Container内其中一个接口连接的就是docker_gwbridge。[vagrant@docker-node1 ~]$ docker network lsNETWORK ID NAME DRIVER SCOPEd0cf99b69499 demo overlay globalc43108631ed7 docker_gwbridge bridge local[vagrant@docker-node1 ~]$ docker exec test1 ping 10.0.0.3PING 10.0.0.3 (10.0.0.3): 56 data bytes64 bytes from 10.0.0.3: seq=0 ttl=64 time=10.259 ms[vagrant@docker-node1 ~]$ docker exec test1 ping test2PING test2 (10.0.0.3): 56 data bytes64 bytes from 10.0.0.3: seq=0 ttl=64 time=5.188 ms[vagrant@docker-node2 ~]$ docker exec test2 ping 10.0.0.2PING 10.0.0.2 (10.0.0.2): 56 data bytes64 bytes from 10.0.0.2: seq=0 ttl=64 time=8.995 ms[vagrant@docker-node2 ~]$ docker exec test2 ping test1PING test1 (10.0.0.2): 56 data bytes64 bytes from 10.0.0.2: seq=0 ttl=64 time=5.485 ms]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker-2 从零制作Base Image]]></title>
    <url>%2F2018%2F12%2F14%2Fdocker-2-%E4%BB%8E%E9%9B%B6%E5%88%B6%E4%BD%9CBase-Image%2F</url>
    <content type="text"><![CDATA[学习视频地址：https://coding.imooc.com/class/189.html 从零制作Base Image1、编写一个Demo程序 12345678910111213[vagrant@localhost ~]$ sudo yum install glibc-static gcc -y[vagrant@localhost ~]$ cat helloworld.c #include &lt;stdio.h&gt;int main() &#123; printf(&quot;hello world!\n&quot;); return 0;&#125;# 不加-static编译，docker run会报错，为什么？后续再查... ...# standard_init_linux.go:190: exec user process caused &quot;no such file or directory&quot;[vagrant@localhost ~]$ gcc -static helloworld.c -o helloworld 2、编写Dockerfile 1234[vagrant@localhost ~]$ cat Dockerfile FROM scratchADD helloworld /CMD [&quot;/helloworld&quot;] 3、build镜像 12345678910111213141516171819202122232425262728293031[vagrant@localhost ~]$ docker build -t yujiang/helloworld .Sending build context to Docker daemon 19.97kBStep 1/3 : FROM scratch ---&gt; Step 2/3 : ADD helloworld / ---&gt; 2949199fbdb8Step 3/3 : CMD [&quot;/helloworld&quot;] ---&gt; Running in 3af2e910629eRemoving intermediate container 3af2e910629e ---&gt; b898a6498b21Successfully built b898a6498b21Successfully tagged yujiang/helloworld:latest[vagrant@localhost ~]$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEyujiang/helloworld latest b898a6498b21 21 seconds ago 857kB查看分层[vagrant@localhost ~]$ docker history yujiang/helloworldIMAGE CREATED CREATED BY SIZE COMMENTb898a6498b21 About a minute ago /bin/sh -c #(nop) CMD [&quot;/helloworld&quot;] 0B 2949199fbdb8 About a minute ago /bin/sh -c #(nop) ADD file:5e0b91d4866514aa0… 857kB [vagrant@localhost ~]$ ll -htotal 848K-rw-rw-r--. 1 vagrant vagrant 50 Dec 13 16:41 Dockerfile-rwxrwxr-x. 1 vagrant vagrant 837K Dec 13 16:47 helloworld-rw-rw-r--. 1 vagrant vagrant 79 Dec 13 16:11 helloworld.c[vagrant@localhost ~]$ docker run yujiang/helloworldhello world! 使用docker commit制作image当container已经存在时，并对容器中进行了一些变化（如安装了某一个软件）。可以把已经改变的container commit一个新的image，命令如下： 1docker container commit 1、启动一个容器并安装vim 123456[vagrant@localhost ~]$ docker run -it centos[root@b2985a1be234 /]# yum install vim -y[root@b2985a1be234 /]# exit[vagrant@localhost ~]$ docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb2985a1be234 centos &quot;/bin/bash&quot; 3 minutes ago Exited (0) About a minute ago nervous_haslett 2、使用docker commit创建image 123456789101112131415161718[vagrant@localhost ~]$ docker commit b2985a1be234 yujiang/centos-vimsha256:d0a8856e664eb754f9854c0c27a92c7d01623d2087a77269fbd12ba5021e6e13[vagrant@localhost ~]$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEyujiang/centos-vim latest d0a8856e664e 59 seconds ago 327MBcentos latest 1e1148e4cc2c 7 days ago 202MB[vagrant@localhost ~]$ docker history 1e1148e4cc2cIMAGE CREATED CREATED BY SIZE COMMENT1e1148e4cc2c 7 days ago /bin/sh -c #(nop) CMD [&quot;/bin/bash&quot;] 0B &lt;missing&gt; 7 days ago /bin/sh -c #(nop) LABEL org.label-schema.sc… 0B &lt;missing&gt; 7 days ago /bin/sh -c #(nop) ADD file:6f877549795f4798a… 202MB [vagrant@localhost ~]$ docker history d0a8856e664eIMAGE CREATED CREATED BY SIZE COMMENTd0a8856e664e About a minute ago /bin/bash 126MB 1e1148e4cc2c 7 days ago /bin/sh -c #(nop) CMD [&quot;/bin/bash&quot;] 0B &lt;missing&gt; 7 days ago /bin/sh -c #(nop) LABEL org.label-schema.sc… 0B &lt;missing&gt; 7 days ago /bin/sh -c #(nop) ADD file:6f877549795f4798a… 202MB 这样发布image是不安全的，因为其他人不知道你对镜像做了哪些修改。不提倡。 使用docker build制作image1、创建centos-vim目录 1[vagrant@localhost ~]$ mkdir centos-vim &amp;&amp; cd centos-vim 2、编写Dockerfile 123[vagrant@localhost centos-vim]$ vim DockerfileFROM centosRUN yum install vim -y 3、使用docker build创建image 123456789101112131415161718192021222324[vagrant@localhost centos-vim]$ docker build -t yujiang/centos-vim .Sending build context to Docker daemon 2.048kBStep 1/2 : FROM centos ---&gt; 1e1148e4cc2cStep 2/2 : RUN yum install vim -y ---&gt; Running in 8122262ff02a...安装过程省略...Complete!Removing intermediate container 8122262ff02a ---&gt; e00635baf672Successfully built e00635baf672Successfully tagged yujiang/centos-vim:latestStep 1/2 直接引用centos这层image(1e1148e4cc2c)Step 2/2 生成了一个临时的image(8122262ff02a)，在临时的image里面通过yum安装vim，安装完以后临时的image会被remove掉（Removing intermediate container 8122262ff02a），最后根据刚才临时的image commit成为一个新的image。如果build过程中报错，可以使用这个debug(docker run -it 8122262ff02a /bin/bash)[vagrant@localhost centos-vim]$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEyujiang/centos-vim latest e00635baf672 6 minutes ago 327MBcentos latest 1e1148e4cc2c 7 days ago 202MB 推荐使用docker build来构建image。 Dockerfile语法1官方文档：https://docs.docker.com/engine/reference/builder/ FROM 1234FROM scratch # 制作base imageFROM centos # 使用base imageFROM ubuntu:14.04 尽量使用官方的image作为base image LABEL 1234LABEL maintainer=&quot;lnsyyj@xxx.com&quot;LABEL version=&quot;1.0&quot;LABEL description=&quot;Tish is description&quot;定义image的metadata RUN 1234567RUN yum update &amp;&amp; yum install -y vim \ python-devel # 反斜线换行RUN apt-get update &amp;&amp; apt-get install -y perl \ pwgen --no-install-recommends &amp;&amp; rm -rf \ /var/lib/apt/lists/* # 注意清理cacheRUN /bin/bash -c &apos;source $HOME/.bashrc; echo $HOME&apos;为了美观，复杂的RUN请用反斜杠换行，避免无用分层，合并多条命令成一行。 WORKDIR 123456WORKDIR /rootWORKDIR /test # 如果没有会自动创建test目录WORKDIR demoRUN pwd # 输出结果应该是/test/demo用WORKDIR，不要用RUN cd，尽量使用绝对路径！ ADD 和 COPY 12345678910ADD hello /ADD test.tar.gz / # 添加到根目录并解压WORKDIR /rootADD hello test/ # /root/test/helloWORKDIR /rootCOPY hello test/大部分情况，COPY优于ADD！ADD除了COPY还有额外功能(解压)！添加远程文件/目录请使用curl或者wget下载到docker中！ ENV 1234ENV MYSQL_VERSION 5.6 # 设置常量RUN apt-get install -y mysql-server=&quot;$&#123;MYSQL_VERSION&#125;&quot; \ &amp;&amp; rm -rf /var/lib/apt/lists/* # 引用常量尽量使用ENV增加可维护性！ VOLUME 和 EXPOSE 1（存储和网络），后面我们单独讲！ CMD 和 ENTRYPOINT 1后面我们单独讲！ RUN vs CMD vs ENTRYPOINT123RUN：执行命令并创建新的Image LayerCMD：设置容器启动后默认执行的命令和参数ENTRYPOINT：设置容器启动时运行的命令 Shell和Exec格式Shell格式 123RUN apt-get install -y vimCMD echo &quot;hello docker&quot;ENTRYPOINT echo &quot;hello docker&quot; Exec格式 123RUN [ &quot;apt-get&quot;, &quot;install&quot;, &quot;-y&quot;, &quot;vim&quot; ]CMD [ &quot;/bin/echo&quot;, &quot;hello docker&quot; ]ENTRYPOINT [ &quot;/bin/echo&quot;, &quot;hello docker&quot; ] DEMO 12345678910111213141516Dockerfile1：FROM centosENV name DockerENTRYPOINT echo &quot;hello $name&quot;Dockerfile2：FROM centosENV name DockerENTRYPOINT [ &quot;/bin/echo&quot;, &quot;hello $name&quot; ]Dockerfile2会原样输出&quot;hello $name&quot;，不会像Dockerfile1那样输出&quot;hello Docker&quot;。如何使Dockerfile2像Dockerfile1那样输出的？请看Dockerfile3Dockerfile3：FROM centosENV name DockerENTRYPOINT [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo hello $name&quot; ] # 指定-c参数，后面的命令需要放在一个&quot;&quot;中，作为一条命令 CMD 容器启动时默认执行的命令 如果docker run指定了其他命令，CMD命令被忽略 如果定义了多个CMD，只有最后一个会执行 123456FROM centosENV name DockerCMD echo &quot;hello $name&quot;docker run [image]输出？打印&quot;hello Docker&quot;Docker run -it [image] /bin/bash输出？不会打印&quot;hello Docker&quot;，/bin/bash覆盖了CMD ENTRYPOINT 让容器以应用程序或者服务的形式运行 不会被忽略，一定会执行 最佳实践：写一个shell脚本作为entrypoint 12345COPY docker-entrypoint.sh /usr/local/bin/ENTRYPOINT [ &quot;docker-entrypoint.sh&quot; ]EXPOSE 27017CMD [ &quot;mongod&quot; ] 镜像存放公有镜像https://hub.docker.com/ 123456789101112131415161718[vagrant@localhost ~]$ docker build -t lnsyyj/helloworld .[vagrant@localhost ~]$ docker imageslnsyyj/helloworld latest 1a8620e6d6de 46 hours ago 857kB[vagrant@localhost ~]$ docker loginLogin with your Docker ID to push and pull images from Docker Hub. If you don&apos;t have a Docker ID, head over to https://hub.docker.com to create one.Username: lnsyyjPassword: Login Succeeded[vagrant@localhost ~]$ docker push lnsyyj/helloworld:latestThe push refers to repository [docker.io/lnsyyj/helloworld]a9094ec14918: Pushed latest: digest: sha256:dd740db962a1e3a8fb74461505f539248b7c88de80b133db612c22e80d7b2d17 size: 527删除本地镜像，测试下载镜像[vagrant@localhost ~]$ docker rmi lnsyyj/helloworld[vagrant@localhost ~]$ docker pull lnsyyj/helloworld 也可以link到github，github中的Repositories中有Dockerfile，dockehub会自动build镜像 私有镜像搭建docker registry 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950https://hub.docker.com/_/registry[vagrant@localhost ~]$ docker run -d -p 5000:5000 --restart always --name registry registry:2Unable to find image &apos;registry:2&apos; locally2: Pulling from library/registryd6a5679aa3cf: Pull complete ad0eac849f8f: Pull complete 2261ba058a15: Pull complete f296fda86f10: Pull complete bcd4a541795b: Pull complete Digest: sha256:5a156ff125e5a12ac7fdec2b90b7e2ae5120fa249cf62248337b6d04abc574c8Status: Downloaded newer image for registry:22707d472d3dba19f366c7ca51e621b83a63975492152880e01268a326d34bf50[vagrant@localhost ~]$ docker build -t 10.0.2.15:5000/helloworld .Sending build context to Docker daemon 873.5kBStep 1/3 : FROM scratch ---&gt; Step 2/3 : ADD helloworld / ---&gt; Using cache ---&gt; 1b468168e95eStep 3/3 : CMD [&quot;/helloworld&quot;] ---&gt; Using cache ---&gt; 1a8620e6d6deSuccessfully built 1a8620e6d6deSuccessfully tagged 10.0.2.15:5000/helloworld:latest[vagrant@localhost ~]$ cat /etc/docker/daemon.json &#123; &quot;insecure-registries&quot;: [&quot;10.0.2.15:5000&quot;] &#125;[vagrant@localhost ~]$ sudo vim /lib/systemd/system/docker.service 添加EnvironmentFile=-/etc/docker/daemon.json[vagrant@localhost ~]$ sudo systemctl daemon-reload[vagrant@localhost ~]$ service docker restartpush到私有镜像仓库[vagrant@localhost ~]$ docker images10.0.2.15:5000/helloworld latest 1a8620e6d6de 47 hours ago 857kB[vagrant@localhost ~]$ curl http://10.0.2.15:5000/v2/_catalog&#123;&quot;repositories&quot;:[]&#125;[vagrant@localhost ~]$ docker push 10.0.2.15:5000/helloworldThe push refers to repository [10.0.2.15:5000/helloworld]a9094ec14918: Pushed latest: digest: sha256:dd740db962a1e3a8fb74461505f539248b7c88de80b133db612c22e80d7b2d17 size: 527[vagrant@localhost ~]$ curl http://10.0.2.15:5000/v2/_catalog&#123;&quot;repositories&quot;:[&quot;helloworld&quot;]&#125;验证[vagrant@localhost ~]$ docker rmi 10.0.2.15:5000/helloworld[vagrant@localhost ~]$ docker pull 10.0.2.15:5000/helloworld container常用命令docker exec，进入运行中的容器。 1docker exec -it &lt;container ID&gt; /bin/bash docker stop，停止运行中的容器。 1docker stop &lt;container ID&gt; docker inspect，查看容器的详细信息。 1docker inspect &lt;container ID&gt; docker logs，查看容器的log。 1docker logs &lt;container ID&gt; linux压力测试工具stressdocker build命令行程序 1234567891011121314151617181920[vagrant@localhost ~]$ mkdir stress &amp;&amp; cd stress/[vagrant@localhost stress]$ cat Dockerfile FROM ubuntuRUN apt-get update &amp;&amp; apt-get install -y stressENTRYPOINT [&quot;/usr/bin/stress&quot;]CMD [][vagrant@localhost stress]$ docker build -t lnsyyj/ubuntu-stress .[vagrant@localhost stress]$ docker run -it lnsyyj/ubuntu-stress --vm 1 --verbosestress: info: [1] dispatching hogs: 0 cpu, 0 io, 1 vm, 0 hddstress: dbug: [1] using backoff sleep of 3000usstress: dbug: [1] --&gt; hogvm worker 1 [6] forkedstress: dbug: [6] allocating 268435456 bytes ...stress: dbug: [6] touching bytes in strides of 4096 bytes ...stress: dbug: [6] freed 268435456 bytesstress: dbug: [6] allocating 268435456 bytes ...stress: dbug: [6] touching bytes in strides of 4096 bytes ...stress: dbug: [6] freed 268435456 bytes--vm 1 --verbose是通过Dockerfile中的CMD []接收的。ENTRYPOINT+CMD这种方式是非常流行的。 对一个容器进行资源限制12345678910[vagrant@localhost stress]$ docker run --memory=200M -it lnsyyj/ubuntu-stress --vm 1 --vm-bytes 500M --verbosestress: info: [1] dispatching hogs: 0 cpu, 0 io, 1 vm, 0 hddstress: dbug: [1] using backoff sleep of 3000usstress: dbug: [1] --&gt; hogvm worker 1 [6] forkedstress: dbug: [6] allocating 524288000 bytes ...stress: dbug: [6] touching bytes in strides of 4096 bytes ...stress: FAIL: [1] (415) &lt;-- worker 6 got signal 9stress: WARN: [1] (417) now reaping child worker processesstress: FAIL: [1] (421) kill error: No such processstress: FAIL: [1] (451) failed run completed in 1s docker 底层技术支持 namespaces：做隔离pid、net、ipc、mnt、uts control groups：做资源限制 union file systems：container和image的分层]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker-1 Docker installed on centos7]]></title>
    <url>%2F2018%2F12%2F12%2Fdocker-1-Docker-installed-on-centos7%2F</url>
    <content type="text"><![CDATA[学习视频地址：https://coding.imooc.com/class/189.html 在centos7上安装docker1https://docs.docker.com/install/linux/docker-ce/centos/ 新建docker用户，并安装依赖 123456789[root@docker ~]# adduser docker[root@docker ~]# hostnamectl set-hostname docker[root@docker ~]# vi /etc/sudoers添加docker ALL=(ALL) ALL[root@docker ~]# su - docker上一次登录：三 12月 12 10:26:46 EST 2018pts/0 上[docker@docker ~]$ [docker@docker ~]$ sudo yum install epel-release vim -y 安装社区版 1234567891011121314151617181920212223242526272829303132卸载已安装的dockersudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine安装required packagessudo yum install -y yum-utils \ device-mapper-persistent-data \ lvm2安装docker源sudo yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo安装docker[docker@docker ~]$ sudo yum install docker-ce -y启动docker进程，并设置开机启动[docker@docker ~]$ sudo systemctl start docker[docker@docker ~]$ sudo systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.测试docker安装[docker@docker ~]$ sudo docker run hello-world Vagrant建立虚拟机，并在虚拟机中安装docker1、安装VirtualBox然后安装Vagrant 2、下载Vagrant box 12yujiangdeMBP-13:centos7 yujiang$ ls ~/.vagrant.d/downloadboxes/centos7/virtualbox.box 3、添加Vagrant box到镜像列表 12345678yujiangdeMBP-13:centos7 yujiang$ vagrant box add centos/centos7 ~/.vagrant.d/downloadboxes/centos7/virtualbox.box ==&gt; box: Box file was not detected as metadata. Adding it directly...==&gt; box: Adding box &apos;centos/centos7&apos; (v0) for provider: box: Unpacking necessary files from: file:///Users/yujiang/.vagrant.d/downloadboxes/centos7/virtualbox.box==&gt; box: Successfully added box &apos;centos/centos7&apos; (v0) for &apos;virtualbox&apos;!yujiangdeMBP-13:centos7 yujiang$ vagrant box listcentos/centos7 (virtualbox, 0) 4、创建Vagrant虚拟机目录并启动虚拟机 123456yujiangdeMBP-13:~ yujiang$ mkdir Vagrant &amp;&amp; cd Vagrant/yujiangdeMBP-13:Vagrant yujiang$ vagrant init centos/centos7yujiangdeMBP-13:Vagrant yujiang$ vagrant upyujiangdeMBP-13:Vagrant yujiang$ vagrant ssh[vagrant@localhost ~]$ exityujiangdeMBP-13:Vagrant yujiang$ vagrant destroy 5、之后使用《在centos7上安装docker》安装docker 编辑Vagrantfile，启动虚拟机时自动安装docker123456789101112131415161718192021222324252627282930313233343536# -*- mode: ruby -*-Vagrant.configure(&quot;2&quot;) do |config| config.vm.box = &quot;centos/centos7&quot; config.vm.provision &quot;shell&quot;, inline: &lt;&lt;-SHELL sudo yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine sudo yum install -y yum-utils device-mapper-persistent-data lvm2 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce -y sudo systemctl start docker sudo systemctl enable docker sudo groupadd docker sudo gpasswd -a vagrant docker SHELLendyujiangdeMBP-13:Vagrant yujiang$ vagrant upyujiangdeMBP-13:Vagrant yujiang$ vagrant ssh[vagrant@localhost ~]$ sudo docker versionClient: Version: 18.09.0 API version: 1.39 Go version: go1.10.4 Git commit: 4d60db4 Built: Wed Nov 7 00:48:22 2018 OS/Arch: linux/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 18.09.0 API version: 1.39 (minimum version 1.12) Go version: go1.10.4 Git commit: 4d60db4 Built: Wed Nov 7 00:19:08 2018 OS/Arch: linux/amd64 Experimental: false 使用docker命令时，去掉sudo1、添加docker group 12[vagrant@localhost ~]$ sudo groupadd dockergroupadd: group &apos;docker&apos; already exists 2、添加当前用户到docker group 12[vagrant@localhost ~]$ sudo gpasswd -a vagrant dockerAdding user vagrant to group docker 3、退出当前终端，重新连接 1234567891011121314151617181920212223[vagrant@localhost ~]$ exityujiangdeMBP-13:Vagrant yujiang$ vagrant ssh[vagrant@localhost ~]$ docker versionClient: Version: 18.09.0 API version: 1.39 Go version: go1.10.4 Git commit: 4d60db4 Built: Wed Nov 7 00:48:22 2018 OS/Arch: linux/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 18.09.0 API version: 1.39 (minimum version 1.12) Go version: go1.10.4 Git commit: 4d60db4 Built: Wed Nov 7 00:19:08 2018 OS/Arch: linux/amd64 Experimental: false]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitlab jenkins ci]]></title>
    <url>%2F2018%2F12%2F03%2Fgitlab-jenkins-ci%2F</url>
    <content type="text"><![CDATA[搭建jenkins（centos 7）12345678910111213141516171819安装jenkinssudo yum install wget -ysudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat/jenkins.reposudo rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.keysudo yum install jenkins java -y启动jenkinssudo systemctl start jenkinssudo systemctl enable jenkinssudo systemctl status jenkins设置防火墙sudo firewall-cmd --permanent --new-service=jenkinssudo firewall-cmd --permanent --service=jenkins --set-short=&quot;Jenkins Service Ports&quot;sudo firewall-cmd --permanent --service=jenkins --set-description=&quot;Jenkins service firewalld port exceptions&quot;sudo firewall-cmd --permanent --service=jenkins --add-port=8080/tcpsudo firewall-cmd --permanent --add-service=jenkinssudo firewall-cmd --zone=public --add-service=http --permanentsudo firewall-cmd --reload 搭建gitlab123456789101112131415161718192021222324sudo yum install -y curl policycoreutils-python openssh-serversudo systemctl enable sshdsudo systemctl start sshdsudo firewall-cmd --permanent --add-service=httpsudo firewall-cmd --permanent --add-service=httpsudo systemctl reload firewalldsudo systemctl reload firewalld安装Postfix以发送通知电子邮件sudo yum install postfixsudo systemctl enable postfixsudo systemctl start postfix安装gitblawget https://mirror.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/gitlab-ce-11.5.1-ce.0.el7.x86_64.rpmrpm -i gitlab-ce-11.5.1-ce.0.el7.x86_64.rpmgitlab-ctl reconfigurevi /etc/gitlab/gitlab.rbunicorn[&apos;port&apos;] = 80gitlab-ctl reconfiguregitlab-ctl restart 配置jenkins gitlab ci]]></content>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手动部署ceph]]></title>
    <url>%2F2018%2F11%2F26%2F%E6%89%8B%E5%8A%A8%E9%83%A8%E7%BD%B2ceph%2F</url>
    <content type="text"><![CDATA[手动部署ceph是看懂官方ansible或ceph-deploy的关键，了解部署步骤和相关组件才能更好的理解代码，理解OSD的挂载流程，所以个人认为需要做此实验。 官方文档：http://docs.ceph.com/docs/master/install/manual-deployment/ MANUAL DEPLOYMENT所有Ceph集群都需要至少一个monitor，并且至少需要与集群中存储对象副本一样多的OSD。引导初始monitor(s)是部署Ceph存储集群的第一步。monitor部署还为整个集群设置了重要准则，例如pool的副本数，每个OSD的placement groups数（PG数），心跳间隔，是否需要身份验证等。这些值中的大多数都是默认设置的，因此在设置生产环境集群时，了解它们很有用。遵循与安装（快速）相同的配置，我们将设置一个集群，其中node1作为监控节点，node2和node3作为OSD节点 MONITOR BOOTSTRAPPING引导monitor需要许多东西： Unique Identifier（唯一标识符）: fsid是集群的唯一标识符，它是 Ceph 作为文件系统时的文件系统标识符。现在Ceph也支持原生接口，块设备和对象存储网关接口，因此fsid有点不恰当的。 Cluster Name（集群名称）: Ceph集群有一个集群名称，这是一个没有空格的字符串。默认集群名称为ceph，但您可以指定其他集群名称。当您使用多个集群并且需要清楚地了解正在使用哪个集群时，覆盖默认集群名称尤其有用。 例如，在multisite configuration中运行多个集群时，集群名称（例如，us-west，us-east）标识当前CLI会话的集群。注意：要在CLI标识集群名称，请使用集群名称指定Ceph配置文件（例如，ceph.conf，us-west.conf，us-east.conf等）。另请参阅CLI用法（ceph –cluster {cluster-name}）。 Monitor Name: 集群中的每个monitor实例都具有唯一的名称。通常，Ceph Monitor名称是主机名（我们建议独立一台主机运行Ceph Monitor，而不要将Ceph OSD Daemons与Ceph Monitors混合部署在一台机器上）。您可以使用hostname -s检索短主机名。 Monitor Map: 引导初始monitor(s)需要您生成monitor map。monitor map需要fsid，cluster name，以及至少一个host name及其IP地址。 Monitor Keyring: Monitors通过secret key相互通信。您必须生成带有monitor secret的keyring，并在引导初始monitor(s)时提供它。 Administrator Keyring: 要使用ceph CLI tools，您必须具有client.admin用户。因此，您必须生成admin用户和相应keyring，并且还必须将client.admin用户添加到monitor keyring。 上述要求并不意味着创建Ceph配置文件。但是，作为最佳实践，我们建议创建一个Ceph配置文件，并使用fsid，mon initial members和mon host填充它。 您也可以在运行时获取并设置所有monitor settings。但是，Ceph配置文件可能只包含覆盖默认值的那些settings。将settings添加到Ceph配置文件时，这些settings会覆盖默认settings。在Ceph配置文件中维护这些settings可以使您更轻松地维护集群。 过程如下： 1、登录monitor节点： 1ssh &#123;hostname&#125; 例如： 1ssh node1 2、确保您有Ceph配置文件的目录。 默认情况下，Ceph使用/etc/ceph。 安装ceph时，安装程序将自动创建/etc/ceph目录。 1ls /etc/ceph 注意：部署工具可能在清除集群时删除此目录（例如，ceph-deploy purgedata {node-name}，ceph-deploy purge {node-name}）。 3、创建Ceph配置文件。 默认情况下，Ceph使用ceph.conf，其中ceph反映了集群名称。 1sudo vim /etc/ceph/ceph.conf 4、为您的集群生成唯一ID（即fsid）。 1uuidgen 5、将唯一ID添加到Ceph配置文件中。 1fsid = &#123;UUID&#125; 例如： 1fsid = a7f64266-0894-4f1e-a635-d0aeaca0e993 6、将初始monitor(s)添加到Ceph配置文件中。 1mon initial members = &#123;hostname&#125;[,&#123;hostname&#125;] 例如： 1mon initial members = node1 7、将初始monitor(s)的IP地址添加到Ceph配置文件并保存文件。 1mon host = &#123;ip-address&#125;[,&#123;ip-address&#125;] 例如： 1mon host = 192.168.0.1 注意：您可以使用IPv6地址而不是IPv4地址，但必须将ms bind ipv6设置为true。有关网络配置的详细信息，请参阅网络配置参考。 8、为集群创建keyring并生成monitor secret key。 1ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon &apos;allow *&apos; 9、生成administrator keyring，生成client.admin用户并将用户添加到keyring。 1sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --cap mon &apos;allow *&apos; --cap osd &apos;allow *&apos; --cap mds &apos;allow *&apos; --cap mgr &apos;allow *&apos; 10、生成bootstrap-osd keyring，生成client.bootstrap-osd用户并将用户添加到keyring。 1sudo ceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring --gen-key -n client.bootstrap-osd --cap mon &apos;profile bootstrap-osd&apos; 11、将生成的keys添加到ceph.mon.keyring。 12sudo ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyringsudo ceph-authtool /tmp/ceph.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring 12、使用主机名，主机IP地址和FSID生成monitor map。将其另存为/tmp/monmap。 1monmaptool --create --add &#123;hostname&#125; &#123;ip-address&#125; --fsid &#123;uuid&#125; /tmp/monmap 例如： 1monmaptool --create --add node1 192.168.0.1 --fsid a7f64266-0894-4f1e-a635-d0aeaca0e993 /tmp/monmap 13、在monitor主机上创建默认数据目录（或多个目录）。 1sudo mkdir /var/lib/ceph/mon/&#123;cluster-name&#125;-&#123;hostname&#125; 例如： 1sudo -u ceph mkdir /var/lib/ceph/mon/ceph-node1 有关详细信息，请参阅Monitor Config Reference - Data。 14、使用monitor map和keyring填充monitor daemon(s)。 1sudo -u ceph ceph-mon [--cluster &#123;cluster-name&#125;] --mkfs -i &#123;hostname&#125; --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring 例如： 1sudo -u ceph ceph-mon --mkfs -i node1 --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring 15、考虑Ceph配置文件的settings，常见settings包括以下内容： 123456789101112131415[global]fsid = &#123;cluster-id&#125;mon initial members = &#123;hostname&#125;[, &#123;hostname&#125;]mon host = &#123;ip-address&#125;[, &#123;ip-address&#125;]public network = &#123;network&#125;[, &#123;network&#125;]cluster network = &#123;network&#125;[, &#123;network&#125;]auth cluster required = cephxauth service required = cephxauth client required = cephxosd journal size = &#123;n&#125;osd pool default size = &#123;n&#125; # Write an object n times.osd pool default min size = &#123;n&#125; # Allow writing n copies in a degraded state.osd pool default pg num = &#123;n&#125;osd pool default pgp num = &#123;n&#125;osd crush chooseleaf type = &#123;n&#125; 在上面的示例中，配置的[global]部分可能如下所示： 1234567891011121314[global]fsid = a7f64266-0894-4f1e-a635-d0aeaca0e993mon initial members = node1mon host = 192.168.0.1public network = 192.168.0.0/24auth cluster required = cephxauth service required = cephxauth client required = cephxosd journal size = 1024osd pool default size = 3osd pool default min size = 2osd pool default pg num = 333osd pool default pgp num = 333osd crush chooseleaf type = 1 16、启动monitor(s)。 对于大多数linux发行版，services通过systemd启动： 1sudo systemctl start ceph-mon@node1 对于较旧的Debian/CentOS/RHEL，请使用sysvinit： 1sudo /etc/init.d/ceph start mon.node1 17、验证monitor是否正在运行。 1ceph -s 您应该看到monitor已启动并打印运行的输出，并且您应该看到一个运行状况错误，提示placement groups（PG）处于inactive状态。看起来像这样： 1234567891011121314cluster: id: a7f64266-0894-4f1e-a635-d0aeaca0e993 health: HEALTH_OKservices: mon: 1 daemons, quorum node1 mgr: node1(active) osd: 0 osds: 0 up, 0 indata: pools: 0 pools, 0 pgs objects: 0 objects, 0 bytes usage: 0 kB used, 0 kB / 0 kB avail pgs: 注意：添加OSD并启动它们后，placement group（PG）错误应该消失。有关详细信息，请参阅添加OSD。 MANAGER DAEMON CONFIGURATION在运行ceph-mon daemon的每个节点上，还应该设置ceph-mgr daemon。 请参阅ceph-mgr管理员指南。 ADDING OSDS初始monitor(s)运行后，应添加OSD。在有足够的OSD来处理对象的副本数量之前，您的集群无法达到active + clean状态（例如，osd pool default size = 2需要至少两个OSD）。 引导monitor后，您的集群具有默认的CRUSH map，但是，CRUSH map没有映射到Ceph节点的任何Ceph OSD Daemons。 SHORT FORM（简写）Ceph提供了ceph-volume实用程序，它可以prepare logical volume，disk或partition，以便与ceph一起使用。 ceph-volume实用程序通过递增索引来创建OSD ID。 此外，ceph-volume会将新OSD添加到CRUSH map中的host下。执行ceph-volume -h以获取CLI详细信息。ceph-volume实用程序自动执行下面的Long Form的步骤。要使用short form过程创建前两个OSD，请在node2和node3上执行以下操作： BLUESTORE1、创建OSD。 12ssh &#123;node-name&#125;sudo ceph-volume lvm create --data &#123;data-path&#125; 例如： 12ssh node1sudo ceph-volume lvm create --data /dev/hdd1 或者，创建过程可以分为两个阶段（prepare和activate）： 1、Prepare the OSD。 12ssh &#123;node-name&#125;sudo ceph-volume lvm prepare --data &#123;data-path&#125; &#123;data-path&#125; 例如： 12ssh node1sudo ceph-volume lvm prepare --data /dev/hdd1 prepare后，激活prepare后的OSD的ID和FSID。这些可以通过列出当前服务器中的OSD来获得： 1sudo ceph-volume lvm list 2、Activate the OSD。 1sudo ceph-volume lvm activate &#123;ID&#125; &#123;FSID&#125; 例如： 1sudo ceph-volume lvm activate 0 a7f64266-0894-4f1e-a635-d0aeaca0e993 FILESTORE1、Create the OSD。 12ssh &#123;node-name&#125;sudo ceph-volume lvm create --filestore --data &#123;data-path&#125; --journal &#123;journal-path&#125; 例如： 12ssh node1sudo ceph-volume lvm create --filestore --data /dev/hdd1 --journal /dev/hdd2 或者，创建过程可以分为两个阶段（prepare和activate）： 1、Prepare the OSD。 12ssh &#123;node-name&#125;sudo ceph-volume lvm prepare --filestore --data &#123;data-path&#125; --journal &#123;journal-path&#125; 例如： 12ssh node1sudo ceph-volume lvm prepare --filestore --data /dev/hdd1 --journal /dev/hdd2 prepare后，激活prepare后的OSD的ID和FSID。这些可以通过列出当前服务器中的OSD来获得： 1sudo ceph-volume lvm list 2、Activate the OSD。 1sudo ceph-volume lvm activate --filestore &#123;ID&#125; &#123;FSID&#125; 例如： 1sudo ceph-volume lvm activate --filestore 0 a7f64266-0894-4f1e-a635-d0aeaca0e993 LONG FORM如果没有任何helper实用程序，请创建一个OSD并使用以下步骤将其添加到集群和CRUSH map中。要使用long form方式创建前两个OSD，请对每个OSD执行以下步骤。 1注意：此过程没有描述使用dm-crypt“lockbox”在dm-crypt之上的部署。 1、连接到OSD主机并变为root用户。 12ssh &#123;node-name&#125;sudo bash 2、为OSD生成UUID。 1UUID=$(uuidgen) 3、为OSD生成cephx key。 1OSD_SECRET=$(ceph-authtool --gen-print-key) 4、创建OSD。 请注意，如果您需要重用之前销毁的OSD ID，可以提供OSD ID作为ceph osd new的附加参数。我们假设机器上存在client.bootstrap-osd key。您也可以在拥有该key的其他主机上以client.admin身份执行此命令： 123ID=$(echo &quot;&#123;\&quot;cephx_secret\&quot;: \&quot;$OSD_SECRET\&quot;&#125;&quot; | \ ceph osd new $UUID -i - \ -n client.bootstrap-osd -k /var/lib/ceph/bootstrap-osd/ceph.keyring) 也可以在JSON中包含crush_device_class属性，以设置默认值以外的initial class（基于自动检测到的设备类型，ssd或hdd）。 5、在新OSD上创建默认目录。 1mkdir /var/lib/ceph/osd/ceph-$ID 6、如果OSD使用操作系统以外的drive，先创建文件系统，并将其mount到刚刚创建的目录。 12mkfs.xfs /dev/&#123;DEV&#125;mount /dev/&#123;DEV&#125; /var/lib/ceph/osd/ceph-$ID 7、将secret写入OSD keyring文件中。 12ceph-authtool --create-keyring /var/lib/ceph/osd/ceph-$ID/keyring \ --name osd.$ID --add-key $OSD_SECRET 8、初始化OSD数据目录。 1ceph-osd -i $ID --mkfs --osd-uuid $UUID 9、修复所有权。 1chown -R ceph:ceph /var/lib/ceph/osd/ceph-$ID 10、将OSD添加到Ceph后，OSD就在您的配置中。但是，它还没有运行。必须先启动新的OSD才能开始接收数据。 systemd方式： 12systemctl enable ceph-osd@$IDsystemctl start ceph-osd@$ID 例如： 12systemctl enable ceph-osd@12systemctl start ceph-osd@12 ADDING MDS在以下说明中，{id}是任意名称，例如计算机的hostname。 1、创建mds数据目录： 1mkdir -p /var/lib/ceph/mds/&#123;cluster-name&#125;-&#123;id&#125; 2、创建一个keyring： 1ceph-authtool --create-keyring /var/lib/ceph/mds/&#123;cluster-name&#125;-&#123;id&#125;/keyring --gen-key -n mds.&#123;id&#125; 3、导入keyring并设置caps： 1ceph auth add mds.&#123;id&#125; osd &quot;allow rwx&quot; mds &quot;allow&quot; mon &quot;allow profile mds&quot; -i /var/lib/ceph/mds/&#123;cluster&#125;-&#123;id&#125;/keyring 4、添加到ceph.conf： 12[mds.&#123;id&#125;]host = &#123;id&#125; 5、以手动方式启动daemon： 1ceph-mds --cluster &#123;cluster-name&#125; -i &#123;id&#125; -m &#123;mon-hostname&#125;:&#123;mon-port&#125; [-f] 6、以正确的方式启动daemon（使用ceph.conf entry）： 1service ceph start 7、如果启动daemon失败并显示以下错误： 1mds.-1.0 ERROR: failed to authenticate: (22) Invalid argument 那么，需要确认你没有在ceph.conf的global section中设置keyring；并将其移至client section。或添加特定于此mds daemon的keyring设置。并验证您是否在mds data目录中看到相同的key，并且与ceph auth get mds.{id}命令的输出相同。 8、现在您已准备好创建Ceph filesystem。 SUMMARY一旦您的monitor和两个OSD启动并运行，可以通过执行以下操作来观察placement groups peer： 1ceph -w 要查看tree，请执行以下操作： 1ceph osd tree 你应该看到像这样的输出： 123456# id weight type name up/down reweight-1 2 root default-2 2 host node10 1 osd.0 up 1-3 1 host node21 1 osd.1 up 1 要添加（或删除）其他monitors，请参阅Add/Remove Monitors。要添加（或删除）其他Ceph OSD Daemons，请参阅Add/Remove OSDs。 部署实验monitor部署实验1、添加ceph源 12345678910[root@cephlm ~]# vi /etc/yum.repos.d/ceph.repo [ceph]name=Cephbaseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/x86_64/# baseurl=https://download.ceph.com/rpm-luminous/el7/x86_64/enabled=1gpgcheck=1type=rpm-mdgpgkey=http://mirrors.163.com/ceph/keys/release.asc# gpgkey=https://download.ceph.com/keys/release.asc 2、安装epel与ceph 1234[1]epel参考：https://blog.csdn.net/yasi_xi/article/details/11746255EPEL的全称叫 Extra Packages for Enterprise Linux 。EPEL是由 Fedora 社区打造，为 RHEL 及衍生发行版如 CentOS、Scientific Linux 等提供高质量软件包的项目。装上了 EPEL之后，就相当于添加了一个第三方源。[root@cephlm ~]# yum install epel-release -y &amp;&amp; yum install ceph -y 3、为集群创建keyring并生成monitor secret key。 123456[root@cephlm ~]# ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon &apos;allow *&apos;creating /tmp/ceph.mon.keyring[root@cephlm ~]# cat /tmp/ceph.mon.keyring[mon.] key = AQAgbv1bc62FBBAAvuCz2a5EDtbAmy9ep1Dxxw== caps mon = &quot;allow *&quot; 4、生成administrator keyring，生成client.admin用户并将用户添加到keyring。 123456789[root@cephlm ~]# sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --cap mon &apos;allow *&apos; --cap osd &apos;allow *&apos; --cap mds &apos;allow *&apos; --cap mgr &apos;allow *&apos;creating /etc/ceph/ceph.client.admin.keyring[root@cephlm ~]# cat /etc/ceph/ceph.client.admin.keyring[client.admin] key = AQBFbv1bImaJCxAAYUiUCuia//zZSMIPyOHJuA== caps mds = &quot;allow *&quot; caps mgr = &quot;allow *&quot; caps mon = &quot;allow *&quot; caps osd = &quot;allow *&quot; 5、生成bootstrap-osd keyring，生成client.bootstrap-osd用户并将用户添加到keyring。 123456[root@cephlm ~]# sudo ceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring --gen-key -n client.bootstrap-osd --cap mon &apos;profile bootstrap-osd&apos;creating /var/lib/ceph/bootstrap-osd/ceph.keyring[root@cephlm ~]# cat /var/lib/ceph/bootstrap-osd/ceph.keyring[client.bootstrap-osd] key = AQBQbv1bXv0WAhAAo/hv7OOaftMHOovHNeyOFg== caps mon = &quot;profile bootstrap-osd&quot; 6、将生成的keys添加到ceph.mon.keyring。 12345678910111213141516[root@cephlm ~]# sudo ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring[root@cephlm ~]# sudo ceph-authtool /tmp/ceph.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring[root@cephlm ~]# cat /tmp/ceph.mon.keyring[mon.] key = AQAgbv1bc62FBBAAvuCz2a5EDtbAmy9ep1Dxxw== caps mon = &quot;allow *&quot;[client.admin] key = AQBFbv1bImaJCxAAYUiUCuia//zZSMIPyOHJuA== caps mds = &quot;allow *&quot; caps mgr = &quot;allow *&quot; caps mon = &quot;allow *&quot; caps osd = &quot;allow *&quot;[client.bootstrap-osd] key = AQBQbv1bXv0WAhAAo/hv7OOaftMHOovHNeyOFg== caps mon = &quot;profile bootstrap-osd&quot; 7、使用主机名，主机IP地址和FSID生成monitor map。将其另存为/tmp/monmap。 123456789101112131415[root@cephlm ~]# monmaptool --create --add cephlm 192.168.0.10 --fsid c8b0b137-1ba7-4c1f-a514-281139c35233 /tmp/monmapmonmaptool: monmap file /tmp/monmapmonmaptool: set fsid to c8b0b137-1ba7-4c1f-a514-281139c35233monmaptool: writing epoch 0 to /tmp/monmap (1 monitors)更改权限，可是使ceph用户有读该文件的权限[root@cephlm ~]# chmod +r /tmp/monmap &amp;&amp; chmod +r /tmp/ceph.mon.keyring[root@cephlm ~]# monmaptool --print /tmp/monmapmonmaptool: monmap file /tmp/monmapepoch 0fsid c8b0b137-1ba7-4c1f-a514-281139c35233last_changed 2018-11-28 10:55:11.677288created 2018-11-28 10:55:11.6772880: 192.168.56.205:6789/0 mon.cephlm 8、创建配置文件文件&lt;集群名&gt;.conf 1234567891011121314[root@cephlm ~]# vi /etc/ceph/ceph.conf [global]fsid = c8b0b137-1ba7-4c1f-a514-281139c35233mon initial members = cephlmmon host = 192.168.0.10public network = 192.168.0.0/24cluster network = 192.168.0.0/24auth cluster required = cephxauth service required = cephxauth client required = cephxosd pool default size = 1osd pool default min size = 1osd pool default pg num = 16osd pool default pgp num = 16 9、使用monitor map和keyring填充monitor daemon(s)。 1[root@cephlm ~]# sudo -u ceph ceph-mon --cluster ceph --mkfs -i cephlm --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring 10、启动ceph-mon服务 123456789101112131415161718[root@cephlm ~]# systemctl enable ceph-mon@cephlm[root@cephlm ~]# systemctl start ceph-mon@cephlm[root@cephlm ~]# ceph -s cluster: id: c8b0b137-1ba7-4c1f-a514-281139c35233 health: HEALTH_OK services: mon: 1 daemons, quorum cephlm mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0B usage: 0B used, 0B / 0B avail pgs: osd部署实验1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736741、为OSD生成UUID。[root@cephlm ~]# UUID=$(uuidgen)[root@cephlm ~]# echo $&#123;UUID&#125;265b6807-fa12-46f7-8e25-ce8b03dc2a2d2、为OSD生成cephx key。[root@cephlm ~]# OSD_SECRET=$(ceph-authtool --gen-print-key)[root@cephlm ~]# echo $&#123;OSD_SECRET&#125;AQDljwhcB8MxBxAATx/pqYOv0uRhQI9Tey5UeQ==3、为OSD生成cephx key（秘钥）。[root@cephlm ~]# ID=$(echo &quot;&#123;\&quot;cephx_secret\&quot;: \&quot;$OSD_SECRET\&quot;&#125;&quot; | \ ceph osd new $UUID -i - \/* -n client.bootstrap-osd -k /var/lib/ceph/bootstrap-osd/ceph.keyring)[root@cephlm ~]# echo $&#123;ID&#125;04、[root@cephlm ~]# mkdir /var/lib/ceph/osd/ceph-$ID5、[root@cephlm ~]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTvda 253:0 0 20G 0 disk ├─vda1 253:1 0 1G 0 part /boot├─vda2 253:2 0 4G 0 part [SWAP]└─vda3 253:3 0 15G 0 part /vdb 253:16 0 50G 0 disk vdc 253:32 0 50G 0 disk vdd 253:48 0 50G 0 disk [root@cephlm ~]# ll /var/lib/ceph/osd/总用量 0drwxr-xr-x 2 root root 6 12月 6 11:03 ceph-0[root@cephlm ~]# mkfs.xfs /dev/vdb -fmeta-data=/dev/vdb isize=512 agcount=4, agsize=3276800 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0data = bsize=4096 blocks=13107200, imaxpct=25 = sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal log bsize=4096 blocks=6400, version=2 = sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0[root@cephlm ~]# mount /dev/vdb /var/lib/ceph/osd/ceph-$ID/[root@cephlm ~]# df -Th文件系统 类型 容量 已用 可用 已用% 挂载点/dev/vda3 xfs 15G 1.6G 14G 11% /devtmpfs devtmpfs 1.9G 0 1.9G 0% /devtmpfs tmpfs 1.9G 0 1.9G 0% /dev/shmtmpfs tmpfs 1.9G 8.6M 1.9G 1% /runtmpfs tmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup/dev/vda1 xfs 1014M 172M 843M 17% /boottmpfs tmpfs 379M 0 379M 0% /run/user/0/dev/vdb xfs 50G 33M 50G 1% /var/lib/ceph/osd/ceph-06、将secret写入OSD keyring文件。[root@cephlm ~]# ceph-authtool --create-keyring /var/lib/ceph/osd/ceph-$ID/keyring --name osd.$ID --add-key $OSD_SECRETcreating /var/lib/ceph/osd/ceph-0/keyringadded entity osd.0 auth auth(auid = 18446744073709551615 key=AQDljwhcB8MxBxAATx/pqYOv0uRhQI9Tey5UeQ== with 0 caps)7、初始化OSD数据目录[root@cephlm ~]# ceph-osd -i $ID --mkfs --osd-uuid $UUID2019-08-02 18:07:34.990087 7f41b6680d80 -1 journal FileJournal::_open: disabling aio for non-block journal. Use journal_force_aio to force use of aio anyway2019-08-02 18:07:35.061501 7f41b6680d80 -1 journal FileJournal::_open: disabling aio for non-block journal. Use journal_force_aio to force use of aio anyway2019-08-02 18:07:35.062969 7f41b6680d80 -1 journal do_read_entry(4096): bad header magic2019-08-02 18:07:35.062993 7f41b6680d80 -1 journal do_read_entry(4096): bad header magic2019-08-02 18:07:35.063581 7f41b6680d80 -1 read_settings error reading settings: (2) No such file or directory2019-08-02 18:07:35.146735 7f41b6680d80 -1 created object store /var/lib/ceph/osd/ceph-0 for osd.0 fsid c8b0b137-1ba7-4c1f-a514-281139c352338、[root@cephlm ~]# chown -R ceph:ceph /var/lib/ceph/osd/ceph-$ID9、[root@cephlm ~]# systemctl enable ceph-osd@$ID[root@cephlm ~]# systemctl start ceph-osd@$ID10、[root@sds-ceph-1 ~]# ceph osd pool create rbd 16[root@sds-ceph-1 ~]# rados bench -p rbd 10 write --no-cleanup[root@sds-ceph-1 ~]# tree -h /var/lib/ceph/osd/ceph-0/current//var/lib/ceph/osd/ceph-0/current/├── [4.0K] 1.0_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject126__head_314B9110__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject138__head_F0F776D0__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject149__head_9DC23D60__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject154__head_03D0DB50__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject179__head_E33CA580__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject209__head_3BFDF2F0__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject211__head_DECF96B0__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject212__head_5CED1CF0__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject217__head_1319A430__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject220__head_524D4D00__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject229__head_F40C7DE0__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject22__head_1686F940__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject235__head_516A6B20__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject236__head_12CA1390__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject73__head_397FC990__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject77__head_ADAEA100__1│ └── [ 0] __head_00000000__1├── [ 6] 1.0_TEMP├── [4.0K] 1.1_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject102__head_862AA4F1__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject129__head_F5965891__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject151__head_98E52B51__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject184__head_E52D8D41__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject1__head_F5F320A1__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject234__head_00E8BB81__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject244__head_2363F6E1__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject25__head_581DDE31__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject32__head_324147A1__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject35__head_34FABC91__1│ └── [ 0] __head_00000001__1├── [ 6] 1.1_TEMP├── [4.0K] 1.2_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject104__head_413DA2B2__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject117__head_C0124C02__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject125__head_9DBB0642__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject153__head_37933452__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject167__head_0AE2CE12__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject173__head_821A3292__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject175__head_734C3FB2__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject180__head_74A8D2D2__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject195__head_7A482B82__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject214__head_C62A3262__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject227__head_FBA5E8D2__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject230__head_F81DABF2__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject240__head_96D571D2__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject251__head_8C210072__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject28__head_8D1859C2__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject43__head_9D4243F2__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject4__head_6D94E022__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject53__head_F31A17D2__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject74__head_BA8C88D2__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject93__head_26EE91D2__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject97__head_1025B4F2__1│ └── [ 0] __head_00000002__1├── [ 6] 1.2_TEMP├── [4.0K] 1.3_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject114__head_C05EC3E3__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject145__head_BB9B5383__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject187__head_7F2D3B93__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject20__head_FB09C543__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject222__head_CA878AF3__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject223__head_45D47B63__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject249__head_EB2C03F3__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject2__head_E1BEFAD3__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject48__head_C2ABBF03__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject58__head_73A784F3__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject66__head_7D1C7D23__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject68__head_64431003__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject75__head_EFA44023__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject7__head_1EDC2B13__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject83__head_56DE9F23__1│ └── [ 0] __head_00000003__1├── [ 6] 1.3_TEMP├── [4.0K] 1.4_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject0__head_F8BC28D4__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject160__head_866C8F94__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject162__head_02475BD4__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject164__head_26707F24__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject16__head_3B6FF704__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject174__head_ECF6CC74__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject18__head_7CB94CE4__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject202__head_503B6AE4__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject237__head_1AF8F074__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject247__head_A70993B4__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject31__head_09DBA9F4__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject36__head_ABBB7824__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject39__head_E1727904__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject3__head_E0FE6334__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject46__head_9E9B4204__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject51__head_8C1E0DC4__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject56__head_7065C2F4__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject62__head_DB32DD24__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject87__head_BDBE9FC4__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject90__head_A1654F74__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject96__head_FC6F33F4__1│ └── [ 0] __head_00000004__1├── [ 6] 1.4_TEMP├── [4.0K] 1.5_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject118__head_7B842745__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject123__head_5CEFA465__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject183__head_10F85FE5__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject186__head_3000C235__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject191__head_BC532EE5__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject224__head_92A1E3C5__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject29__head_027ADDC5__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject67__head_5BDDDAA5__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject82__head_A64A01D5__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject8__head_F68FCDE5__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject94__head_ECE61315__1│ └── [ 0] __head_00000005__1├── [ 6] 1.5_TEMP├── [4.0K] 1.6_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject121__head_3F39BC76__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject12__head_88A43B86__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject13__head_FD5483D6__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject141__head_CDACF3C6__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject157__head_E56D1786__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject197__head_804A2DC6__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject207__head_A3266F06__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject216__head_DE5D3A76__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject226__head_CAFCCF66__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject44__head_0544C586__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject45__head_65E3FAB6__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject61__head_A2704176__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject72__head_6E3BA9D6__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject80__head_1B1E6F16__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject85__head_A0756156__1│ └── [ 0] __head_00000006__1├── [ 6] 1.6_TEMP├── [4.0K] 1.7_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject100__head_EF2E6867__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject139__head_5E961CE7__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject142__head_01BCFCA7__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject143__head_A3CDD177__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject144__head_74883667__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject158__head_F5DD1EA7__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject165__head_031366F7__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject168__head_9BE47967__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject169__head_E96B49F7__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject182__head_7119D257__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject194__head_D3C65A17__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject250__head_E9F80417__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject41__head_9295C277__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject55__head_70B967B7__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject78__head_367C2607__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject89__head_7CDE0FC7__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject98__head_96668A47__1│ └── [ 0] __head_00000007__1├── [ 6] 1.7_TEMP├── [4.0K] 1.8_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject116__head_F4318178__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject128__head_32ED4D38__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject134__head_486DF128__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject156__head_F2E7C068__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject190__head_5AAF63A8__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject219__head_50799E88__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject225__head_9EB9D0C8__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject242__head_F2EC42B8__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject33__head_64C2C558__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject34__head_65F25F38__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject38__head_D38CB608__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject49__head_FF4A4208__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject50__head_0F3F77A8__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject52__head_0F2A0548__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject69__head_BDEA5548__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject71__head_EA5383B8__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject76__head_02238F98__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject88__head_45AB4318__1│ └── [ 0] __head_00000008__1├── [ 6] 1.8_TEMP├── [4.0K] 1.9_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject110__head_CE2B10A9__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject136__head_04EDA809__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject163__head_37F8AE49__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject177__head_5F771509__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject193__head_23935A99__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject215__head_15DEBEA9__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject21__head_11518C29__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject248__head_FAFE5689__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject27__head_5B7D7369__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject60__head_64B13949__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject70__head_F3A64E09__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject91__head_31511109__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject99__head_8A68E509__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject9__head_FC52C009__1│ └── [ 0] __head_00000009__1├── [ 6] 1.9_TEMP├── [4.0K] 1.a_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject112__head_FF4D01DA__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject113__head_CADF065A__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject119__head_A72F161A__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject122__head_BF4A0A6A__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject124__head_5BC12E2A__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject140__head_37A19CDA__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject14__head_64FC3EDA__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject152__head_14C2CA0A__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject166__head_C47A6B0A__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject171__head_79A2CFEA__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject17__head_2A4A393A__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject185__head_1D2DDDDA__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject188__head_7FBA73AA__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject204__head_AB1FBD8A__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject205__head_B24EE76A__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject206__head_9891B01A__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject208__head_C07981EA__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject210__head_1227C4DA__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject228__head_0B9B39AA__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject238__head_C56F1D5A__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject23__head_4F279EBA__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject26__head_CCB3C59A__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject86__head_5ED418EA__1│ └── [ 0] __head_0000000A__1├── [ 6] 1.a_TEMP├── [4.0K] 1.b_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject103__head_F8EF0F1B__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject107__head_F3693ABB__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject108__head_13D4D0EB__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject109__head_11C8A99B__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject132__head_227AA5FB__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject137__head_5D82744B__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject148__head_989FE31B__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject159__head_422DA2AB__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject178__head_01C4C3CB__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject181__head_0DC3436B__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject213__head_05B8BA6B__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject246__head_2140121B__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject24__head_5B8169AB__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject64__head_CC62D71B__1│ └── [ 0] __head_0000000B__1├── [ 6] 1.b_TEMP├── [4.0K] 1.c_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject101__head_7711D38C__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject11__head_0218B38C__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject127__head_9A51BCDC__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject133__head_7E6B67EC__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject135__head_C67A3E8C__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject150__head_8635CA6C__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject161__head_3A42EA3C__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject170__head_612CFAFC__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject172__head_EB33BB8C__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject196__head_475FC6BC__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject19__head_D21864EC__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject200__head_52F1139C__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject201__head_14AC036C__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject243__head_3E9C237C__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject245__head_DABE713C__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject5__head_7CE3E36C__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject65__head_86389E8C__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject6__head_1FBF833C__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject81__head_05B34FAC__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject92__head_3988A13C__1│ └── [ 0] __head_0000000C__1├── [ 6] 1.c_TEMP├── [4.0K] 1.d_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject111__head_66CE6C0D__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject115__head_71F498AD__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject120__head_1BFD990D__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject130__head_301FC3CD__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject232__head_320DC13D__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject37__head_AC68B41D__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject40__head_423D9AAD__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject47__head_0951C33D__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject84__head_5EB447DD__1│ ├── [ 24] benchmark\\ulast\\umetadata__head_8E7A861D__1│ └── [ 0] __head_0000000D__1├── [ 6] 1.d_TEMP├── [4.0K] 1.e_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject105__head_C439984E__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject10__head_1BF35FCE__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject131__head_DBAC810E__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject146__head_6153061E__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject176__head_80C2B02E__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject189__head_4F8A59BE__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject199__head_5ACCE8EE__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject203__head_D77F59BE__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject221__head_C3ADCA4E__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject231__head_0C48675E__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject233__head_F01E13EE__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject30__head_9464F9BE__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject54__head_B1E1B44E__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject59__head_CE84DF4E__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject79__head_CAFFBC0E__1│ └── [ 0] __head_0000000E__1├── [ 6] 1.e_TEMP├── [4.0K] 1.f_head│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject106__head_48E9311F__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject147__head_7211B38F__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject155__head_551133AF__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject15__head_58EF207F__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject192__head_9E2A248F__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject198__head_D7D45B1F__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject218__head_F2A93B5F__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject239__head_2FC0444F__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject241__head_286B4E0F__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject42__head_6FF5EA1F__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject57__head_2A0A792F__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject63__head_2685437F__1│ ├── [4.0M] benchmark\\udata\\usds-ceph-1.novalocal\\u12560\\uobject95__head_1786F82F__1│ └── [ 0] __head_0000000F__1├── [ 6] 1.f_TEMP├── [ 4] commit_op_seq├── [4.0K] meta│ ├── [ 802] inc\\uosdmap.1__0_B65F4306__none│ ├── [ 232] inc\\uosdmap.2__0_B65F40D6__none│ ├── [ 635] inc\\uosdmap.3__0_B65F4066__none│ ├── [ 763] inc\\uosdmap.4__0_B65F4136__none│ ├── [ 406] inc\\uosdmap.5__0_B65F46C6__none│ ├── [ 454] inc\\uosdmap.6__0_B65F4796__none│ ├── [ 212] inc\\uosdmap.7__0_B65F4726__none│ ├── [ 204] inc\\uosdmap.8__0_B65F44F6__none│ ├── [ 598] osdmap.1__0_FD6E49B1__none│ ├── [ 869] osdmap.2__0_FD6E4941__none│ ├── [ 960] osdmap.3__0_FD6E4E11__none│ ├── [1.1K] osdmap.4__0_FD6E4FA1__none│ ├── [1.0K] osdmap.5__0_FD6E4F71__none│ ├── [1.3K] osdmap.6__0_FD6E4C01__none│ ├── [1.3K] osdmap.7__0_FD6E4DD1__none│ ├── [1.3K] osdmap.8__0_FD6E4D61__none│ ├── [ 511] osd\\usuperblock__0_23C2FCDE__none│ └── [ 0] snapmapper__0_A468EC03__none├── [ 0] nosnap└── [ 179] omap ├── [1002] 000007.sst ├── [208K] 000009.log ├── [ 16] CURRENT ├── [ 37] IDENTITY ├── [ 0] LOCK ├── [ 15K] LOG ├── [ 108] MANIFEST-000008 ├── [4.1K] OPTIONS-000008 ├── [4.1K] OPTIONS-000011 └── [ 37] osd_uuid34 directories, 299 files[root@sds-ceph-1 ~]# rados ls -p rbdbenchmark_data_sds-ceph-1.novalocal_12560_object77benchmark_data_sds-ceph-1.novalocal_12560_object220benchmark_data_sds-ceph-1.novalocal_12560_object179benchmark_data_sds-ceph-1.novalocal_12560_object22benchmark_data_sds-ceph-1.novalocal_12560_object235benchmark_data_sds-ceph-1.novalocal_12560_object149benchmark_data_sds-ceph-1.novalocal_12560_object229benchmark_data_sds-ceph-1.novalocal_12560_object126benchmark_data_sds-ceph-1.novalocal_12560_object73benchmark_data_sds-ceph-1.novalocal_12560_object236benchmark_data_sds-ceph-1.novalocal_12560_object154benchmark_data_sds-ceph-1.novalocal_12560_object138benchmark_data_sds-ceph-1.novalocal_12560_object217benchmark_data_sds-ceph-1.novalocal_12560_object211benchmark_data_sds-ceph-1.novalocal_12560_object212benchmark_data_sds-ceph-1.novalocal_12560_object209benchmark_data_sds-ceph-1.novalocal_12560_object49benchmark_data_sds-ceph-1.novalocal_12560_object38benchmark_data_sds-ceph-1.novalocal_12560_object219benchmark_data_sds-ceph-1.novalocal_12560_object52benchmark_data_sds-ceph-1.novalocal_12560_object69benchmark_data_sds-ceph-1.novalocal_12560_object225benchmark_data_sds-ceph-1.novalocal_12560_object134benchmark_data_sds-ceph-1.novalocal_12560_object190benchmark_data_sds-ceph-1.novalocal_12560_object50benchmark_data_sds-ceph-1.novalocal_12560_object156benchmark_data_sds-ceph-1.novalocal_12560_object88benchmark_data_sds-ceph-1.novalocal_12560_object76benchmark_data_sds-ceph-1.novalocal_12560_object33benchmark_data_sds-ceph-1.novalocal_12560_object128benchmark_data_sds-ceph-1.novalocal_12560_object34benchmark_data_sds-ceph-1.novalocal_12560_object242benchmark_data_sds-ceph-1.novalocal_12560_object71benchmark_data_sds-ceph-1.novalocal_12560_object116benchmark_data_sds-ceph-1.novalocal_12560_object46benchmark_data_sds-ceph-1.novalocal_12560_object39benchmark_data_sds-ceph-1.novalocal_12560_object16benchmark_data_sds-ceph-1.novalocal_12560_object51benchmark_data_sds-ceph-1.novalocal_12560_object87benchmark_data_sds-ceph-1.novalocal_12560_object36benchmark_data_sds-ceph-1.novalocal_12560_object62benchmark_data_sds-ceph-1.novalocal_12560_object164benchmark_data_sds-ceph-1.novalocal_12560_object18benchmark_data_sds-ceph-1.novalocal_12560_object202benchmark_data_sds-ceph-1.novalocal_12560_object160benchmark_data_sds-ceph-1.novalocal_12560_object0benchmark_data_sds-ceph-1.novalocal_12560_object162benchmark_data_sds-ceph-1.novalocal_12560_object3benchmark_data_sds-ceph-1.novalocal_12560_object247benchmark_data_sds-ceph-1.novalocal_12560_object237benchmark_data_sds-ceph-1.novalocal_12560_object174benchmark_data_sds-ceph-1.novalocal_12560_object90benchmark_data_sds-ceph-1.novalocal_12560_object56benchmark_data_sds-ceph-1.novalocal_12560_object31benchmark_data_sds-ceph-1.novalocal_12560_object96benchmark_data_sds-ceph-1.novalocal_12560_object65benchmark_data_sds-ceph-1.novalocal_12560_object135benchmark_data_sds-ceph-1.novalocal_12560_object101benchmark_data_sds-ceph-1.novalocal_12560_object11benchmark_data_sds-ceph-1.novalocal_12560_object172benchmark_data_sds-ceph-1.novalocal_12560_object81benchmark_data_sds-ceph-1.novalocal_12560_object150benchmark_data_sds-ceph-1.novalocal_12560_object201benchmark_data_sds-ceph-1.novalocal_12560_object5benchmark_data_sds-ceph-1.novalocal_12560_object19benchmark_data_sds-ceph-1.novalocal_12560_object133benchmark_data_sds-ceph-1.novalocal_12560_object200benchmark_data_sds-ceph-1.novalocal_12560_object127benchmark_data_sds-ceph-1.novalocal_12560_object161benchmark_data_sds-ceph-1.novalocal_12560_object92benchmark_data_sds-ceph-1.novalocal_12560_object245benchmark_data_sds-ceph-1.novalocal_12560_object6benchmark_data_sds-ceph-1.novalocal_12560_object196benchmark_data_sds-ceph-1.novalocal_12560_object243benchmark_data_sds-ceph-1.novalocal_12560_object170benchmark_data_sds-ceph-1.novalocal_12560_object117benchmark_data_sds-ceph-1.novalocal_12560_object195benchmark_data_sds-ceph-1.novalocal_12560_object125benchmark_data_sds-ceph-1.novalocal_12560_object28benchmark_data_sds-ceph-1.novalocal_12560_object4benchmark_data_sds-ceph-1.novalocal_12560_object214benchmark_data_sds-ceph-1.novalocal_12560_object167benchmark_data_sds-ceph-1.novalocal_12560_object173benchmark_data_sds-ceph-1.novalocal_12560_object153benchmark_data_sds-ceph-1.novalocal_12560_object74benchmark_data_sds-ceph-1.novalocal_12560_object227benchmark_data_sds-ceph-1.novalocal_12560_object180benchmark_data_sds-ceph-1.novalocal_12560_object93benchmark_data_sds-ceph-1.novalocal_12560_object240benchmark_data_sds-ceph-1.novalocal_12560_object53benchmark_data_sds-ceph-1.novalocal_12560_object104benchmark_data_sds-ceph-1.novalocal_12560_object175benchmark_data_sds-ceph-1.novalocal_12560_object251benchmark_data_sds-ceph-1.novalocal_12560_object97benchmark_data_sds-ceph-1.novalocal_12560_object43benchmark_data_sds-ceph-1.novalocal_12560_object230benchmark_data_sds-ceph-1.novalocal_12560_object152benchmark_data_sds-ceph-1.novalocal_12560_object166benchmark_data_sds-ceph-1.novalocal_12560_object204benchmark_data_sds-ceph-1.novalocal_12560_object124benchmark_data_sds-ceph-1.novalocal_12560_object228benchmark_data_sds-ceph-1.novalocal_12560_object188benchmark_data_sds-ceph-1.novalocal_12560_object122benchmark_data_sds-ceph-1.novalocal_12560_object205benchmark_data_sds-ceph-1.novalocal_12560_object86benchmark_data_sds-ceph-1.novalocal_12560_object208benchmark_data_sds-ceph-1.novalocal_12560_object171benchmark_data_sds-ceph-1.novalocal_12560_object206benchmark_data_sds-ceph-1.novalocal_12560_object119benchmark_data_sds-ceph-1.novalocal_12560_object26benchmark_data_sds-ceph-1.novalocal_12560_object113benchmark_data_sds-ceph-1.novalocal_12560_object238benchmark_data_sds-ceph-1.novalocal_12560_object210benchmark_data_sds-ceph-1.novalocal_12560_object140benchmark_data_sds-ceph-1.novalocal_12560_object14benchmark_data_sds-ceph-1.novalocal_12560_object112benchmark_data_sds-ceph-1.novalocal_12560_object185benchmark_data_sds-ceph-1.novalocal_12560_object17benchmark_data_sds-ceph-1.novalocal_12560_object23benchmark_data_sds-ceph-1.novalocal_12560_object207benchmark_data_sds-ceph-1.novalocal_12560_object44benchmark_data_sds-ceph-1.novalocal_12560_object12benchmark_data_sds-ceph-1.novalocal_12560_object157benchmark_data_sds-ceph-1.novalocal_12560_object197benchmark_data_sds-ceph-1.novalocal_12560_object141benchmark_data_sds-ceph-1.novalocal_12560_object226benchmark_data_sds-ceph-1.novalocal_12560_object80benchmark_data_sds-ceph-1.novalocal_12560_object85benchmark_data_sds-ceph-1.novalocal_12560_object72benchmark_data_sds-ceph-1.novalocal_12560_object13benchmark_data_sds-ceph-1.novalocal_12560_object45benchmark_data_sds-ceph-1.novalocal_12560_object121benchmark_data_sds-ceph-1.novalocal_12560_object216benchmark_data_sds-ceph-1.novalocal_12560_object61benchmark_data_sds-ceph-1.novalocal_12560_object79benchmark_data_sds-ceph-1.novalocal_12560_object131benchmark_data_sds-ceph-1.novalocal_12560_object105benchmark_data_sds-ceph-1.novalocal_12560_object54benchmark_data_sds-ceph-1.novalocal_12560_object221benchmark_data_sds-ceph-1.novalocal_12560_object59benchmark_data_sds-ceph-1.novalocal_12560_object10benchmark_data_sds-ceph-1.novalocal_12560_object176benchmark_data_sds-ceph-1.novalocal_12560_object199benchmark_data_sds-ceph-1.novalocal_12560_object233benchmark_data_sds-ceph-1.novalocal_12560_object146benchmark_data_sds-ceph-1.novalocal_12560_object231benchmark_data_sds-ceph-1.novalocal_12560_object189benchmark_data_sds-ceph-1.novalocal_12560_object203benchmark_data_sds-ceph-1.novalocal_12560_object30benchmark_data_sds-ceph-1.novalocal_12560_object234benchmark_data_sds-ceph-1.novalocal_12560_object184benchmark_data_sds-ceph-1.novalocal_12560_object1benchmark_data_sds-ceph-1.novalocal_12560_object32benchmark_data_sds-ceph-1.novalocal_12560_object244benchmark_data_sds-ceph-1.novalocal_12560_object129benchmark_data_sds-ceph-1.novalocal_12560_object35benchmark_data_sds-ceph-1.novalocal_12560_object151benchmark_data_sds-ceph-1.novalocal_12560_object25benchmark_data_sds-ceph-1.novalocal_12560_object102benchmark_data_sds-ceph-1.novalocal_12560_object9benchmark_data_sds-ceph-1.novalocal_12560_object136benchmark_data_sds-ceph-1.novalocal_12560_object70benchmark_data_sds-ceph-1.novalocal_12560_object91benchmark_data_sds-ceph-1.novalocal_12560_object99benchmark_data_sds-ceph-1.novalocal_12560_object177benchmark_data_sds-ceph-1.novalocal_12560_object248benchmark_data_sds-ceph-1.novalocal_12560_object163benchmark_data_sds-ceph-1.novalocal_12560_object60benchmark_data_sds-ceph-1.novalocal_12560_object21benchmark_data_sds-ceph-1.novalocal_12560_object110benchmark_data_sds-ceph-1.novalocal_12560_object215benchmark_data_sds-ceph-1.novalocal_12560_object27benchmark_data_sds-ceph-1.novalocal_12560_object193benchmark_data_sds-ceph-1.novalocal_12560_object118benchmark_data_sds-ceph-1.novalocal_12560_object29benchmark_data_sds-ceph-1.novalocal_12560_object224benchmark_data_sds-ceph-1.novalocal_12560_object67benchmark_data_sds-ceph-1.novalocal_12560_object123benchmark_data_sds-ceph-1.novalocal_12560_object191benchmark_data_sds-ceph-1.novalocal_12560_object8benchmark_data_sds-ceph-1.novalocal_12560_object183benchmark_data_sds-ceph-1.novalocal_12560_object94benchmark_data_sds-ceph-1.novalocal_12560_object82benchmark_data_sds-ceph-1.novalocal_12560_object186benchmark_data_sds-ceph-1.novalocal_12560_object111benchmark_data_sds-ceph-1.novalocal_12560_object120benchmark_data_sds-ceph-1.novalocal_12560_object130benchmark_data_sds-ceph-1.novalocal_12560_object115benchmark_data_sds-ceph-1.novalocal_12560_object40benchmark_data_sds-ceph-1.novalocal_12560_object37benchmark_last_metadatabenchmark_data_sds-ceph-1.novalocal_12560_object84benchmark_data_sds-ceph-1.novalocal_12560_object232benchmark_data_sds-ceph-1.novalocal_12560_object47benchmark_data_sds-ceph-1.novalocal_12560_object68benchmark_data_sds-ceph-1.novalocal_12560_object48benchmark_data_sds-ceph-1.novalocal_12560_object145benchmark_data_sds-ceph-1.novalocal_12560_object20benchmark_data_sds-ceph-1.novalocal_12560_object75benchmark_data_sds-ceph-1.novalocal_12560_object66benchmark_data_sds-ceph-1.novalocal_12560_object83benchmark_data_sds-ceph-1.novalocal_12560_object223benchmark_data_sds-ceph-1.novalocal_12560_object114benchmark_data_sds-ceph-1.novalocal_12560_object7benchmark_data_sds-ceph-1.novalocal_12560_object187benchmark_data_sds-ceph-1.novalocal_12560_object2benchmark_data_sds-ceph-1.novalocal_12560_object58benchmark_data_sds-ceph-1.novalocal_12560_object222benchmark_data_sds-ceph-1.novalocal_12560_object249benchmark_data_sds-ceph-1.novalocal_12560_object137benchmark_data_sds-ceph-1.novalocal_12560_object178benchmark_data_sds-ceph-1.novalocal_12560_object159benchmark_data_sds-ceph-1.novalocal_12560_object24benchmark_data_sds-ceph-1.novalocal_12560_object213benchmark_data_sds-ceph-1.novalocal_12560_object181benchmark_data_sds-ceph-1.novalocal_12560_object108benchmark_data_sds-ceph-1.novalocal_12560_object246benchmark_data_sds-ceph-1.novalocal_12560_object148benchmark_data_sds-ceph-1.novalocal_12560_object64benchmark_data_sds-ceph-1.novalocal_12560_object103benchmark_data_sds-ceph-1.novalocal_12560_object109benchmark_data_sds-ceph-1.novalocal_12560_object107benchmark_data_sds-ceph-1.novalocal_12560_object132benchmark_data_sds-ceph-1.novalocal_12560_object78benchmark_data_sds-ceph-1.novalocal_12560_object98benchmark_data_sds-ceph-1.novalocal_12560_object89benchmark_data_sds-ceph-1.novalocal_12560_object142benchmark_data_sds-ceph-1.novalocal_12560_object158benchmark_data_sds-ceph-1.novalocal_12560_object100benchmark_data_sds-ceph-1.novalocal_12560_object144benchmark_data_sds-ceph-1.novalocal_12560_object168benchmark_data_sds-ceph-1.novalocal_12560_object139benchmark_data_sds-ceph-1.novalocal_12560_object250benchmark_data_sds-ceph-1.novalocal_12560_object194benchmark_data_sds-ceph-1.novalocal_12560_object182benchmark_data_sds-ceph-1.novalocal_12560_object55benchmark_data_sds-ceph-1.novalocal_12560_object41benchmark_data_sds-ceph-1.novalocal_12560_object143benchmark_data_sds-ceph-1.novalocal_12560_object165benchmark_data_sds-ceph-1.novalocal_12560_object169benchmark_data_sds-ceph-1.novalocal_12560_object241benchmark_data_sds-ceph-1.novalocal_12560_object192benchmark_data_sds-ceph-1.novalocal_12560_object147benchmark_data_sds-ceph-1.novalocal_12560_object239benchmark_data_sds-ceph-1.novalocal_12560_object95benchmark_data_sds-ceph-1.novalocal_12560_object57benchmark_data_sds-ceph-1.novalocal_12560_object155benchmark_data_sds-ceph-1.novalocal_12560_object42benchmark_data_sds-ceph-1.novalocal_12560_object106benchmark_data_sds-ceph-1.novalocal_12560_object198benchmark_data_sds-ceph-1.novalocal_12560_object218benchmark_data_sds-ceph-1.novalocal_12560_object15benchmark_data_sds-ceph-1.novalocal_12560_object63]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deploy k8s test]]></title>
    <url>%2F2018%2F11%2F24%2Fdeploy-k8s-test%2F</url>
    <content type="text"><![CDATA[部署kubernetes有多种方式： 1234567891011121、不借助任何工具，从基本的CLI操作开始部署k8s集群，可以学习k8s各个组件 https://github.com/kelseyhightower/kubernetes-the-hard-way2、部署单节点k8s集群，适合快速学习 https://github.com/kubernetes/minikube3、部署多节点k8s集群 https://github.com/kubernetes/kubeadm4、在cloud上部署k8s集群 https://github.com/kubernetes/kops5、coreos的工具，大于十个节点收费，小于等于十个节点免费 https://coreos.com/tectonic/6、实验环境 https://labs.play-with-k8s.com/ 安装minikubemac1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071721、安装kubectl，文档https://kubernetes.io/docs/tasks/tools/install-kubectl/yujiangdeMBP-13:~ yujiang$ brew install kubernetes-cli2、安装minikube，文档https://github.com/kubernetes/minikube/releases0.32.0版本在minikube start时遇到了Starting cluster components失败，在网上查找资料需要降级0.25.2。如果想使用新版本，还是找台虚拟机试一下。yujiangdeMBP-13:~ yujiang$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.25.2/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo cp minikube /usr/local/bin/ &amp;&amp; rm minikube如果已经安装了0.32.0版本的minikube，降级方法请看链接：https://coding.m.imooc.com/questiondetail.html?qid=577843、安装virtualbox4、启动minikube单节点k8s(https://www.jianshu.com/p/a7620f73c7f3)yujiangdeMBP-13:~ yujiang$ minikube start Starting local Kubernetes v1.9.4 cluster...Starting VM...Getting VM IP address...Moving files into cluster...Setting up certs...Connecting to cluster...Setting up kubeconfig...Starting cluster components...Kubectl is now configured to use the cluster.Loading cached images from config file.=====================================以下为测试，有坑，不需要执行。yujiangdeMBP-13:~ yujiang$ rm -rf ~/.minikube/machines/minikube/yujiangdeMBP-13:~ yujiang$ minikube startyujiangdeMBP-13:~ yujiang$ minikube start --kubernetes-version v1.8.0 --bootstrapper kubeadm=====================================以上为测试，有坑，不需要执行。minikube start命令创建一个名为“minikube”的“kubectl context”。这个context包含与Minikube群集通信的配置。Minikube会自动将此context设置为默认值，但如果您以后需要切换回它，请运行：kubectl config use-context minikube,或者传递每个命令的context，如下所示：kubectl get pods --context=minikube。可以使用不同的context连接不同的k8s集群。查看当前configyujiangdeMBP-13:~ yujiang$ kubectl config viewapiVersion: v1clusters:- cluster: certificate-authority: /Users/yujiang/.minikube/ca.crt server: https://192.168.99.101:8443 name: minikubecontexts:- context: cluster: minikube user: minikube name: minikubecurrent-context: minikubekind: Configpreferences: &#123;&#125;users:- name: minikube user: client-certificate: /Users/yujiang/.minikube/client.crt client-key: /Users/yujiang/.minikube/client.key查看当前contextyujiangdeMBP-13:~ yujiang$ kubectl config get-contextsCURRENT NAME CLUSTER AUTHINFO NAMESPACE* minikube minikube minikube 查看当前k8s集群情况yujiangdeMBP-13:~ yujiang$ kubectl cluster-infoKubernetes master is running at https://192.168.99.100:8443进入minikube虚拟机yujiangdeMBP-13:~ yujiang$ minikube ssh设置代理（ss-ng），否则无法pull docker镜像(https://blog.zhesih.com/2018/06/24/k8s-minikube-setup/)$ sudo vi /etc/systemd/system/docker.service.d/http-proxy.conf$ sudo systemctl daemon-reload$ sudo systemctl restart docker$ systemctl show --property=Environment dockerEnvironment=DOCKER_RAMDISK=yes HTTP_PROXY=http://192.168.199.165:1087 HTTPS_PROXY=https://192.168.199.165:1087 实验一（kind: Pod）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142Pod是k8s调度的最小单位，一个Pod中可以有多个Container1、创建一个PodyujiangdeMBP-13:k8s_yaml yujiang$ cat pod_nginx.yml apiVersion: v1kind: Podmetadata: name: nginx labels: app: nginxspec: containers: - name: nginx image: nginx ports: - containerPort: 80yujiangdeMBP-13:k8s_yaml yujiang$ kubectl create -f pod_nginx.yml pod/nginx created2、查看PodyujiangdeMBP-13:k8s_yaml yujiang$ kubectl get pod --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEdefault nginx 1/1 Running 0 20mkube-system kube-addon-manager-minikube 1/1 Running 8 60mkube-system kube-dns-54cccfbdf8-kr847 3/3 Running 12 57mkube-system kubernetes-dashboard-77d8b98585-vq77b 1/1 Running 4 57mkube-system storage-provisioner 1/1 Running 5 57m3、查看Pod的详细信息，可以看到Pod运行在哪台机器上yujiangdeMBP-13:k8s_yaml yujiang$ kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx 1/1 Running 0 31m 172.17.0.2 minikube &lt;none&gt; &lt;none&gt;4、打开dashboardyujiangdeMBP-13:k8s_yaml yujiang$ minikube dashboard5、查询nginx IP$ docker network lsNETWORK ID NAME DRIVER SCOPEfb08befd952b bridge bridge local2de0a434731f host host local4cd7f4c7083c none null local$ docker network inspect bridge[ &#123; &quot;Name&quot;: &quot;bridge&quot;, &quot;Id&quot;: &quot;fb08befd952b4448e86e491f271a1e6a50056b5a18bdfb8a3e930edde2ac44da&quot;, &quot;Created&quot;: &quot;2018-12-24T17:23:11.618994916Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: null, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; &quot;25e63a3ff8a091319fdec698f554f1b47c3e019fbdda9b06709d15630003ee6e&quot;: &#123; &quot;Name&quot;: &quot;k8s_POD_kube-dns-54cccfbdf8-kr847_kube-system_69d32f1a-0799-11e9-be08-080027bea66e_8&quot;, &quot;EndpointID&quot;: &quot;004b9a81ab36cd4e0aba21572b4736b8d85d5541c3533571a67383e5d07e34b6&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:03&quot;, &quot;IPv4Address&quot;: &quot;172.17.0.3/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125;, &quot;4138eeb8348781495987d47f6683138515111d2b6afa6fc7352d5f98d4a0858b&quot;: &#123; &quot;Name&quot;: &quot;k8s_POD_kubernetes-dashboard-77d8b98585-vq77b_kube-system_69b46ed4-0799-11e9-be08-080027bea66e_8&quot;, &quot;EndpointID&quot;: &quot;bb31db755c5a5cd8f36bec35d89c73267d3d283ba8caa691b58ba504b0d6fabc&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:04&quot;, &quot;IPv4Address&quot;: &quot;172.17.0.4/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125;, &quot;6874ee85f9257b5b0dfdf808ebf92df9c3c677079a778980956e4655abf23507&quot;: &#123; &quot;Name&quot;: &quot;k8s_POD_nginx_default_9e0d9ffc-079e-11e9-be08-080027bea66e_0&quot;, &quot;EndpointID&quot;: &quot;040a5616a76665ff9e35bb6cef2388f8db443b2db94b67282a7496b9803acc20&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;IPv4Address&quot;: &quot;172.17.0.2/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123; &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;, &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;, &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot; &#125;, &quot;Labels&quot;: &#123;&#125; &#125;]$ curl 172.17.0.2&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;6、指定Pod名，进入Container(默认进入第1个Container)。如果Pod中有多个Container，可以加&quot;-c&quot;参数指定进入哪个Container。yujiangdeMBP-13:k8s_yaml yujiang$ kubectl exec -it nginx sh7、打印nginx Pod的详细信息yujiangdeMBP-13:k8s_yaml yujiang$ kubectl describe pod nginx8、怎样把nginx的端口映射出来？有两种方法： (1)port-forward yujiangdeMBP-13:k8s_yaml yujiang$ kubectl port-forward nginx 8080:80 打开浏览器：http://127.0.0.1:8080/ 实验二（kind: ReplicationController）12345678910111213141516171819202122232425262728293031323334353637383940414243yujiangdeMBP-13:k8s_yaml yujiang$ cat rc_nginx.yml apiVersion: v1kind: ReplicationControllermetadata: name: nginxspec: replicas: 3 selector: app: nginx template: metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80yujiangdeMBP-13:k8s_yaml yujiang$ kubectl create -f rc_nginx.yml replicationcontroller/nginx createdyujiangdeMBP-13:k8s_yaml yujiang$ kubectl get pod NAME READY STATUS RESTARTS AGEnginx-prlfw 1/1 Running 0 19snginx-szrh8 1/1 Running 0 19snginx-z59kd 1/1 Running 0 19s# 修改横向扩展数量yujiangdeMBP-13:k8s_yaml yujiang$ kubectl scale rc nginx --replicas=2replicationcontroller/nginx scaledyujiangdeMBP-13:k8s_yaml yujiang$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx-szrh8 1/1 Running 1 21hnginx-z59kd 1/1 Running 1 21hyujiangdeMBP-13:k8s_yaml yujiang$ kubectl get rcNAME DESIRED CURRENT READY AGEnginx 2 2 2 21hyujiangdeMBP-13:k8s_yaml yujiang$ kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-szrh8 1/1 Running 1 21h 172.17.0.4 minikube &lt;none&gt; &lt;none&gt;nginx-z59kd 1/1 Running 1 21h 172.17.0.6 minikube &lt;none&gt; &lt;none&gt; 实验三（kind: ReplicaSet）1234567891011121314151617181920212223242526272829303132333435363738apiVersion: apps/v1kind: ReplicaSetmetadata: name: nginx labels: tier: frontendspec: replicas: 3 selector: matchLabels: tier: frontend template: metadata: name: nginx labels: tier: frontend spec: containers: - name: nginx image: nginx ports: - containerPort: 80yujiangdeMBP-13:k8s_yaml yujiang$ kubectl create -f rs_nginx.yml replicaset.apps/nginx createdyujiangdeMBP-13:k8s_yaml yujiang$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx 3 3 3 52syujiangdeMBP-13:k8s_yaml yujiang$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx-brtzk 1/1 Running 0 81snginx-m5xgq 1/1 Running 0 81snginx-qvdq6 1/1 Running 0 81syujiangdeMBP-13:k8s_yaml yujiang$ kubectl scale rs nginx --replicas=2replicaset.extensions/nginx scaledyujiangdeMBP-13:k8s_yaml yujiang$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx 2 2 2 2m32s 实验四（kind: Deployment）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586yujiangdeMBP-13:k8s_yaml yujiang$ cat deployment_nginx.yml apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.12.2 ports: - containerPort: 80yujiangdeMBP-13:k8s_yaml yujiang$ kubectl create -f deployment_nginx.ymldeployment.apps/nginx-deployment createdyujiangdeMBP-13:k8s_yaml yujiang$ kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEnginx-deployment 3/3 3 3 5m31syujiangdeMBP-13:k8s_yaml yujiang$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deployment-9898d9674 3 3 3 6m6syujiangdeMBP-13:k8s_yaml yujiang$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx-deployment-9898d9674-2mrdl 1/1 Running 0 6m31snginx-deployment-9898d9674-ndblx 1/1 Running 0 6m31snginx-deployment-9898d9674-t8ngw 1/1 Running 0 6m31syujiangdeMBP-13:k8s_yaml yujiang$ kubectl get deployment -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnginx-deployment 3/3 3 3 7m31s nginx nginx:1.12.2 app=nginx# 更新nginxyujiangdeMBP-13:k8s_yaml yujiang$ kubectl set image deployment nginx-deployment nginx=nginx:1.13deployment.extensions/nginx-deployment image updatedyujiangdeMBP-13:k8s_yaml yujiang$ kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEnginx-deployment 3/3 3 3 12myujiangdeMBP-13:k8s_yaml yujiang$ kubectl get deployment -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnginx-deployment 3/3 3 3 12m nginx nginx:1.13 app=nginxyujiangdeMBP-13:k8s_yaml yujiang$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deployment-9898d9674 0 0 0 13mnginx-deployment-d8d99448f 3 3 3 3m42syujiangdeMBP-13:k8s_yaml yujiang$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx-deployment-d8d99448f-fzfdw 1/1 Running 0 3m59snginx-deployment-d8d99448f-l8m8w 1/1 Running 0 72snginx-deployment-d8d99448f-pbnml 1/1 Running 0 73s# 回滚到上一次更新前yujiangdeMBP-13:k8s_yaml yujiang$ kubectl rollout history deployment nginx-deploymentdeployment.extensions/nginx-deployment REVISION CHANGE-CAUSE1 &lt;none&gt;2 &lt;none&gt;yujiangdeMBP-13:k8s_yaml yujiang$ kubectl rollout undo deployment nginx-deploymentdeployment.extensions/nginx-deployment rolled backyujiangdeMBP-13:k8s_yaml yujiang$ kubectl get deployment -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnginx-deployment 3/3 3 3 15m nginx nginx:1.12.2 app=nginxyujiangdeMBP-13:k8s_yaml yujiang$ kubectl rollout history deployment nginx-deploymentdeployment.extensions/nginx-deployment REVISION CHANGE-CAUSE2 &lt;none&gt;3 &lt;none&gt;# 暴露nginx Container端口到宿主机(node节点)yujiangdeMBP-13:k8s_yaml yujiang$ kubectl get node -o wideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEminikube Ready &lt;none&gt; 23h v1.9.4 192.168.99.100 &lt;none&gt; Buildroot 2017.11 4.9.64 docker://17.9.0yujiangdeMBP-13:k8s_yaml yujiang$ kubectl expose deployment nginx-deployment --type=NodePort service/nginx-deployment exposedyujiangdeMBP-13:k8s_yaml yujiang$ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 23hnginx-deployment NodePort 10.98.51.33 &lt;none&gt; 80:30233/TCP 68s# 打开浏览器http://192.168.99.100:30233/ 实验五（kubectl自动补全）1https://kubernetes.io/docs/tasks/tools/install-kubectl/ kubeadm12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# 1、创建kubernetes虚拟机，并关闭防火墙，关闭SELinux(所有节点执行)systemctl disable firewalld.service &amp;&amp; systemctl stop firewalld.servicesetenforce 0# 2、cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --systemyum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum makecacheyum install -y https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpmyum install -y docker-ce-&lt;17.03.1.ce-1.el7.centos&gt; systemctl enable docker.servicesystemctl start docker.servicecat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpghttp://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum install -y kubelet-1.11.0-0 kubeadm-1.11.0-0 kubectl-1.11.0-0systemctl enable kubelet.servicesystemctl start kubelet.servicevi /etc/sysconfig/kubeletKUBELET_EXTRA_ARGS=&quot;--cgroup-driver=cgroupfs --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1&quot;systemctl daemon-reloadsystemctl restart kubelet.servicedocker pull quay.io/calico/typha:v0.7.4docker pull quay.io/calico/node:v3.1.3docker pull quay.io/calico/cni:v3.1.3vi kubeadm-master.configapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.0imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapi: advertiseAddress: &lt;10.0.90.217&gt;controllerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: &lt;10.211.0.0&gt;/16 serviceSubnet: &lt;10.96.0.0&gt;/16kubeProxy: config: mode: iptableskubeadm config images pull --config kubeadm-master.config# 执行此CLI会输出token，需要保存该tokenkubeadm init --config kubeadm-master.configkubeadm resetswapoff -asudo systemctl status kubelet.servicekubectl get pod --all-namespaces -o wide添加nodekubeadm join 192.168.56.201:6443 --token mzbbw0.5z6zpgauylva58na --discovery-token-ca-cert-hash sha256:932a24f271bd2c14cb41d6698e9781c1fcede848b7b9fe4a0731b9c54f275df2 Vagrantfile1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495yujiangdeMacBook-Pro-13:docker-k8s yujiang$ cat Vagrantfile # -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.require_version &quot;&gt;= 1.6.0&quot;boxes = [ &#123; :name =&gt; &quot;k8s-node-1&quot;, :eth1 =&gt; &quot;192.168.56.61&quot;, :mem =&gt; &quot;2048&quot;, :cpu =&gt; &quot;2&quot; &#125;, &#123; :name =&gt; &quot;k8s-node-2&quot;, :eth1 =&gt; &quot;192.168.56.62&quot;, :mem =&gt; &quot;2048&quot;, :cpu =&gt; &quot;2&quot; &#125;,]Vagrant.configure(2) do |config| config.vm.box = &quot;centos/centos7&quot; boxes.each do |opts| config.vm.define opts[:name] do |config| config.vm.hostname = opts[:name] config.vm.provider &quot;vmware_fusion&quot; do |v| v.vmx[&quot;memsize&quot;] = opts[:mem] v.vmx[&quot;numvcpus&quot;] = opts[:cpu] end config.vm.provider &quot;virtualbox&quot; do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--memory&quot;, opts[:mem]] v.customize [&quot;modifyvm&quot;, :id, &quot;--cpus&quot;, opts[:cpu]] end config.vm.network :private_network, ip:opts[:eth1] end end config.vm.provision &quot;shell&quot;, inline: &lt;&lt;-SHELL # install dependent systemctl disable firewalld.service &amp;&amp; systemctl stop firewalld.service &amp;&amp; setenforce 0 sed -i &quot;s/\(SELINUX=\).*/\1disabled/g&quot; /etc/selinux/config # close swapoff swapoff -a # install docker cat &lt;&lt;-&apos;EOF&apos; &gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF modprobe br_netfilter sysctl -p /etc/sysctl.d/k8s.conf yum install -y ipset ipvsadm cat &lt;&lt;-&apos;EOF&apos; &gt; /etc/sysconfig/modules/ipvs.modules \#!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4 yum install -y yum-utils device-mapper-persistent-data lvm2 &amp;&amp; yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo &amp;&amp; yum makecache &amp;&amp; yum install -y https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm &amp;&amp; yum install -y docker-ce-17.03.1.ce-1.el7.centos systemctl enable docker.service &amp;&amp; systemctl start docker.service # install k8s cat &lt;&lt;-&apos;EOF&apos; &gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum install -y kubelet-1.11.0-0 kubeadm-1.11.0-0 kubectl-1.11.0-0 cat &lt;&lt;-&apos;EOF&apos; &gt; /etc/sysconfig/kubelet KUBELET_EXTRA_ARGS=&quot;--cgroup-driver=cgroupfs --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1&quot; EOF systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service docker pull quay.io/calico/typha:v0.7.4 docker pull quay.io/calico/node:v3.1.3 docker pull quay.io/calico/cni:v3.1.3 SHELLend]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx rtmp]]></title>
    <url>%2F2018%2F11%2F13%2Fnginx-rtmp%2F</url>
    <content type="text"><![CDATA[编译安装1234567891011121314151617181920212223242526克隆nginx项目与nginx-rtmp-module[root@cephJ ~]# yum install gcc pcre-devel openssl-devel epel-release -y[root@cephJ ~]# rpm --import http://li.nux.ro/download/nux/RPM-GPG-KEY-nux.ro[root@cephJ ~]# rpm -Uvh http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-5.el7.nux.noarch.rpm[root@cephJ ~]# yum -y install ffmpeg ffmpeg-devel[root@cephJ ~]# cd github/[root@cephJ github]# git clone https://github.com/nginx/nginx.git[root@cephJ github]# cd nginx[root@cephJ github]# git checkout -b myrelease-1.15.6 release-1.15.6[root@cephJ github]# git clone https://github.com/arut/nginx-rtmp-module.git[root@cephJ github]# cd nginx-rtmp-module[root@cephJ github]# git checkout -b myv1.2.1 v1.2.1[root@cephJ ~]# tree github/ -L 1github/├── nginx└── nginx-rtmp-module[root@cephJ ~]# cd github/nginx[root@cephJ nginx]# ./auto/configure --prefix=/usr/bin/ --add-module=../nginx-rtmp-module/ --with-http_ssl_module --with-debug[root@cephJ nginx]# make -j 8[root@cephJ nginx]# make install 配置nginx12345678910111213141516171819[root@cephJ ~]# vim /usr/local/nginx/conf/nginx.conf# 添加如下配置rtmp &#123; # RTMP服务 server &#123; listen 1935; # 服务端口 chunk_size 4096; # 数据传输块的大小 application vod &#123; play /root/videos/; # 视频文件存放位置 &#125; &#125;&#125;启动nginx[root@cephJ ~]# /usr/local/nginx/sbin/nginx[root@cephJ ~]# ps -ef | grep nginxroot 20015 1 0 11:24 ? 00:00:00 nginx: master process /usr/local/nginx/sbin/nginxnobody 20016 20015 0 11:24 ? 00:00:00 nginx: worker processroot 20019 12686 0 11:24 pts/2 00:00:00 grep --color=auto nginx 概念FFmpegFFmpeg 是一个库和工具的集合，用于处理多媒体，如音频、视频、字幕和相关元数据。（FF指的是Fast Forward ——快速前进） 123相关链接：【1】https://zh.wikipedia.org/wiki/FFmpeg【2】https://github.com/FFmpeg/FFmpeg]]></content>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Generic Block Device Tuning Parameters - read_ahead_kb]]></title>
    <url>%2F2018%2F10%2F29%2FGeneric-Block-Device-Tuning-Parameters-read-ahead-kb%2F</url>
    <content type="text"><![CDATA[read_ahead_kb 定义sequential read operation期间OS可以预读的最大千字节数（kb）。因此，可能需要的信息已存在于kernel page cache中，以便进行下一次sequential read，从而提高read I/O性能。 Device mappers通常受益于高read_ahead_kb值。每个要mapped的设备128 KB是一个很好的起点，但将read_ahead_kb值增加到4-8 MB可能会提高sequential read大文件应用程序的性能。]]></content>
      <tags>
        <tag>Generic Block Device Tuning Parameters</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[慕课网-使用Beego构建完整web项目]]></title>
    <url>%2F2018%2F10%2F28%2F%E6%85%95%E8%AF%BE%E7%BD%91-%E4%BD%BF%E7%94%A8Beego%E6%9E%84%E5%BB%BA%E5%AE%8C%E6%95%B4web%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[课程介绍 go语言简介 beego框架介绍 beego环境搭建 应用实践 —— 看图猜电影项目 项目部署与发布 项目总结 go简介 语法简明紧凑 语法一致 噪音少 简洁的并发 go在语言级并发抽象 简洁优雅 执行速度与开发效率兼优 编译型语言 语法简明 环境搭建beego安装 beego的安装 1yujiangdeMBP-13:~ yujiang$ go get -u -v github.com/astaxie/beego bee工具安装 bee工具是什么？ ​ 一个工具包，为了协助快速开发beego项目而创建的项目。（beego项目创建、热编译、开发测试、部署） 12345678910111213141516171819yujiangdeMBP-13:~ yujiang$ go get -u -v github.com/beego/beeyujiangdeMBP-13:~ yujiang$ bee version______| ___ \| |_/ / ___ ___| ___ \ / _ \ / _ \| |_/ /| __/| __/\____/ \___| \___| v1.10.0├── Beego : 1.10.1├── GoVersion : go1.10├── GOOS : darwin├── GOARCH : amd64├── NumCPU : 4├── GOPATH : /Users/yujiang/go├── GOROOT : /usr/local/go├── Compiler : gc└── Date : Sunday, 28 Oct 2018 beego框架介绍特点 快速开发 MVC架构 文档齐全，社区活跃 bee工具应用 bee new：新建项目结构 bee run：自动编译部署 bee generate：自动生成代码 12345678910111213141516171819202122232425yujiangdeMBP-13:~ yujiang$ bee new imoocyujiangdeMBP-13:imooc yujiang$ bee run浏览器输入 http://localhost:8080/yujiangdeMBP-13:imooc yujiang$ tree.├── conf│ └── app.conf├── controllers│ └── default.go├── main.go├── models├── routers│ └── router.go├── static│ ├── css│ ├── img│ └── js│ └── reload.min.js├── tests│ └── default_test.go└── views └── index.tpl10 directories, 7 files beego项目运行基本流程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051文件 - main.gopackage mainimport ( _ &quot;imooc/routers&quot; // _表示只执行imooc/routers的init()函数 &quot;github.com/astaxie/beego&quot;)func main() &#123; beego.Run()&#125;文件 - routers/router.gopackage routersimport ( &quot;imooc/controllers&quot; &quot;github.com/astaxie/beego&quot;)func init() &#123; // beego.Router(&quot;/&quot;, &amp;controllers.MainController&#123;&#125;)&#125;文件 - controllers/default.gopackage controllersimport ( &quot;github.com/astaxie/beego&quot;)type MainController struct &#123; beego.Controller&#125;func (c *MainController) Get() &#123; // 这里就是处理get请求的逻辑 c.Data[&quot;Website&quot;] = &quot;beego.me&quot; // 向模板传递的数据 c.Data[&quot;Email&quot;] = &quot;astaxie@gmail.com&quot; // 向模板传递的数据 c.TplName = &quot;index.tpl&quot; // 渲染的模板文件&#125;beego通过浏览器传递参数的方法：（http://localhost:8080/?name=&quot;yujiang&quot;）package controllersimport ( &quot;github.com/astaxie/beego&quot;)type MainController struct &#123; beego.Controller&#125;func (c *MainController) Get() &#123; name := c.GetString(&quot;name&quot;) //c.Data[&quot;Website&quot;] = &quot;beego.me&quot; c.Data[&quot;Website&quot;] = name c.Data[&quot;Email&quot;] = &quot;astaxie@gmail.com&quot; c.TplName = &quot;index.tpl&quot;&#125; beego如何进行数据交互12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091[root@localhost ~]# yum install mariadb-server -y[root@localhost ~]# systemctl start mariadb[root@localhost ~]# systemctl enable mariadb[root@localhost ~]# mysql -u root -pMariaDB [(none)]&gt; create database imooc;Query OK, 1 row affected (0.00 sec)MariaDB [(none)]&gt; use imooc;MariaDB [imooc]&gt; CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(128) NOT NULL DEFAULT &apos;&apos;, `gender` tinyint(4) NOT NULL DEFAULT &apos;0&apos;, `age` int(11) NOT NULL DEFAULT &apos;0&apos;, PRIMARY KEY (`id`) )ENGINE=InnoDB DEFAULT CHARSET=utf8;Query OK, 0 rows affected (0.00 sec)MariaDB [imooc]&gt; INSERT INTO user (name,gender,age) values(&apos;zhangsan&apos;,1,21);Query OK, 1 row affected (0.00 sec)MariaDB [imooc]&gt; INSERT INTO user (name,gender,age) values(&apos;lisi&apos;,0,22);Query OK, 1 row affected (0.01 sec)MariaDB [imooc]&gt; INSERT INTO user (name,gender,age) values(&apos;wangwu&apos;,1,20);Query OK, 1 row affected (0.00 sec)MariaDB [imooc]&gt; select * from user;+----+----------+--------+-----+| id | name | gender | age |+----+----------+--------+-----+| 1 | zhangsan | 1 | 21 || 2 | lisi | 0 | 22 || 3 | wangwu | 1 | 20 |+----+----------+--------+-----+3 rows in set (0.00 sec)生成代码yujiangdeMBP-13:imooc yujiang$ bee generate scaffold user -fields=&quot;id:int64,name:string,gender:int,age:int&quot; -driver=mysql -conn=&quot;root:@tcp(192.168.56.101:3306)/imooc&quot;______| ___ \| |_/ / ___ ___| ___ \ / _ \ / _ \| |_/ /| __/| __/\____/ \___| \___| v1.10.02018/10/28 19:30:25 INFO ▶ 0001 Do you want to create a &apos;user&apos; model? [Yes|No] Yes2018/10/28 19:31:34 INFO ▶ 0002 Using &apos;User&apos; as model name2018/10/28 19:31:34 INFO ▶ 0003 Using &apos;models&apos; as package name create /Users/yujiang/go/src/imooc/models/user.go2018/10/28 19:31:34 INFO ▶ 0004 Do you want to create a &apos;user&apos; controller? [Yes|No] Yes2018/10/28 19:31:43 INFO ▶ 0005 Using &apos;User&apos; as controller name2018/10/28 19:31:43 INFO ▶ 0006 Using &apos;controllers&apos; as package name2018/10/28 19:31:43 INFO ▶ 0007 Using matching model &apos;User&apos; create /Users/yujiang/go/src/imooc/controllers/user.go2018/10/28 19:31:43 INFO ▶ 0008 Do you want to create views for this &apos;user&apos; resource? [Yes|No] Yes2018/10/28 19:31:52 INFO ▶ 0009 Generating view... create /Users/yujiang/go/src/imooc/views/user/index.tpl create /Users/yujiang/go/src/imooc/views/user/show.tpl create /Users/yujiang/go/src/imooc/views/user/create.tpl create /Users/yujiang/go/src/imooc/views/user/edit.tpl2018/10/28 19:31:52 INFO ▶ 0010 Do you want to create a &apos;user&apos; migration and schema for this resource? [Yes|No] No2018/10/28 19:32:11 INFO ▶ 0011 Do you want to migrate the database? [Yes|No] No2018/10/28 19:32:13 SUCCESS ▶ 0012 All done! Don&apos;t forget to add beego.Router(&quot;/user&quot; ,&amp;controllers.UserController&#123;&#125;) to routers/route.go2018/10/28 19:32:13 SUCCESS ▶ 0013 Scaffold successfully generated!yujiangdeMBP-13:imooc yujiang$ tree.├── conf│ └── app.conf├── controllers│ ├── default.go│ └── user.go├── imooc├── main.go├── models│ └── user.go├── routers│ └── router.go├── static│ ├── css│ ├── img│ └── js│ └── reload.min.js├── tests│ └── default_test.go└── views ├── index.tpl └── user ├── create.tpl ├── edit.tpl ├── index.tpl └── show.tpl11 directories, 14 files 安装go-sql-driver 123456789101112131415161718192021222324252627282930313233343536373839404142yujiangdeMBP-13:~ yujiang$ go get -v -u github.com/go-sql-driver/mysql文件 - routers/router.gopackage routersimport ( &quot;github.com/astaxie/beego&quot; &quot;imooc/controllers&quot;)func init() &#123; //beego.Router(&quot;/&quot;, &amp;controllers.MainController&#123;&#125;) beego.Include(&amp;controllers.UserController&#123;&#125;)&#125;文件 - main.gopackage mainimport ( _ &quot;imooc/routers&quot; &quot;github.com/astaxie/beego&quot; &quot;github.com/astaxie/beego/orm&quot; _ &quot;github.com/go-sql-driver/mysql&quot;)func main() &#123; orm.RegisterDataBase(&quot;default&quot;,&quot;mysql&quot;,&quot;root:root@tcp(192.168.56.101:3306)/imooc&quot;) beego.Run()&#125;yujiangdeMBP-13:imooc yujiang$ bee run______| ___ \| |_/ / ___ ___| ___ \ / _ \ / _ \| |_/ /| __/| __/\____/ \___| \___| v1.10.02018/10/28 20:31:44 INFO ▶ 0001 Using &apos;imooc&apos; as &apos;appname&apos;2018/10/28 20:31:44 INFO ▶ 0002 Initializing watcher...imooc2018/10/28 20:31:46 SUCCESS ▶ 0003 Built Successfully!2018/10/28 20:31:46 INFO ▶ 0004 Restarting &apos;imooc&apos;...2018/10/28 20:31:46 SUCCESS ▶ 0005 &apos;./imooc&apos; is running...2018/10/28 20:31:46.697 [I] [router.go:269] /Users/yujiang/go/src/imooc/controllers no changed2018/10/28 20:31:46.719 [I] [asm_amd64.s:2361] http server Running on http://:80802018/10/28 20:31:55.475 [D] [server.go:2694] | ::1| 200 | 3.979416ms| match| GET / r:/ 应用实践]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Ceph Benchmarking Tool]]></title>
    <url>%2F2018%2F10%2F16%2FThe-Ceph-Benchmarking-Tool%2F</url>
    <content type="text"><![CDATA[CBT - The Ceph Benchmarking ToolINTRODUCTION（介绍）CBT是一个用python编写的测试工具，可以自动执行并测试Ceph集群性能相关的任务。CBT不安装Ceph软件包，这应该在使用CBT之前完成。CBT可以在测试运行开始时创建 OSDs，也可以在测试运行期间重新创建OSD，或者只针对现有集群运行。CBT使用collectl记录系统指标，它可以选择使用perf，blktrace和valgrind在内的多种工具收集更多信息。除基本基准测试外，CBT还可以进行高级测试，包括自动OSD中断，erasure coded pools和cache tier配置。主要基准模块解释如下。 radosbenchRADOS基准测试使用ceph-common软件包附带的rados二进制文件。它包含一个基准测试工具，它通过librados来运行集群，librados是Ceph提供的底层object storage API。目前，RADOS基准模块为每个client创建一个pool。 librbdfiolibrbdfio基准模块是测试Ceph集群的块存储性能的最简单方法。最新版本的flexible IO tester (fio)提供了RBD引擎。这允许fio通过用户态librbd库测试RBD volumes的块存储性能，而无需配置KVM/QEMU。这些库与QEMU后端使用的库相同，因此它很近似于KVM/QEMU性能。 kvmrbdfiokvmrbdfio基准测试使用flexible IO tester (fio) 来运行已attache到KVM instance的RBD volume。它要求在使用CBT之前创建instance，并attache RBD volumes。此模块用于对通过Cinder创建的RBD并attache到OpenStack instance进行基准测试。或者，可以使用Vagrant或Virtual Machine Manager来提供instance。 rbdfiorbdfio基准测试使用flexible IO tester (fio) 来执行使用KRBD内核驱动映射到块设备的RBD volume。此模块最适用于模拟需要直接使用块设备的应用程序，而不是在虚拟机内运行。 PREREQUISITES（必备条件）CBT使用多个库和工具来运行： python-yaml - 用于读取配置文件的python的YAML库。 python-lxml - 功能强大的Pythonic XML处理库，将libxml2/libxslt与ElementTree API相结合 ssh (and scp) - 远程安全命令执行和数据传输 pdsh (and pdcp) - 并行ssh和scp实现 ceph - 可扩展的分布式存储系统 请注意，目前没有为基于RHEL7和CentOS 7发行版的pdsh。这些RPM包可在此处获得： ftp://rpmfind.net/linux/fedora/linux/releases/23/Everything/x86_64/os/Packages/p/pdsh-2.31-4.fc23.x86_64.rpm ftp://rpmfind.net/linux/fedora/linux/releases/23/Everything/x86_64/os/Packages/p/pdsh-rcmd-rsh-2.31-4.fc23.x86_64.rpm ftp://rpmfind.net/linux/fedora/linux/releases/23/Everything/x86_64/os/Packages/p/pdsh-rcmd-ssh-2.31-4.fc23.x86_64.rpm 如果需要，可以使用可选工具和基准： collectl - 系统数据收集 blktrace - 块设备IO跟踪 seekwatcher - 通过blktrace数据创建图表 perf - 系统和进程分析 valgrind - 特定进程的runtime memory和cpu分析 fio - 基准suite，集成了posix，libaio和librbd的支持 cosbench - Intel的对象存储基准 USER AND NODE SETUP（用户和节点设置）除上述软件外，还必须有许多节点才能运行测试。这些节点可以划分为多个类别。多个类别可以包含相同的host，如果它承担多个角色（例如，运行OSD和mon）。 head - 运行常规ceph命令的节点 clients - 运行基准测试或其他客户端工具的节点 osds - OSD存在的节点 rgws - rgw servers节点 mons - mons节点 还可以指定用户运行所有远程命令。用于运行cbt的主机必须能够以指定用户身份发出无密码ssh命令。这可以通过创建无密码ssh密钥来完成： 1ssh-keygen -t dsa 并将~/.ssh中生成的公钥复制到所有远程主机上的~/.ssh/authorized_key文件中。 此用户还必须能够使用sudo运行某些命令。实现此目的的最简单方法是为该用户简单地启用全局无密码sudo访问，但这仅适用于实验室环境。这可以通过运行visudo并添加如下内容来实现： 12# passwordless sudo for cbt&lt;user&gt; ALL=(ALL) NOPASSWD:ALL 其中&lt;user&gt;是将具有密码sudo访问权限的用户。 有关具体细节，请参阅操作系统文档。 除了上述内容之外，还需要将所有osds和mons添加到ssh的已知主机列表中（/etc/hosts文件）才能正常执行。 否则，基准测试将无法运行。 请注意，如果sudoers文件需要tty，则pdsh命令可能会有困难。如果是这种情况，推荐在visudo中添加Defaults requiretty行。 DISK PARTITIONING（磁盘分区）目前，CBT为Ceph OSD data和journal分区在/dev/disk/by-partlabel中寻找指定分区labels。在未来的某个时刻，这将变得更加灵活，因为现在这是预期的行为。每个OSD主机分区应该用下面的gpt labels指定： 12osd-device-&lt;num&gt;-dataosd-device-&lt;num&gt;-journal 其&lt;num&gt;是从0开始并以系统上最后一个设备结束的device序列。目前，cbt假设系统中的所有节点具有相同数量的device。有一个脚本可以显示我们在测试环境中如何创建分区labels的示例： 1https://github.com/ceph/cbt/blob/master/tools/mkpartmagna.sh CREATING A YAML FILE（创建一个YAML文件）CBT yaml文件具有一个基本结构，您可以在其中定义一个cluster和一组针对它运行的基准测试。例如, 下面的 yaml 文件在具有主机名 “burnupiX “ 的节点上创建单节点群集。使用256个PG为1x replication pool定义pool的配置文件，并且该pool用于使用带有librbd引擎的fio运行RBD性能测试。 123456789101112131415161718192021222324252627282930313233cluster: user: &apos;nhm&apos; head: &quot;burnupiX&quot; clients: [&quot;burnupiX&quot;] osds: [&quot;burnupiX&quot;] mons: burnupiX: a: &quot;127.0.0.1:6789&quot; osds_per_node: 1 fs: &apos;xfs&apos; mkfs_opts: &apos;-f -i size=2048&apos; mount_opts: &apos;-o inode64,noatime,logbsize=256k&apos; conf_file: &apos;/home/nhm/src/ceph-tools/cbt/newstore/ceph.conf.1osd&apos; iterations: 1 use_existing: False clusterid: &quot;ceph&quot; tmp_dir: &quot;/tmp/cbt&quot; pool_profiles: rbd: pg_size: 256 pgp_size: 256 replication: 1benchmarks: librbdfio: time: 300 vol_size: 16384 mode: [read, write, randread, randwrite] op_size: [4194304, 2097152, 1048576] concurrent_procs: [1] iodepth: [64] osd_ra: [4096] cmd_path: &apos;/home/nhm/src/fio/fio&apos; pool_profile: &apos;rbd&apos; 还定义了一个关联的ceph.conf.1osd文件，其中包含要在此测试中使用的各种设置： 123456789101112131415161718192021222324252627282930313233343536[global] osd pool default size = 1 auth cluster required = none auth service required = none auth client required = none keyring = /tmp/cbt/ceph/keyring osd pg bits = 8 osd pgp bits = 8 log to syslog = false log file = /tmp/cbt/ceph/log/$name.log public network = 192.168.10.0/24 cluster network = 192.168.10.0/24 rbd cache = true osd scrub load threshold = 0.01 osd scrub min interval = 137438953472 osd scrub max interval = 137438953472 osd deep scrub interval = 137438953472 osd max scrubs = 16 filestore merge threshold = 40 filestore split multiple = 8 osd op threads = 8 mon pg warn max object skew = 100000 mon pg warn min per osd = 0 mon pg warn max per osd = 32768[mon] mon data = /tmp/cbt/ceph/mon.$id [mon.a] host = burnupiX mon addr = 127.0.0.1:6789[osd.0] host = burnupiX osd data = /tmp/cbt/mnt/osd-device-0-data osd journal = /dev/disk/by-partlabel/osd-device-0-journal 要运行此基准测试suite，cbt启动时带参数来指定用于存储结果archive目录和要使用的配置文件： 1cbt.py --archive=&lt;archive dir&gt; ./mytests.yaml 您还可以通过在命令行上指定ceph.conf文件来指定要使用的文件： 1cbt.py -archive=&lt;archive dir&gt; --conf=./ceph.conf.1osd ./mytests.yaml 通过这种方式，可以混合或匹配ceph.conf文件和yaml测试文件，以创建测试的参数化扫描。tools目录中名为mkcephconf.py的脚本可以自动生成数百或数千个ceph.conf文件（需要自定义选项范围），然后以这种方式与cbt一起使用。 CONCLUSION（结论）还有许多其他功能强大的方法可以使用，本文档中尚未涵盖所有cbt的功能。随着时间的推移，我们将尝试为这些功能提供更好的示例和文档。现在，最好查看示例，查看代码，并提出问题！ 实验该实验没有使用nhm账户，以下实验都是root用户，所以ssh互信需要把nhm改为root 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109cbt节点1、安装pdsh[root@cbt ~]# yum -y install epel-release[root@cbt ~]# yum install -y pdsh与ceph节点做免密[root@cbt ~]# ssh-keygen[root@cbt cbt]# ssh-copy-id nhm@192.168.0.45测试[root@cbt ~]# pdsh -w 192.168.0.45 -l nhm uptime192.168.0.45: 14:28:43 up 32 min, 1 user, load average: 0.00, 0.01, 0.052、安装piip[root@cbt ~]# curl &quot;https://bootstrap.pypa.io/get-pip.py&quot; -o &quot;get-pip.py&quot;[root@cbt ~]# python get-pip.py[root@cbt ~]# python -m pip install -U pip[root@cbt ~]# pip install --upgrade setuptools3、安装python依赖库[root@cbt cbt]# pip install lxml numpy PyYAML4、配置/etc/hosts文件[root@cbt cbt]# cat /etc/hosts192.168.0.45 cbt-ceph-1192.168.0.46 cbt-ceph-2192.168.0.47 cbt-ceph-3ceph节点[root@cbt-ceph-1 ~]# yum -y install epel-release[root@cbt-ceph-1 ~]# yum -y install collectl perf blktrace[root@cbt-ceph-1 ~]# useradd -m nhm -g root -G root[root@cbt-ceph-1 ~]# echo nhm:nhm | chpasswd[root@cbt-ceph-1 ~]# vi /etc/sudoers添加nhm ALL=(ALL) NOPASSWD: ALL[root@cbt-ceph-1 ~]# yum -y install epel-release[root@cbt-ceph-1 ~]# yum install -y pdsh[root@cbt-ceph-1 ~]# yum install -y fiocbt单节点radosbench配置文件[root@cbt ~]# cat /root/mytests.yamlcluster: user: &apos;root&apos; head: &quot;cbt-ceph-1&quot; clients: [&quot;cbt-ceph-1&quot;] osds: [&quot;cbt-ceph-1&quot;] mons: [&quot;cbt-ceph-1&quot;] osds_per_node: 1 fs: &apos;xfs&apos; mkfs_opts: &apos;-f -i size=2048&apos; mount_opts: &apos;-o inode64,noatime,logbsize=256k&apos; conf_file: &apos;/etc/ceph/ceph.conf&apos; iterations: 1 use_existing: True version_compat: jewel clusterid: &quot;ceph&quot; tmp_dir: &quot;/tmp/cbt&quot; pool_profiles: rbd: pg_size: 256 pgp_size: 256 replication: 1benchmarks: radosbench: op_size: [ 4194304, 524288, 4096 ] write_only: False time: 5 concurrent_ops: [ 128 ] concurrent_procs: 1 use_existing: True pool_profile: replicatedcbt单节点librbdfio配置文件[root@cbt cbt]# cat /root/mytests.yamlcluster: user: &apos;root&apos; head: &quot;cbt-ceph-1&quot; clients: [&quot;cbt-ceph-1&quot;] osds: [&quot;cbt-ceph-1&quot;] mons: [&quot;cbt-ceph-1&quot;] osds_per_node: 1 fs: &apos;xfs&apos; mkfs_opts: &apos;-f -i size=2048&apos; mount_opts: &apos;-o inode64,noatime,logbsize=256k&apos; conf_file: &apos;/etc/ceph/ceph.conf&apos; iterations: 1 use_existing: True version_compat: jewel clusterid: &quot;ceph&quot; tmp_dir: &quot;/tmp/cbt&quot; pool_profiles: rbd: pg_size: 256 pgp_size: 256 replication: 1benchmarks: librbdfio: time: 300 vol_size: 16384 mode: [read, write, randread, randwrite] op_size: [4194304, 2097152, 1048576] concurrent_procs: [1] iodepth: [64] osd_ra: [4096] pool_profile: &apos;rbd&apos; 运行cbt[root@cbt cbt]# python cbt.py --archive=yujiangresult /root/mytests.yaml]]></content>
      <tags>
        <tag>performance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[disk_perf_test_tool 项目]]></title>
    <url>%2F2018%2F10%2F13%2Fdisk-perf-test-tool-%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[相关链接Ukraine/Kharkiv（乌克兰/哈尔科夫） https://github.com/Mirantis/disk_perf_test_tool https://github.com/Mirantis/disk_perf_test_tool/wiki http://koder-ua.github.io/ Overview（概览）Wally是一种以分布式方式测量不同类型block storages性能的工具，并提供全面的报告。它在分布式和云环境中运行, 但也可以测量单个磁盘。 Wally在控制的方式中投入了大量精力, 从统计的角度正确的处理结果, 提供你可以依靠、争论和理解的数字。 Wally不是负载生成工具。它使用众所周知的负载发生器 - [fio]来测试系统并为其提供包装, 这有助于discovering cluster features and settings，install sensors，为测试preparing system，并行运行多个测试节点的复杂test suites并可视化结果。 主要特点： Cluster and storage preparation，获得尽可能多的可重复结果 与[openstack]，[ceph]和[fuel]集成 分布式执行测试 与[fio]紧密结合 VM产卵在OS测试 Sensors subsystem，用于在测试期间收集群集设备上的负载 简单而灵活的配置文件，允许指定集群结构并选择在加载期间收集信息 综合可视化报告 资源消耗报告，允许查看用于向客户端提供服务的群集资源量 瓶颈分析器，有助于找到影响结果的部件 yaml/cvs基于所有结果的存储，因此您可以轻松地将它们插入到结果处理管道中 wally可以在故障阶段失败时重新启动测试 wally不能做什么： Deploy/configure storage - 测试系统必须准备好进行测试 在测试执行期间更新报告。沃利是完全 cli 工具, 没有 UI, 报告在测试完成后生成 提供交互式报告。所有图/表都是用 matplotlib 并静态生成的。 Basic architecture overview（基本架构概述）Wally代码主要由3部分组成 - [agent library]，[cephlib] 和 [wally] 本身。Agent library负责提供与群集和测试节点的 [RPC] 连接。Cephlib 包含大部分storage、discovery、sensors、数据处理和可视化代码。Wally本身提供 cli, 负载工具集成, 报告生成和其他部分。 fio是一个主要的负载工具，很好的集成在wally内。 wally有自己的fio版本，为一些Linux发行版构建。wally也可以使用系统fio，但需要安装最新版本之一。fio配置文件位于wally/suits/io目录中，具有cfg扩展名。default_qd.cfg是具有默认设置的文件，它主要包含所有其他配置。ceph.cfg，hdd.cfg，cinder_iscsi.cfg是一个主要的测试suites。cfg文件是一个fio配置文件，由wally提供了一些额外的功能。测试之前，wally在所选的cfg文件中插入提供的设置，展开cycles，将其拆分为jobs并从测试节点逐个同步执行jobs。 虽然fio提供了一些这样的功能，但是wally不会使用它们来更精确地控制结果。 要运行测试需要一个配置文件，其中包含cluster信息，sensors设置，test config和一些其他变量来控制测试执行和处理结果。配置文件的示例位于configs-examples目录中。该目录中的所有配置文件都includes default.yaml，反过来default.yaml中includes logging.yaml。在大多数情况下，您无需更改default.yaml/logging.yaml文件中的任何内容。配置文件详细描述如下。 wally执行由各个阶段组成, 大多数阶段都映射配置文件模块。主要阶段有: 群集发现 通过ssh连接节点 使用rpc服务器检测节点 安装sensors和相应的配置文件 运行测试 生成报告 清理 Wally motivation（Wally动机）主要测试问题和wally如何为您修复 Howto install wallyContainer12git clone https://github.com/Mirantis/disk_perf_test_tool.gitdocker build -t &lt;username&gt;/wally . Local installation123apt install g++ ....pip install XXX python -m wally prepare &lt;&lt; download fio, compile可根据Dockerfile查看安装过程 Howto run a test要运行测试，您需要准备集群和配置文件。如何运行wally：直接使用容器 Configuration SSHURI - 格式为 [user[:passwd]@]host[:port][:key_file]的字符串，其中： 123456789101112131415user - str，用户名，默认为当前用户passwd - str，ssh密码，如果提供了key_file_rr或使用了默认密钥，则可以省略ssh密码host - str，唯一必填字段。主机名或IP地址port - int，要连接的ssh服务器端口，默认为22key_file - str, ssh私有密钥文件（private）的路径。默认~/.ssh/id_rsa by default. 如果端口被省略, 但提供了key_file - 它必须与host分开两列。不能同时使用passwd和key_file。例如：11.12.23.10:37 - 使用ip 和 ssh 端口, 当前用户和 ~/.ssh/id_rsa keyceph-1 - 仅使用主机名，默认端口，当前用户和 ~/.ssh/id_rsa keyceph-12::~/.ssh/keyfile - 使用当前用户和 ~/.ssh/keyfile keyroot@master - 以root身份登录并使用 ~/.ssh/id_rsa keyroot@127.0.0.1:44 - 以root身份登录，使用44端口 和 ~/.ssh/id_rsa keyuser@10.20.20.30::/tmp/keyfile - 以root身份登录 和 /tmp/keyfile keyroot:rootpassword@10.20.30.40 - 以root身份登录并使用rootpassword作为ssh密码 [XXX] - XXX类型列表 {XXX: YYY} - 从类型XXX（键key）映射到类型YYY（值value） SIZE - 带有K/M/G/T/P或不带的整数。请注意，使用1024 base，10M实际上意味着10MiB == 10485760 Bytes，依此类推。 Default settings（默认设置）许多配置设置在config-examples/default.yaml文件中已经有可用的默认值，在大多数情况下，用户可以重复使用它们。在你自己的配置文件中，include它： include: default.yaml 您可以在配置文件中覆盖选定的设置。 Plain settings（普通设置） discover: [str] 列表中可能的值：ceph，openstack，fuel，ignore_errors。例：discover: openstack,ceph 为wally提供要发现的集群列表。群集发现用于查找群集节点及其角色，以简化设置配置和其他一些步骤。您始终可以在显式部分中定义或重新定义节点角色。每个群集都需要其他配置部分。ignore_errors意味着忽略丢失的集群。 run_sensors: bool 设置为true，允许wally在测试期间收集负载信息。 这大大增加了结果大小，但允许wally提供更复杂的报告。 results_storage: str 要放置结果的默认目录。 对于每个测试，wally将在此目录中生成唯一名称并创建子目录，所有结果和设置将存储其中。 wally必须有rwx权限来访问此目录。 例如: results_storage: /var/wally sleep: int，默认为零 告诉wally在X秒内什么都不做。 如果您只需要收集sensors，则非常有用。 例如：sleep: 60 Section ceph提供发现ceph集群节点的设置 root_node: str 必须。 根节点的ssh url。 这可以是任何具有ceph client key的节点（任何节点，您可以在其中运行ceph cli命令）。 cluster: str Ceph集群名称。 默认情况下是ceph。 conf: str 群集配置文件的路径。默认情况下是/etc/ceph/{cluster_name}.conf。 key: str client.admin密钥文件的路径。默认情况下是/etc/ceph/{cluster_name}.client.admin.keyring。 ip_remap: {IP: IP} 如果OSD和Monitor节点在 ceph 中通过内部 ip 地址注册，这在您运行wally的节点是不可见。这允许将non-routable的IP地址映射到可routable的IP地址。例如： 1234ip_remap: 10.8.0.4: 172.16.164.71 10.8.0.3: 172.16.164.72 10.8.0.2: 172.16.164.73 例如： 1234567ceph: root_node: ceph-client cluster: ceph # &lt;&lt; optional ip_remap: # &lt;&lt; optional 10.8.0.4: 172.16.164.71 10.8.0.3: 172.16.164.72 10.8.0.2: 172.16.164.73 Section openstack提供openstack设置，用于发现OS群集和spawn/find测试vm。 skip_preparation: bool 默认值：true，wally需要准备openstack来生成虚拟机。 如果先前已准备好OS群集，则可以将此设置设置为false以节省一些检查时间。 openrc: 或 str ir {str: str} 指定源[openstack connection settings]。 openrc: ENV - 从环境变量中获取OS credentials。你需要在wally开始之前导出openrc设置，就像这样 12$ source openrc$ RUN_WALLY 或 1$ env OS_USER=.. OS_PROJECT=.. RUN_WALLY openrc: str - 使用openrc文件，位于提供的路径以获取OS connection settings。例如：openrc: /tmp/my_cluster_openrc openrc: {str: str} - 直接在配置文件中提供connection settings。例如： 12345openrc: OS_USERNAME: USER OS_PASSWORD: PASSWD OS_TENANT_NAME: KEY_FILE OS_AUTH_URL: URL insecure: bool - 在openrc section中提供，覆盖OS_INSECURE设置。 vms: [SSHURI] vm sshuri的列表，除了使用hostname/ip vm 名称前缀之外。wally将找到所有具有此前缀名称的vm，并将其用作测试节点。例如： 123vms: - wally@wally_vm - root:rootpasswd@test_vm 这将找到所有名为wally_vm*和test_vm的vm，并尝试使用提供的credentials重用它们进行测试。请注意，默认情况下，vm wally使用openstack ssh key，而不是~/.ssh/id_rsa。有关详细信息，请参阅Openstack vm config部分。 VM spawning选项。此选项控制要为测试生成的新vm的数量以及要使用的配置文件。所有衍生的vm将自动获得testnode角色并将用于测试。wally尝试使用anti-affinity组在所有计算节点上均匀地生成vm。 count: str 或 int. 控制生成多少个vm，可能的值： 12345=X，其中X是int - 根据需要产生尽可能多的vm，以使总测试节点计数不小于X。例如 - 如果你已经有1个明确的测试节点，通过节点提供，并且在之前的测试运行中找到了2个vm并且你设置了count: =4，那么wally将产生一个额外的vm。X，其中X是integer整数。 正好生成X新vm。xX，其中X是integer整数。 每个compute产生X个vm。例如：copunt: x3 - 每个compute产生3个vm。 cfg_name: str，vm config。默认情况下，只有wally_1024配置可用。此配置使用来自https://cloud-images.ubuntu.com/trusty/current/trusty-server-cloudimg-amd64-disk1.img的image作为vm镜像，1GiB的RAM，2个vCPU和100GiB卷。 有关详细信息，请参阅Openstack vm config。 network_zone_name: str。 内部ip v4的Network pool。 通常是net04 flt_ip_pool: str。 用于浮动ip v4的Network pool。 通常是net04_ext skip_preparation: bool，默认为false。默认情况下，在spawn vm之前，wally检查所有必需的先决条件，如vm flavor，image，aa-groups，ssh rules是否准备好并创建遗漏的东西。这告诉wally跳过这个阶段。如果你确定openstack准备好了，你可以设置它在这个阶段节省一些时间，但最好还是保留它以防止问题。 Section nodes此section定义要执行的测试suites列表。 每个section都是从suite类型到suite配置的映射。 查看下面不同suites的详细信息。 fio suite config load: str - 必须的选项，负载profile的名称。 默认下一个profiles可用： ceph - 适用于各种ceph支持的块设备hdd - 本地hdd驱动器cinder_iscsi - cinder lvm-over-iscsi卷check_distribution - 检查IOPS/latency的分布 有关详细信息，请参阅fio task files section。 params: {str: Any} - 负载profile的参数列表。子参数： FILENAME: str，所有profiles都需要。它将用作fio的测试文件。如果测试文件名在不同的测试节点上不同，则需要在开始测试之前在所有测试节点上创建具有相同名称的（sym）链接，并在此处使用链接名称。 FILESIZE: SIZE，文件大小参数。虽然在大多数情况下wally可以正确检测device/file大小，但您不需要测试整个文件。此外，如果文件尚不存在，则需要此参数。 Non-standard负载可能需要一些其他参数，有关详细信息，请参阅fio task files section。 use_system_fio: bool，默认为false。告诉wally使用测试节点本地fio二进制文件，而不是wally附带的。如果wally没有为你发行指定版本的fio，你可能需要这个。默认情况下，最好使用wally的fio。有关详细信息，请参阅HOWTO/Supply fio for you distribution。 use_sudo: bool，默认为false。Wally将使用sudo在测试节点上运行fio。 通常，如果本地测试节点用户不是root用户，则需要。 force_prefill: bool，默认为false。在测试之前，告诉wally无条件地用伪随机数据填充测试file/device。默认情况下，首先检查目标是否已包含随机数据并跳过填充步骤。在这一步中，wally填充整个device，因此可能需要很长时间。 skip_prefill: bool，默认为false。强制wally不用伪随机数据填充目标。如果您正在测试本地hdd/ssd/cinder iscsi，请使用此选项。但是，如果测试ceph backed device或任何具有延迟空间分配的系统设备则不要使用。 例如： 12345- fio: load: ceph params: FILENAME: /dev/vdb FILESIZE: 100G Key test_profile: str此section允许使用一些预定义的设置集来生成VM和运行测试。config-examples/default.yaml文件中列出了可用的profiles及其设置。下一个profiles默认可用： openstack_ceph - 每个compute产生1个VM，并针对/dev/vdb运行ceph fio profile openstack_cinder - 每个compute产生1个VM，并针对/dev/vdb运行ceph_iscsi_vdb fio profile openstack_nova - 每个compute产生1个VM，并针对/opt/test.bin运行hdd fio profile 例如： 123456789101112include: default.yamldiscover: openstack,cephrun_sensors: trueresults_storage: /var/wally_resultsceph: root_node: localhostopenstack: openrc: ENV # take creds from environment variabletest_profile: openstack_ceph Howto testLocal block device使用config-examples/local_block_device.yml作为模板。将{STORAGE_FOLDER}替换为存放结果的文件夹的路径。确保wally具有对此文件夹的read/write访问权限，或者可以创建它。您可以直接测试device，也可以测试已mount device上的文件。将{STORAGE_DEV_OR_FILE_NAME}替换为正确的路径。在大多数情况下，wally可以正确检测file或block device大小，但通常最好直接设置{STORAGE_OR_FILE_SIZE}。使用的文件越大，对结果的影响越小，将导致不同的缓存，但也会填充更长的时间。 测试sdb device的示例： 12345678910111213include: default.yamlrun_sensors: falseresults_storage: /var/wallynodes: localhost: testnodetests: - fio: load: hdd params: FILENAME: /dev/sdb FILESIZE: 100G mount到/opt文件夹的device测试示例： 12345678910111213include: default.yamlrun_sensors: falseresults_storage: /var/wallynodes: localhost: testnodetests: - fio: load: hdd params: FILENAME: /opt/some_test_file.bin FILESIZE: 100G 请注意，测试完成后，wally不会删除文件。 Ceph without openstack, or other NAS/SANwally仅支持rbd/cephfs测试，不支持object协议，例如rados和RGW。Cephfs测试不需要任何特殊准备，除了将其mounte在测试节点上，详情请参阅ceph fs quick start。 Ceph线性read/write通常受网络限制。例如，如果您在群集中使用10个SATA drives作为storage drives，则聚合线性读取速度可达到~1Gibps或8Gibps，这接近10Gib网络限制。因此，除非你有一个足够带宽的网络测试节点，否则通常最好在多个测试节点并行测试ceph集群。 Ceph在低QD时通常性能较低，因为在这种模式下，您一次只能使用一个OSD。与此同时，ceph可以扩展到比hdd/ssd drives大得多的QD值，因为在这种情况下，您可以在所有OSD daemons中扩展IO请求。您需要最多(16 OSD_count) QD用于4k随机读取和大约(12 OSD_COUNT / REPLICATION_FACTOR)QD用于4k随机写入以摸清群集限制。对于其他blocks块和modes模式，您可能需要不同的设置。如果您使用默认的ceph配置文件，则无需关心此操作。 有三种测试RBD的方法 - direct，通过使用[krbd]和虚拟机将其mount到节点，由rbd driver提供的volume，内置到qemu。对于最后一个，参考Ceph with openstack section或文档。 Using testnode mounted rbd device首先您需要一个pool作为rbd的target。您可以使用默认rbd pool，也可以创建自己的pool。pool需要有很多PG才能有很好的表现。保守估计是(100 * OSD_COUNT / REPLICATION_FACTOR)。创建后ceph可能会警告“too many PG”，这个消息可以安全地被忽略。ceph文档：PLACEMENT GROUPS。 创建一个pool（有关详细信息，请参阅ceph pools documentation） 1$ ceph osd pool create &#123;pool-name&#125; &#123;pg-num&#125; 等到创建完成，所有PG变为active+clean。 在此pool中创建rbd volume，需要选择足够大的volume size以缓解不可避免的OSD节点FS caches。通常(SUM_RAM_SIZE_ON_ALL_OSD * 3)运行良好，并且在读取时仅产生约20％的缓存命中： 1$ rbd create &#123;vol-name&#125; --size &#123;size&#125; --pool &#123;pool-name&#125; 通过kernel rbd device挂载rbd。这是一个棘手的部分。Kernels通常具有旧版本的rbd driver，并且不支持最新的rbd features。这将导致在mount rbd期间出错。首先尝试挂载rbd device： 1$ rbd map &#123;vol-name&#125; --pool &#123;pool-name&#125; 如果失败 - 您需要运行rbd info –pool {pool-name} {vol-name}，并通过rbd feature disable –pool {pool-name} {vol-name} {feature name}禁用features。然后尝试再次mount。 wally需要对rbd device进行read/write访问。 Direct rbd testingDirect rbd测试通过rbd driver运行，内置在fio中。使用此driver，fio可以直接生成RBD请求，无需外部rbd driver。这是测试RBD的最快和最可靠的方法，但是使用内部rbd driver您可以绕过一些可以在生产环境中使用的代码层。wally附带的fio版本没有rbd support，因为它不能被静态地构建。要使用它，您需要使用rbd support构建fio，请参阅Use you fio binary部分的学习指南。 Ceph with openstack最简单的方法是使用预定义的openstack_ceph profile。它为每个计算节点生成一个VM，并在所有计算节点上运行ceph测试suite。 例如： 123456789101112include: default.yamldiscover: openstack,cephrun_sensors: trueresults_storage: /var/wally_resultsceph: root_node: localhostopenstack: openrc: ENV # take creds from environment variabletest_profile: openstack_ceph Cinder lvm volumesnone Howto Use you fio binary 您需要下载fio源代码，在测试节点上编译它以进行linux distribution，使用bz2进行压缩，命名为fio_{DISTRNAME}_{ARCH}.bz2并放入fio_binaries目录。ARCH是目标系统上arch命令的输出。DISTRNAME应与lsb_release -c -s输出相同。 以下是从master编译最新文件的典型步骤： 123456$ git clone $ cd fio$ ./configure --build-static $ make -jXXX # Replace XXX with you CPU core count to decrease compilation time$ bzip2 fio$ mv fio.bz2 WALLY_FOLDER/fio_binaries/fio_DISTRO_ARCH.bz2 Storage structurewally保存所有输入configurations，所有收集的数据和测试结果保存到results_storage设置目录的单个子文件夹下。所有文件都是csv(results/sensor files)，yaml/js用于配置non-numeric信息，png/svg用于images和联结原始文本文件，如日志和一些输出。 以下是每个文件包含的内容： cli - txt，wally cli in semi-raw formal config.yaml - yaml，完整的最终config，从原始的wally config构建，处理所有替换和cli参数传递。 log - txt，wally执行日志。包含某个测试的所有wally运行log，包括restarts和报告生成。 result_code - yaml，此文件夹上带有’test’子命令的最后一次执行的退出代码。 run_interval - yaml，此文件夹上带有’test’子命令的最后一次执行的[begin_time, end_time]列表。 meta - folder，用于统计计算的cached值。 nodes - folder，有关测试集群的信息。 all.yml - yaml，除节点参数外的所有节点的信息 nodes/parameters.js - js，节点参数。参数是单独存储的，因为它们可能是非常多的ceph节点，并且在python中解析的js文件比yaml快得多。 report - folder，html/css文件报告和所有图表。可以复制到其他地方。 index.html - 报告开始页面。 main.css - css文件报告。 XXX/YYY.png or .svg - 图表文件报告。 results - 所有fio结果的文件夹 fio_{SUITE_NAME}_{IDX1}.yml - yaml，每个已执行suite的完整配置。 fio_{SUITE_NAME}_{IDX1}.{JOB_SHORT_DESCR}_{IDX2} - 包含suite中每个job所有数据的文件夹 {NODE_IP}:{NODE_SSH_PORT}.fio.{TS}.(csv|json) - fio输出文件。TS是解析时间序列（timeseries）名称 - bw或lat或stdout用于输出。 ​ info.yml - 社区的原作者没有写完]]></content>
      <tags>
        <tag>performance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang笔记一接口]]></title>
    <url>%2F2018%2F09%2F22%2Fgolang%E7%AC%94%E8%AE%B0%E4%B8%80%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[靠继承和多态能完成的事情，在go语言中是通过接口来完成的。 duck typing大黄鸭是鸭子吗？ 传统类型系统：脊索动物门，脊椎动物门，鸟纲雁形目… … duck typing：是鸭子。 duck typing 像鸭子走路，像鸭子叫（长得像鸭子），那么就是鸭子 描述事物的外部行为而非内部结构 严格说go属于结构化类型系统，类似duck typing python中的duck typing 12345def download(retriever): return retriever.get(&quot;www.imooc.com&quot;)- 运行时才知道传入的retriever有没有get- 需要注释来说明接口 c++中的duck typing 1234567template &lt;class R&gt;string download(const R&amp; retriever) &#123; return retriever.get(&quot;www.imooc.com&quot;)&#125;- 编译时才知道传入的retriever有没有get- 需要注释来说明接口 java中的类似代码 1234567&lt;R extends Retriever&gt;String download(R r) &#123; return r.get(&quot;www.imooc.com&quot;)&#125;- 传入的参数必须实现Retriever接口- 不是duck typing go语言的duck typing 123同时需要Readable，Appendable怎么办？(java有个apache polygene)同时具有python，c++的duck typing的灵活性又具有java的类型检查 接口的定义 1234 使用者 实现者download --&gt; retrievergo语言的接口由使用者定义(和传统的面向对象思维不同，传统面向对象是实现者告诉大家我实现了某个接口，然后你们可以通过这个接口来用我) demo 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596yujiangdeMacBook-Pro-13:GolangGrammar yujiang$ tree retrieve/retrieve/├── main.go├── mock│ └── mockretiever.go└── real └── retriever.go2 directories, 3 files文件main.gopackage mainimport ( &quot;fmt&quot; &quot;github.com/lnsyyj/GolangGrammar/retrieve/mock&quot; &quot;github.com/lnsyyj/GolangGrammar/retrieve/real&quot;)type Retriever interface &#123; Get(url string) string&#125;func download(r Retriever) string &#123; return r.Get(&quot;http://www.imooc.com&quot;)&#125;func main() &#123; var r Retriever r = mock.Retriever&#123;&quot;This is a fack imooc.com&quot;&#125; fmt.Println(download(r)) r = real.Retriever&#123;&#125; fmt.Println(download(r))&#125;文件retriever.gopackage realimport ( &quot;time&quot; &quot;net/http&quot; &quot;net/http/httputil&quot;)type Retriever struct &#123; UserAgent string TimeOut time.Duration&#125;func (r Retriever) Get(url string) string &#123; resp, err := http.Get(url) if err != nil &#123; panic(err) &#125; result, err := httputil.DumpResponse(resp, true) resp.Body.Close() if err != nil &#123; panic(err) &#125; return string(result)&#125;文件mockretiever.gopackage mocktype Retriever struct &#123; Contents string&#125;func (r Retriever) Get(url string) string &#123; return r.Contents&#125;===================================output===================================This is a fack imooc.comHTTP/1.1 200 OKAccept-Ranges: bytesAge: 9Connection: keep-aliveContent-Type: text/html; charset=utf-8Date: Wed, 10 Oct 2018 14:38:23 GMTServer: nginxVary: Accept-EncodingVia: 1.1 varnish (Varnish/6.0)X-Cache: HIT from CS42X-Varnish: 280162519 281222076......body省略...===================================output===================================看看interface里面有什么？func main() &#123; var r Retriever r = mock.Retriever&#123;&quot;This is a fack imooc.com&quot;&#125; fmt.Printf(&quot;%T %v\n&quot;, r, r) //fmt.Println(download(r)) r = real.Retriever&#123;&#125; fmt.Printf(&quot;%T %v\n&quot;, r, r) //fmt.Println(download(r))&#125;===================================output===================================mock.Retriever &#123;This is a fack imooc.com&#125;real.Retriever &#123; 0s&#125;===================================output===================================]]></content>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang笔记一面向对象]]></title>
    <url>%2F2018%2F09%2F13%2Fgolang%E7%AC%94%E8%AE%B0%E4%B8%80%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[go语言仅支持封装，不支持继承和多态 go语言没有class，只有struct 结构的创建12345678910111213141516171819202122232425262728293031323334353637383940package mainimport &quot;fmt&quot;type treeNode struct &#123; value int left, right *treeNode&#125;func main() &#123; var tree treeNode fmt.Println(tree)&#125;===================================output===================================&#123;0 &lt;nil&gt; &lt;nil&gt;&#125;===================================output===================================package mainimport &quot;fmt&quot;type treeNode struct &#123; value int left, right *treeNode&#125;func main() &#123; var root treeNode root = treeNode&#123;value: 3&#125; root.left = &amp;treeNode&#123;&#125; root.right = &amp;treeNode&#123;5, nil, nil&#125; root.right.left = new(treeNode) nodes := []treeNode &#123; &#123;value: 3&#125;, &#123;&#125;, &#123;6, nil, &amp;root&#125;, &#125; fmt.Println(nodes)&#125;===================================output===================================[&#123;3 &lt;nil&gt; &lt;nil&gt;&#125; &#123;0 &lt;nil&gt; &lt;nil&gt;&#125; &#123;6 &lt;nil&gt; 0xc420098020&#125;]===================================output===================================不论地址还是结构本身，一律使用.来访问成员 go语言没有构造函数，但是可以自己创建工厂函数 12345678910111213141516171819202122232425262728293031package mainimport &quot;fmt&quot;type treeNode struct &#123; value int left, right *treeNode&#125;func createNode(value int) *treeNode &#123; // 相当于在函数里建了一个局部变量，返回的地址是局部变量的地址，如果在c++中是典型的错误，在go语言中局部变量地址也是可以给别人用的 // 这个局部变量是创建在堆上还是栈上？go语言不需要知道，有垃圾回收器 return &amp;treeNode&#123;value: value&#125;&#125;func main() &#123; var root treeNode root = treeNode&#123;value: 3&#125; root.left = &amp;treeNode&#123;&#125; root.right = &amp;treeNode&#123;5, nil, nil&#125; root.right.left = new(treeNode) root.left.right = createNode(2) fmt.Println(root)&#125;===================================output===================================&#123;3 0xc42000a080 0xc42000a0a0&#125;===================================output===================================树 3 / \ 0 5 \ / 2 0 为结构定义方法 显示定义，并为方法接收者命名 只有使用指针才可以改变结构内容，否则是值传递（是拷贝） nil指针也可以调用方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899package mainimport ( &quot;fmt&quot;)type treeNode struct &#123; value int left, right *treeNode&#125;// 给结构定义方法，(node treeNode)就相当于c++的this指针，叫做接收者。就相当于print（函数名）是给node接收的// node treeNode是传值的还是传引用的？当然是传值的func (node treeNode) print() &#123; fmt.Printf(&quot;%d &quot;, node.value)&#125;// 与print其实是一样的func print1(node treeNode) &#123; fmt.Println(node.value)&#125;// node treeNode参数实际上是传值的func (node treeNode) setValue(value int) &#123; node.value = value&#125;// 传指针func (node *treeNode) setValue1(value int) &#123; node.value = value&#125;// nilfunc (node *treeNode) setValue2(value int) &#123; if node == nil &#123; fmt.Println(&quot;Setting value to nil node. Ignored.&quot;) return &#125; node.value = value&#125;// 树的中序遍历func (node *treeNode) traverse() &#123; if node == nil &#123; return &#125; node.left.traverse() node.print() node.right.traverse()&#125;func createNode(value int) *treeNode &#123; // 相当于在函数里建了一个局部变量，返回的地址是局部变量的地址，如果在c++中是典型的错误，在go语言中局部变量地址也是可以给别人用的 return &amp;treeNode&#123;value: value&#125;&#125;func main() &#123; var root treeNode root = treeNode&#123;value: 3&#125; root.left = &amp;treeNode&#123;&#125; root.right = &amp;treeNode&#123;5, nil, nil&#125; root.right.left = new(treeNode) root.left.right = createNode(2) // 使用为结构定义的方法 root.print() fmt.Println() print1(root) fmt.Println() // 参数实际上是传值的，改不掉 root.right.left.setValue(4) root.right.left.print() fmt.Println() // 传指针 root.right.left.setValue1(4) root.right.left.print() fmt.Println() root.print() root.setValue1(100) pRoot := &amp;root pRoot.print() pRoot.setValue1(200) pRoot.print() fmt.Println() // nil var pRoot2 *treeNode pRoot2.setValue2(200) pRoot2 = &amp;root pRoot2.setValue2(300) pRoot2.print() fmt.Println() root.traverse()&#125;===================================output===================================3 30 4 3 100 200 Setting value to nil node. Ignored.300 0 2 300 4 5 ===================================output=================================== 值接收者VS指针接收者 要改变内容必须使用指针接收者 结构过大也考虑使用指针接收者 一致性：如果指针接收者，最好都是指针接收者 值接受者是go语言特有 值/指针接收者均可以接收 值/指针，并不会改变调用者如何调用 封装 名字一般使用CamelCase 首字母大写：public 首字母小写：private 以上都是针对包来说的 包 每个目录一个包（包名不一定和目录名一样），每个目录只能放一个包 main包包含可执行入口（目录中如果有一个main函数，这个目录下只能有一个main包） 为结构定义的方法必须放在同一个包内，可以是不同的文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119yujiangdeMBP-13:~ yujiang$ tree go/src/github.com/lnsyyj/GolangGrammar/tree/go/src/github.com/lnsyyj/GolangGrammar/tree/├── entry│ └── entry.go├── node.go└── traversal.go1 directory, 3 filesyujiangdeMBP-13:~ yujiang$ cat go/src/github.com/lnsyyj/GolangGrammar/tree/node.go package treeimport ( &quot;fmt&quot;)type Node struct &#123; Value int Left, Right *Node&#125;// 给结构定义方法，(node Node)就相当于c++的this指针，叫做接收者。就相当于print（函数名）是给node接收的// node Node是传值的还是传引用的？当然是传值的func (node Node) Print() &#123; fmt.Printf(&quot;%d &quot;, node.Value)&#125;// 与print其实是一样的func Print1(node Node) &#123; fmt.Println(node.Value)&#125;// node Node参数实际上是传值的func (node Node) SetValue(value int) &#123; node.Value = value&#125;// 传指针func (node *Node) SetValue1(value int) &#123; node.Value = value&#125;// nilfunc (node *Node) SetValue2(value int) &#123; if node == nil &#123; fmt.Println(&quot;Setting value to nil node. Ignored.&quot;) return &#125; node.Value = value&#125;func CreateNode(value int) *Node &#123; // 相当于在函数里建了一个局部变量，返回的地址是局部变量的地址，如果在c++中是典型的错误，在go语言中局部变量地址也是可以给别人用的 return &amp;Node&#123;Value: value&#125;&#125;yujiangdeMBP-13:~ yujiang$ cat go/src/github.com/lnsyyj/GolangGrammar/tree/traversal.go package tree// 树的中序遍历func (node *Node) Traverse() &#123; if node == nil &#123; return &#125; node.Left.Traverse() node.Print() node.Right.Traverse()&#125;yujiangdeMBP-13:~ yujiang$ cat go/src/github.com/lnsyyj/GolangGrammar/tree/entry/entry.go package mainimport ( &quot;fmt&quot; &quot;github.com/lnsyyj/GolangGrammar/tree&quot;)func main() &#123; var root tree.Node root = tree.Node&#123;Value: 3&#125; root.Left = &amp;tree.Node&#123;&#125; root.Right = &amp;tree.Node&#123;5, nil, nil&#125; root.Right.Left = new(tree.Node) root.Left.Right = tree.CreateNode(2) // 使用为结构定义的方法 root.Print() fmt.Println() tree.Print1(root) fmt.Println() // 参数实际上是传值的，改不掉 root.Right.Left.SetValue(4) root.Right.Left.Print() fmt.Println() // 传指针 root.Right.Left.SetValue1(4) root.Right.Left.Print() fmt.Println() root.Print() root.SetValue1(100) pRoot := &amp;root pRoot.Print() pRoot.SetValue1(200) pRoot.Print() fmt.Println() // nil var pRoot2 *tree.Node pRoot2.SetValue2(200) pRoot2 = &amp;root pRoot2.SetValue2(300) pRoot2.Print() fmt.Println() root.Traverse()&#125;===================================output===================================3 30 4 3 100 200 Setting value to nil node. Ignored.300 0 2 300 4 5 ===================================output=================================== 如何扩充系统类型或者别人的类型 使用组合 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107yujiangdeMBP-13:~ yujiang$ tree go/src/github.com/lnsyyj/GolangGrammar/tree/go/src/github.com/lnsyyj/GolangGrammar/tree/├── entry│ └── entry.go├── node.go└── traversal.go1 directory, 3 filesyujiangdeMBP-13:~ yujiang$ cat go/src/github.com/lnsyyj/GolangGrammar/tree/node.go package treeimport ( &quot;fmt&quot;)type Node struct &#123; Value int Left, Right *Node&#125;// 给结构定义方法，(node Node)就相当于c++的this指针，叫做接收者。就相当于print（函数名）是给node接收的// node Node是传值的还是传引用的？当然是传值的func (node Node) Print() &#123; fmt.Printf(&quot;%d &quot;, node.Value)&#125;// 与print其实是一样的func Print1(node Node) &#123; fmt.Println(node.Value)&#125;// node Node参数实际上是传值的func (node Node) SetValue(value int) &#123; node.Value = value&#125;// 传指针func (node *Node) SetValue1(value int) &#123; node.Value = value&#125;// nilfunc (node *Node) SetValue2(value int) &#123; if node == nil &#123; fmt.Println(&quot;Setting value to nil node. Ignored.&quot;) return &#125; node.Value = value&#125;func CreateNode(value int) *Node &#123; // 相当于在函数里建了一个局部变量，返回的地址是局部变量的地址，如果在c++中是典型的错误，在go语言中局部变量地址也是可以给别人用的 return &amp;Node&#123;Value: value&#125;&#125;yujiangdeMBP-13:~ yujiang$ cat go/src/github.com/lnsyyj/GolangGrammar/tree/traversal.go package tree// 树的中序遍历func (node *Node) Traverse() &#123; if node == nil &#123; return &#125; node.Left.Traverse() node.Print() node.Right.Traverse()&#125;yujiangdeMBP-13:~ yujiang$ cat go/src/github.com/lnsyyj/GolangGrammar/tree/entry/entry.go package mainimport ( &quot;fmt&quot; &quot;github.com/lnsyyj/GolangGrammar/tree&quot;)type myTreeNode struct &#123; node *tree.Node&#125;func (myNode *myTreeNode) postOrder() &#123; if myNode == nil || myNode.node == nil &#123; return &#125; left := myTreeNode&#123;myNode.node.Left&#125; right := myTreeNode&#123;myNode.node.Right&#125; left.postOrder() right.postOrder() myNode.node.Print()&#125;func main() &#123; var root tree.Node root = tree.Node&#123;Value: 3&#125; root.Left = &amp;tree.Node&#123;&#125; root.Right = &amp;tree.Node&#123;5, nil, nil&#125; root.Right.Left = new(tree.Node) root.Left.Right = tree.CreateNode(2) root.Right.Left.SetValue1(4) root.Traverse() fmt.Println() myRoot := myTreeNode&#123;&amp;root&#125; myRoot.postOrder() fmt.Println()&#125;===================================output===================================0 2 3 4 52 0 4 5 3===================================output===================================树 3 / \ 0 5 \ / 2 4 定义别名 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748yujiangdeMacBook-Pro-13:~ yujiang$ tree go/src/github.com/lnsyyj/GolangGrammar/queue/go/src/github.com/lnsyyj/GolangGrammar/queue/├── entry│ └── main.go└── queue.go1 directory, 2 filesyujiangdeMacBook-Pro-13:~ yujiang$ cat go/src/github.com/lnsyyj/GolangGrammar/queue/queue.go package queuetype Queue []intfunc (q *Queue) Push(v int) &#123; *q = append(*q, v)&#125;func (q *Queue) Pop() int &#123; head := (*q)[0] *q = (*q)[1:] return head&#125;func (q *Queue) IsEmpty() bool &#123; return len(*q) == 0&#125;yujiangdeMacBook-Pro-13:~ yujiang$ cat go/src/github.com/lnsyyj/GolangGrammar/queue/entry/main.go package mainimport ( &quot;github.com/lnsyyj/GolangGrammar/queue&quot; &quot;fmt&quot;)func main() &#123; q := queue.Queue&#123;1&#125; q.Push(2) q.Push(3) fmt.Println(q.Pop()) fmt.Println(q.Pop()) fmt.Println(q.IsEmpty()) fmt.Println(q.Pop()) fmt.Println(q.IsEmpty())&#125;===================================output===================================12false3true===================================output=================================== GOPATH环境变量 默认在~/go(unix, linux)，%USERPROFILE%\go(windows) 官方推荐：所有项目和第三方库都放在同一个GOPATH下 也可以将每个项目放在不同的GOPATH下（环境变量可以配置多个GOPATH，编译时会去不同的GOPATH找到依赖的包） 1234567yujiangdeMacBook-Pro-13:~ yujiang$ echo $GOPATH/Users/yujiang/goyujiangdeMacBook-Pro-13:~ yujiang$ cat ~/.bash_profile export GOPATH=/Users/yujiang/goexport GOBIN=/Users/yujiang/go/binexport PATH=$PATH:$GOBIN GoLand自动清理import工具 123Preferences | Tools | File WatchersyujiangdeMBP-13:~ yujiang$ go get golang.org/x/tools/cmd/goimports被墙了，获取不下来 go get 获取第三方库 go get命令演示 使用gopm来获取无法下载的包 12345678yujiangdeMBP-13:~ yujiang$ go get -v github.com/gpmgo/gopmyujiangdeMBP-13:~ yujiang$ gopm get -g -v golang.org/x/tools/cmd/goimportsyujiangdeMBP-13:~ yujiang$ ls go/src/golang.org/x/tools/imports/fix.go fix_test.go imports.go mkindex.go mkstdlib.go sortimports.go zstdlib.go# 编译出goimportsyujiangdeMBP-13:~ yujiang$ go build golang.org/x/tools/cmd/goimports# 编译并安装到$GOPATH/bin/目录下yujiangdeMBP-13:~ yujiang$ go install golang.org/x/tools/cmd/goimports go build编译 go install产生pkg文件和可执行文件 go run直接编译运行 GOPATH下目录结构12345678910src git repository 1 git repository 2pkg git repository 1 git repository 2bin 执行文件1，2，3 一般每个目录（包）下有一个main文件（package main func main() &#123;&#125;）]]></content>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph build rpm]]></title>
    <url>%2F2018%2F09%2F13%2Fceph-build-rpm%2F</url>
    <content type="text"><![CDATA[最近做了一个实验，ceph代码如何build出rpm包？ 官方文档：http://docs.ceph.com/docs/master/install/build-ceph/ 步骤1、首先需要ceph源码，官方的例子是把ceph源码压缩成tar.bz2 2、然后rpmbuild工具根据ceph源码中的ceph.spec规则进行build 详细实验步骤方法一： 12就是按照官方文档的思路去做tar --strip-components=1 -C ~/rpmbuild/SPECS/ --no-anchored -xvjf ~/rpmbuild/SOURCES/ceph-10.2.11.tar.bz2 &quot;ceph.spec&quot; 方法二： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283为了实验方便，我们会下载官方发布出来的.src.rpm包来实验。.src.rpm会随着rpm同时发布，可以用来移植不同系统平台。安装依赖[root@yujiang-dev-20180912135521 ~]# git clone https://github.com/ceph/ceph.git[root@yujiang-dev-20180912135521 ~]# cd ceph[root@yujiang-dev-20180912135521 ceph]# ./install-deps.sh [root@yujiang-dev-20180907154634 ~]# yum install rpm-build rpmdevtools hdparm libatomic_ops-devel fcgi-devel boost-devel cmake gcc-c++ tree selinux-policy-doc -y下载官方发布出来的.src.rpm[root@yujiang-dev-20180907154634 ~]# wget http://download.ceph.com/rpm-jewel/el7/SRPMS/ceph-10.2.11-0.el7.src.rpm生成rpmbuild目录[root@yujiang-dev-20180907154634 ~]# rpmdev-setuptree[root@yujiang-dev-20180907154634 ~]# rpm -i ceph-10.2.11-0.el7.src.rpm[root@yujiang-dev-20180912135521 ~]# tree rpmbuild/SRPMS/rpmbuild/SRPMS/└── ceph-10.2.11-0.el7.centos.src.rpm[root@yujiang-dev-20180912135521 ~]# tree rpmbuild/SPECS/rpmbuild/SPECS/└── ceph.spec开始构建rpm包[root@yujiang-dev-20180912135521 ~]# rpmbuild -ba ~/rpmbuild/SPECS/ceph.spec[root@yujiang-dev-20180912135521 ~]# tree rpmbuild/RPMS/rpmbuild/RPMS/└── x86_64 ├── ceph-10.2.11-0.el7.centos.x86_64.rpm ├── ceph-base-10.2.11-0.el7.centos.x86_64.rpm ├── ceph-common-10.2.11-0.el7.centos.x86_64.rpm ├── ceph-debuginfo-10.2.11-0.el7.centos.x86_64.rpm ├── ceph-devel-compat-10.2.11-0.el7.centos.x86_64.rpm ├── cephfs-java-10.2.11-0.el7.centos.x86_64.rpm ├── ceph-fuse-10.2.11-0.el7.centos.x86_64.rpm ├── ceph-libs-compat-10.2.11-0.el7.centos.x86_64.rpm ├── ceph-mds-10.2.11-0.el7.centos.x86_64.rpm ├── ceph-mon-10.2.11-0.el7.centos.x86_64.rpm ├── ceph-osd-10.2.11-0.el7.centos.x86_64.rpm ├── ceph-radosgw-10.2.11-0.el7.centos.x86_64.rpm ├── ceph-resource-agents-10.2.11-0.el7.centos.x86_64.rpm ├── ceph-selinux-10.2.11-0.el7.centos.x86_64.rpm ├── ceph-test-10.2.11-0.el7.centos.x86_64.rpm ├── libcephfs1-10.2.11-0.el7.centos.x86_64.rpm ├── libcephfs1-devel-10.2.11-0.el7.centos.x86_64.rpm ├── libcephfs_jni1-10.2.11-0.el7.centos.x86_64.rpm ├── libcephfs_jni1-devel-10.2.11-0.el7.centos.x86_64.rpm ├── librados2-10.2.11-0.el7.centos.x86_64.rpm ├── librados2-devel-10.2.11-0.el7.centos.x86_64.rpm ├── libradosstriper1-10.2.11-0.el7.centos.x86_64.rpm ├── libradosstriper1-devel-10.2.11-0.el7.centos.x86_64.rpm ├── librbd1-10.2.11-0.el7.centos.x86_64.rpm ├── librbd1-devel-10.2.11-0.el7.centos.x86_64.rpm ├── librgw2-10.2.11-0.el7.centos.x86_64.rpm ├── librgw2-devel-10.2.11-0.el7.centos.x86_64.rpm ├── python-ceph-compat-10.2.11-0.el7.centos.x86_64.rpm ├── python-cephfs-10.2.11-0.el7.centos.x86_64.rpm ├── python-rados-10.2.11-0.el7.centos.x86_64.rpm ├── python-rbd-10.2.11-0.el7.centos.x86_64.rpm ├── rbd-fuse-10.2.11-0.el7.centos.x86_64.rpm ├── rbd-mirror-10.2.11-0.el7.centos.x86_64.rpm └── rbd-nbd-10.2.11-0.el7.centos.x86_64.rpm1 directory, 34 files解压bz2文件yum install bzip2 -ybzip2 -d ceph-12.2.8.tar.bz2tar xvf ceph-12.2.8.tartar zcvf ceph-12.2.8.tar.gz ceph-12.2.8如果是ceph-14.2.4需要安装yum -y install centos-release-sclyum -y install devtoolset-7-gcc devtoolset-7-gcc-c++ devtoolset-7-binutilsscl enable devtoolset-7 bashyum install ceph-2:14.2.4-0.el7.x86_64rpmbuild -ba ~/rpmbuild/SPECS/ceph.spec解包：tar xvf FileName.tar打包：tar cvf FileName.tar DirName压缩：bzip2 [原文件名].tar解压：bunzip2 [原文件名].tar.bz2]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang笔记一基础语法]]></title>
    <url>%2F2018%2F09%2F04%2Fgolang%E7%AC%94%E8%AE%B0%E4%B8%80%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[课程地址：https://coding.imooc.com/class/chapter/180.html 变量的定义1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package mainimport &quot;fmt&quot;//函数外定义变量不能用:=,作用域只在包package内部var aa = 3var ss = &quot;string&quot;//或者放在括号内var ( bb = 4 cc = 5)//不初始化变量值func variableZeroValue()&#123; var i int var s string fmt.Printf(&quot;%d %q\n&quot;, i, s)&#125;//初始化变量值func variableInitialValue()&#123; var a, b int = 3, 4 var s string = &quot;Hello&quot; fmt.Println(a, b, s)&#125;//初始化多个不同类型变量func variableTypeDeduction()&#123; var a, b, c, s = 3, 4, true, &quot;World&quot; fmt.Println(a, b, c, s)&#125;//:=，省略掉varfunc variableShorter()&#123; a, b, c, s := 3, 4, true, &quot;World&quot; b = 5 fmt.Println(a, b, c, s)&#125;func main() &#123; fmt.Println(&quot;Hello World&quot;) variableZeroValue() variableInitialValue() variableTypeDeduction() variableShorter() fmt.Println(aa, ss, bb, cc)&#125;===================================output===================================Hello World0 &quot;&quot;3 4 Hello3 4 true World3 5 true World3 string 4 5 内建变量类型 bool, string (u)int, (u)int8, (u)int16, (u)int32, (u)int64, uintptr byte, rune(这是字符型，golang中不叫char，是32位的) float32, float64, complex64, complex128 1234567891011121314151617package mainimport ( &quot;fmt&quot; &quot;math/cmplx&quot; &quot;math&quot;)func euler()&#123; fmt.Printf(&quot;%.3f\n&quot;, cmplx.Exp(1i*math.Pi) + 1)&#125;func main() &#123; euler()&#125;===================================output===================================(0.000+0.000i) 强制类型转换 类型转换是强制的（golang只有强制类型转换，没有隐式类型转换） 1234567891011121314151617import ( &quot;fmt&quot; &quot;math&quot;)func triangle()&#123; var a, b int = 3, 4 var c int c = int(math.Sqrt(float64(a * a + b * b))) fmt.Println(c)&#125;func main() &#123; triangle()&#125;===================================output===================================5 常量的定义123456789101112131415161718192021222324252627package mainimport ( &quot;math&quot; &quot;fmt&quot;)const filename = &quot;abc.txt&quot;func consts()&#123; //不指定类型就是一个文本 //如果指定类型，某些地方就需要强制类型转换 //const a, b int = 3, 4 //c = int(math.Sqrt(float64(a * a + b * b))) const a, b = 3, 4 const ( name = &quot;hello world&quot; d, e= 5,6 ) var c int c = int(math.Sqrt(a * a + b * b)) fmt.Println(filename, c, name, d, e)&#125;func main() &#123; consts()&#125;===================================output===================================abc.txt 5 hello world 5 6 使用常量定义枚举类型123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package mainimport &quot;fmt&quot;func enums()&#123; const ( cpp = 0 java = 1 python = 2 golang = 3 ) fmt.Println(cpp, java, python, golang)&#125;func main() &#123; enums()&#125;===================================output===================================0 1 2 3===================================output===================================package mainimport &quot;fmt&quot;func enums()&#123; const ( cpp = iota java python golang ) fmt.Println(cpp, java, python, golang)&#125;func main() &#123; enums()&#125;===================================output===================================0 1 2 3===================================output===================================package mainimport &quot;fmt&quot;func enums()&#123; const ( cpp = iota _ python golang javascript ) fmt.Println(cpp, javascript, python, golang)&#125;func main() &#123; enums()&#125;===================================output===================================0 4 2 3===================================output===================================package mainimport &quot;fmt&quot;func enums()&#123; const ( b = 1 &lt;&lt; (10 * iota) kb mb gb tb pb ) fmt.Println(b, kb, mb, gb, tb, pb)&#125;func main() &#123; enums()&#125;===================================output===================================1 1024 1048576 1073741824 1099511627776 1125899906842624===================================output=================================== 条件语句12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package mainimport ( &quot;io/ioutil&quot; &quot;fmt&quot;)func main() &#123; const filename = &quot;abc.txt&quot; contents, err := ioutil.ReadFile(filename) if err != nil &#123; fmt.Println(err) &#125;else &#123; fmt.Printf(&quot;%s\n&quot;, contents) &#125;&#125;===================================output===================================open abc.txt: no such file or directory===================================output===================================hello world===================================output===================================package mainimport ( &quot;io/ioutil&quot; &quot;fmt&quot;)func main() &#123; const filename = &quot;abc.txt&quot; //条件里赋值的变量作用域就在这个if语句里 if contents, err := ioutil.ReadFile(filename); err != nil &#123; fmt.Println(err) &#125;else &#123; fmt.Printf(&quot;%s\n&quot;, contents) &#125;&#125;===================================output===================================hello world===================================output===================================package mainimport ( &quot;fmt&quot;)func grade(score int) string &#123; g := &quot;&quot; //不带表达式也可以进行switch switch &#123; case score &lt; 0 || score &gt; 100: panic(fmt.Sprintf(&quot;Wrong score: %d&quot;, score)) case score &lt; 60: g = &quot;F&quot; case score &lt; 80: g = &quot;C&quot; case score &lt; 90: g = &quot;B&quot; case score &lt;=100: g = &quot;A&quot; &#125; return g&#125;func main() &#123; fmt.Println( grade(0), grade(59), grade(60), grade(82), grade(99), grade(100), )&#125;===================================output===================================F F C B A A===================================output=================================== 循环golang中只有for，没有while 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package mainimport ( &quot;strconv&quot; &quot;fmt&quot;)func convertToBin(n int) string &#123; result := &quot;&quot; for ; n &gt; 0; n /= 2 &#123; lsb := n % 2 result = strconv.Itoa(lsb) + result &#125; return result&#125;func main() &#123; fmt.Println( convertToBin(5), convertToBin(13), convertToBin(789), convertToBin(0), )&#125;===================================output===================================101 1101 1100010101 ===================================output===================================package mainimport ( &quot;fmt&quot; &quot;os&quot; &quot;bufio&quot;)func readFile(filename string) &#123; if file, err := os.Open(filename); err != nil &#123; panic(err) &#125;else &#123; scanner := bufio.NewScanner(file) for scanner.Scan() &#123; fmt.Println(scanner.Text()) &#125; &#125;&#125;func main() &#123; readFile(&quot;/Users/yujiang/go/src/github.com/lnsyyj/GolangGrammar/abc.txt&quot;)&#125;===================================output===================================hello worldyes===================================output===================================//不写条件，死循环for &#123; fmt.Println(&quot;abc&quot;)&#125; 函数1234567891011121314151617181920212223242526272829303132333435package mainimport &quot;fmt&quot;//函数可以返回多个值func eval(a, b int ,op string) (int, error) &#123; switch op &#123; case &quot;+&quot;: return a + b, nil case &quot;-&quot;: return a - b, nil case &quot;*&quot;: return a * b, nil case &quot;/&quot;: q , _ := div(a, b) return q, nil default: return 0, fmt.Errorf(&quot;unsupported operator: %s&quot;, op) &#125;&#125;//返回值可以起名func div(a, b int) (q, r int)&#123; return a / b, a % b&#125;func main() &#123; if result, err := eval(3, 4, &quot;；&quot;); err != nil &#123; fmt.Println(err) &#125;else &#123; fmt.Println(result) &#125; q, r := div(13, 3) fmt.Println(q, r)&#125;===================================output===================================unsupported operator: ；4 1===================================output=================================== 函数式编程123456789101112131415161718192021222324252627282930package mainimport ( &quot;fmt&quot; &quot;reflect&quot; &quot;runtime&quot; &quot;math&quot;)func apply(op func(int, int) int, a, b int) int &#123; //拿到函数名 p := reflect.ValueOf(op).Pointer() opName := runtime.FuncForPC(p).Name() fmt.Printf(&quot;Calling function %s with args &quot; + &quot;(%d, %d)\n&quot;, opName, a, b) return op(a, b)&#125;func pow(a, b int) int &#123; return int(math.Pow(float64(a), float64(b)))&#125;func main() &#123; fmt.Println(apply(pow, 3, 4)) fmt.Println(apply( func(a int, b int) int &#123; return int(math.Pow(float64(a), float64(b))) &#125;, 3, 4))&#125;===================================output===================================Calling function main.pow with args (3, 4)81Calling function main.main.func1 with args (3, 4)81===================================output=================================== 可变参数列表1234567891011121314151617package mainimport ( &quot;fmt&quot;)func sum(numbers ...int) int &#123; s := 0 for i := range numbers &#123; s += numbers[i] &#125; return s&#125;func main() &#123; fmt.Println(sum(1, 2, 3, 4, 5))&#125;===================================output===================================15===================================output=================================== 指针golang指针不能运算，go语言只有值传递一种方式。没有引用传递。 1234567891011121314151617181920212223242526package mainimport &quot;fmt&quot;func swap1(a, b int) &#123; a, b = b, a&#125;//指针func swap2(a ,b *int) &#123; *a, *b = *b, *a&#125;func swap3(a, b int) (int, int) &#123; return b, a&#125;func main() &#123; a, b := 3, 4 swap1(a, b) fmt.Println(a, b) swap2(&amp;a, &amp;b) fmt.Println(a, b) a, b = swap3(a, b) fmt.Println(a, b)&#125;===================================output===================================3 44 33 4===================================output=================================== 数组1234567891011121314151617181920212223242526272829303132333435363738package mainimport &quot;fmt&quot;func main() &#123; //数组的定义，数量写在类型的前面 var arr1 [5]int arr2 := [3]int&#123;1,3,5&#125; //数组必须用...，否则是切片 arr3 := [...]int&#123;2,4,6,8,10&#125; //二维数组 var grid [4][5]int fmt.Println(arr1, arr2, arr3) fmt.Println(grid) //遍历数组 for i, v := range arr3 &#123; fmt.Println(i, v) &#125; fmt.Println(&quot;++++++++++&quot;) //可通过 _ 省略变量。不仅range，任何地方都可以通过 _ 省略变量 for _, v := range arr3 &#123; fmt.Println(v) &#125;&#125;===================================output===================================[0 0 0 0 0] [1 3 5] [2 4 6 8 10][[0 0 0 0 0] [0 0 0 0 0] [0 0 0 0 0] [0 0 0 0 0]]0 21 42 63 84 10++++++++++246810===================================output=================================== 数组是值类型 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package mainimport &quot;fmt&quot;func printArray(arr [5]int) &#123; arr[0] = 100 for i, v := range arr &#123; fmt.Println(i, v) &#125;&#125;func main() &#123; //数组的定义，数量写在类型的前面 var arr1 [5]int //数组必须用...，否则是切片 arr3 := [...]int&#123;2,4,6,8,10&#125; printArray(arr1) fmt.Println(&quot;+++++++++++++&quot;) printArray(arr3) fmt.Println(&quot;+++++++++++++&quot;) fmt.Println(arr1, arr3)&#125;===================================output===================================0 1001 02 03 04 0+++++++++++++0 1001 42 63 84 10+++++++++++++[0 0 0 0 0] [2 4 6 8 10]===================================output===================================//[10]int和[20]int是不同类型，调用func f(arr [10]int)会拷贝数组package mainimport &quot;fmt&quot;func printArray(arr *[5]int) &#123; arr[0] = 100 for i, v := range arr &#123; fmt.Println(i, v) &#125;&#125;func main() &#123; //数组的定义，数量写在类型的前面 var arr1 [5]int //数组必须用...，否则是切片 arr3 := [...]int&#123;2,4,6,8,10&#125; printArray(&amp;arr1) fmt.Println(&quot;+++++++++++++&quot;) printArray(&amp;arr3) fmt.Println(&quot;+++++++++++++&quot;) fmt.Println(arr1, arr3)&#125;===================================output===================================0 1001 02 03 04 0+++++++++++++0 1001 42 63 84 10+++++++++++++[100 0 0 0 0] [100 4 6 8 10]===================================output===================================//go语言一般不直接使用数组 Slice（切片）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103package mainimport &quot;fmt&quot;func main() &#123; arr := [...]int&#123;0, 1, 2, 3, 4, 5, 6, 7&#125; fmt.Println(&quot;arr[2:6] = &quot;, arr[2:6]) fmt.Println(&quot;arr[:6] = &quot;, arr[:6]) fmt.Println(&quot;arr[2:] = &quot;, arr[2:]) fmt.Println(&quot;arr[:] = &quot;, arr[:])&#125;===================================output===================================arr[2:6] = [2 3 4 5]arr[:6] = [0 1 2 3 4 5]arr[2:] = [2 3 4 5 6 7]arr[:] = [0 1 2 3 4 5 6 7]===================================output===================================//文档中说slice是array的viewpackage mainimport &quot;fmt&quot;func updateSlice(s []int) &#123; s[0] = 100&#125;func main() &#123; arr := [...]int&#123;0, 1, 2, 3, 4, 5, 6, 7&#125; fmt.Println(&quot;arr[2:6] = &quot;, arr[2:6]) fmt.Println(&quot;arr[:6] = &quot;, arr[:6]) s1 := arr[2:] fmt.Println(&quot;s1 = &quot;, s1) s2 := arr[:] fmt.Println(&quot;s2 = &quot;, s2) fmt.Println(&quot;After updateSlice(s1)&quot;) updateSlice(s1) fmt.Println(s1) fmt.Println(arr) fmt.Println(&quot;After updateSlice(s2)&quot;) updateSlice(s2) fmt.Println(s2) fmt.Println(arr) fmt.Println(&quot;Reslice&quot;) fmt.Println(s2) s2 = s2[:5] fmt.Println(s2) s2 = s2[2:] fmt.Println(s2)&#125;===================================output===================================arr[2:6] = [2 3 4 5]arr[:6] = [0 1 2 3 4 5]s1 = [2 3 4 5 6 7]s2 = [0 1 2 3 4 5 6 7]After updateSlice(s1)[100 3 4 5 6 7][0 1 100 3 4 5 6 7]After updateSlice(s2)[100 1 100 3 4 5 6 7][100 1 100 3 4 5 6 7]Reslice[100 1 100 3 4 5 6 7][100 1 100 3 4][100 3 4]===================================output===================================//数组版，slicepackage mainimport &quot;fmt&quot;func printArray(arr []int) &#123; arr[0] = 100 for i, v := range arr &#123; fmt.Println(i, v) &#125;&#125;func main() &#123; //数组的定义，数量写在类型的前面 var arr1 [5]int //数组必须用...，否则是切片 arr3 := [...]int&#123;2,4,6,8,10&#125; //**********使用arr1[:]获得数组的切片********** printArray(arr1[:]) fmt.Println(&quot;+++++++++++++&quot;) printArray(arr3[:]) fmt.Println(&quot;+++++++++++++&quot;) fmt.Println(arr1, arr3)&#125;===================================output===================================0 1001 02 03 04 0+++++++++++++0 1001 42 63 84 10+++++++++++++[100 0 0 0 0] [100 4 6 8 10]===================================output===================================//slice本身没有数据，是对底层array的一个view//arr的值变为[0 1 10 3 4 5 6 7] Slice的扩展 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package mainimport &quot;fmt&quot;func main() &#123; arr := [...]int&#123;0, 1, 2, 3, 4, 5, 6, 7&#125; fmt.Println(&quot;Extending slice&quot;) arr[0], arr[2] = 0, 2 s1 := arr[2:6] s2 := s1[3:5] fmt.Println(&quot;s1=&quot;, s1) fmt.Println(&quot;s2=&quot;, s2)&#125;===================================output===================================Extending slices1= [2 3 4 5]s2= [5 6]===================================output===================================//6不在s1中，slice是怎么取到的？slice是数组的view//s2:=arr[3:5] [0 1 2] s2下标，下标2在s2中是看不到的//s1:=arr[2:6] [0 1 2 3 4 5] s1下标，下标4、5在s1中是看不到的，s1[4]会报错//arr [0 1 2 3 4 5 6 7] 数组中数值//slice中有ptr、len、cap，只能取值到len的值，越界会报错//slice可以向后扩展，不可以向前扩展//s[i]不可以超越len(s)，向后扩展不可以超越底层数组cap(s)package mainimport &quot;fmt&quot;func main() &#123; arr := [...]int&#123;0, 1, 2, 3, 4, 5, 6, 7&#125; fmt.Println(&quot;Extending slice&quot;) arr[0], arr[2] = 0, 2 fmt.Println(&quot;arr =&quot;, arr) s1 := arr[2:6] s2 := s1[3:5] fmt.Printf(&quot;s1=%v, len(s1)=%d, cap(s1)=%d\n&quot;, s1, len(s1), cap(s1)) fmt.Printf(&quot;s2=%v, len(s2)=%d, cap(s2)=%d\n&quot;, s2, len(s2), cap(s2)) //我的理解是下标虽然取不到，但是可以扩展 fmt.Println(s1[3:6])&#125;===================================output===================================Extending slicearr = [0 1 2 3 4 5 6 7]s1=[2 3 4 5], len(s1)=4, cap(s1)=6s2=[5 6], len(s2)=2, cap(s2)=3[5 6 7]===================================output=================================== 向Slice添加元素 1234567891011121314151617181920212223242526272829303132package mainimport &quot;fmt&quot;func main() &#123; arr := [...]int&#123;0, 1, 2, 3, 4, 5, 6, 7&#125; fmt.Println(&quot;Extending slice&quot;) arr[0], arr[2] = 0, 2 fmt.Println(&quot;arr =&quot;, arr) s1 := arr[2:6] s2 := s1[3:5] fmt.Printf(&quot;s1=%v, len(s1)=%d, cap(s1)=%d\n&quot;, s1, len(s1), cap(s1)) fmt.Printf(&quot;s2=%v, len(s2)=%d, cap(s2)=%d\n&quot;, s2, len(s2), cap(s2)) fmt.Println(s1[3:6]) s3 := append(s2, 10) s4 := append(s3, 11) //append超出arr的下标范围就不是原来的arr了，是一个新的array，新的array会设置的长一些 s5 := append(s4, 12) //go语言会开辟一个新的array，把arr中的元素拷贝过去 fmt.Println(&quot;s3, s4, s5 =&quot;, s3, s4, s5) // s4 and s5 no longer view arr. fmt.Println(&quot;arr =&quot;, arr)&#125;===================================output===================================Extending slicearr = [0 1 2 3 4 5 6 7]s1=[2 3 4 5], len(s1)=4, cap(s1)=6s2=[5 6], len(s2)=2, cap(s2)=3[5 6 7]s3, s4, s5 = [5 6 10] [5 6 10 11] [5 6 10 11 12]arr = [0 1 2 3 4 5 6 10]===================================output===================================//添加元素时如果超越cap，系统会重新分配更大的底层数组//由于值传递的关系，必须接收append的返回值//s = append(s, val) Slice的创建，拷贝，删除 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package mainimport &quot;fmt&quot;func printSlice(s []int) &#123; fmt.Printf(&quot;%v, len=%d, cap=%d\n&quot;, s, len(s), cap(s))&#125;func main() &#123; fmt.Println(&quot;Creating slice&quot;) var s []int //Zero value for slice is nil for i := 0; i &lt; 10 ; i++ &#123; printSlice(s) s = append(s, 2 * i + 1) &#125; fmt.Println(s) s1 := []int&#123;2, 4, 6, 8&#125; printSlice(s1) s2 := make([]int, 16) printSlice(s2) s3 := make([]int, 10, 32) printSlice(s3) fmt.Println(&quot;Copying slice&quot;) copy(s2, s1) printSlice(s2) fmt.Println(&quot;Deleting elements from slice&quot;) s2 = append(s2[:3], s2[4:]...) printSlice(s2) fmt.Println(&quot;Popping from front&quot;) front := s2[0] s2 = s2[1:] fmt.Println(front) printSlice(s2) fmt.Println(&quot;Popping from back&quot;) tail := s2[len(s2) - 1] s2 = s2[:len(s2) - 1] fmt.Println(tail) printSlice(s2)&#125;===================================output===================================Creating slice[], len=0, cap=0[1], len=1, cap=1[1 3], len=2, cap=2[1 3 5], len=3, cap=4[1 3 5 7], len=4, cap=4[1 3 5 7 9], len=5, cap=8[1 3 5 7 9 11], len=6, cap=8[1 3 5 7 9 11 13], len=7, cap=8[1 3 5 7 9 11 13 15], len=8, cap=8[1 3 5 7 9 11 13 15 17], len=9, cap=16[1 3 5 7 9 11 13 15 17 19][2 4 6 8], len=4, cap=4[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], len=16, cap=16[0 0 0 0 0 0 0 0 0 0], len=10, cap=32Copying slice[2 4 6 8 0 0 0 0 0 0 0 0 0 0 0 0], len=16, cap=16Deleting elements from slice[2 4 6 0 0 0 0 0 0 0 0 0 0 0 0], len=15, cap=16Popping from front2[4 6 0 0 0 0 0 0 0 0 0 0 0 0], len=14, cap=15Popping from back0[4 6 0 0 0 0 0 0 0 0 0 0 0], len=13, cap=15===================================output=================================== Map定义：map[key]value map[key_1]map[key_2]value 创建：make(map[string]int) 获取元素：m[key] key不存在时，获得value类型的初始值 用value, ok := m[key]来判断是否存在key 用delete删除一个key 使用range遍历key，或者遍历key, value对 不保证遍历顺序，如需顺序，需手动对key排序 使用len获取元素个数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package mainimport &quot;fmt&quot;func main() &#123; // map的定义 m := map[string]string &#123; &quot;name&quot;: &quot;ccmouse&quot;, &quot;course&quot;: &quot;golang&quot;, &quot;site&quot;: &quot;imooc&quot;, &quot;quality&quot;: &quot;notbad&quot;, &#125; m2 := make(map[string]int) // m2 == empty map var m3 map[string]int // m3 == nil fmt.Println(m, m2, m3) // map的遍历 fmt.Println(&quot;Traversing map&quot;) for k, v := range m &#123; fmt.Println(k, v) &#125; // 获取value fmt.Println(&quot;Getting values&quot;) courseName, ok := m[&quot;course&quot;] fmt.Println(courseName, ok) // key拼错了怎么办？会打印空串 causeName, ok := m[&quot;cause&quot;] fmt.Println(causeName, ok) if causeName, ok := m[&quot;cause&quot;]; ok &#123; fmt.Println(causeName) &#125;else &#123; fmt.Println(&quot;key does not exist&quot;) &#125; // 删除 fmt.Println(&quot;Deleting values&quot;) name, ok := m[&quot;name&quot;] fmt.Println(name, ok) delete(m, &quot;name&quot;) name, ok = m[&quot;name&quot;] fmt.Println(name, ok)&#125;===================================output===================================map[name:ccmouse course:golang site:imooc quality:notbad] map[] map[]Traversing mapsite imoocquality notbadname ccmousecourse golangGetting valuesgolang true falsekey does not existDeleting valuesccmouse true false===================================output=================================== map的key map使用哈希表，必须可以比较相等 除了slice，map，function的内建类型都可以作为key struct类型不包含上述字段，也可作为key 1234567891011121314151617181920212223242526272829303132333435363738394041424344例：寻找最长不含有重复字符的子串abcabcbb -&gt; abcbbbbb -&gt; bpwwkew -&gt; wke思路： 对于每一个字母x lastOccurred[x]不存在，或者 &lt; start -----&gt; 无需操作 lastOccurred[x] &gt;= start -----&gt; 更新start 更新lastOccurred[x]，更新maxLengthpackage mainimport &quot;fmt&quot;func lengthOfNonRepeatingSubStr(s string) int &#123; lastOccurred := make(map[byte]int) start := 0 maxLength := 0 for i, ch := range []byte(s) &#123; if lastI, ok := lastOccurred[ch]; ok &amp;&amp; lastI &gt;= start &#123; //if lastOccurred[ch] &gt;= start &#123; start = lastI + i &#125; if i - start + 1 &gt; maxLength &#123; maxLength = i - start + 1 &#125; lastOccurred[ch] = i &#125; return maxLength&#125;func main() &#123; fmt.Println(lengthOfNonRepeatingSubStr(&quot;abcabcbb&quot;)) fmt.Println(lengthOfNonRepeatingSubStr(&quot;bbbbb&quot;)) fmt.Println(lengthOfNonRepeatingSubStr(&quot;pwwkew&quot;)) fmt.Println(lengthOfNonRepeatingSubStr(&quot;&quot;)) fmt.Println(lengthOfNonRepeatingSubStr(&quot;b&quot;)) fmt.Println(lengthOfNonRepeatingSubStr(&quot;abcdef&quot;))&#125;===================================output===================================313016===================================output=================================== rune相当于go的char 使用range遍历pos，rune对 使用utf8.RuneCountInString获得字符数量 使用len获得字节长度 使用[]byte获得字节 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package mainimport ( &quot;fmt&quot; &quot;unicode/utf8&quot;)func main() &#123; s := &quot;Yes我爱慕课网!&quot; //UTF-8 fmt.Println(s) for _, b := range []byte(s) &#123; fmt.Printf(&quot;%X &quot;, b) &#125; fmt.Println() for i, ch := range s &#123; // ch is a rune fmt.Printf(&quot;(%d %X) &quot;, i ,ch) &#125; fmt.Println() fmt.Println(&quot;Rune count:&quot;, utf8.RuneCountInString(s)) bytes := []byte(s) for len(bytes) &gt; 0 &#123; ch, size := utf8.DecodeRune(bytes) bytes = bytes[size:] fmt.Printf(&quot;%c &quot;, ch) &#125; fmt.Println() for i, ch := range []rune(s) &#123; fmt.Printf(&quot;(%d %c) &quot;, i, ch) &#125; fmt.Println()&#125;===================================output===================================Yes我爱慕课网!59 65 73 E6 88 91 E7 88 B1 E6 85 95 E8 AF BE E7 BD 91 21 (0 59) (1 65) (2 73) (3 6211) (6 7231) (9 6155) (12 8BFE) (15 7F51) (18 21) Rune count: 9Y e s 我 爱 慕 课 网 ! (0 Y) (1 e) (2 s) (3 我) (4 爱) (5 慕) (6 课) (7 网) (8 !) ===================================output===================================package mainimport ( &quot;fmt&quot;)func lengthOfNonRepeatingSubStr(s string) int &#123; lastOccurred := make(map[rune]int) for k, v := range lastOccurred &#123; fmt.Printf(&quot;%k=%v, v=%v &quot;, k, v) &#125; start := 0 maxLength := 0 for i, ch := range []rune(s) &#123; lastI, ok := lastOccurred[ch] if ok &amp;&amp; lastI &gt;= start &#123; //if lastOccurred[ch] &gt;= start &#123; start = lastI + i &#125; if i - start + 1 &gt; maxLength &#123; maxLength = i - start + 1 &#125; lastOccurred[ch] = i &#125; return maxLength&#125;func main() &#123; fmt.Println(lengthOfNonRepeatingSubStr(&quot;abcabcbb&quot;)) fmt.Println(lengthOfNonRepeatingSubStr(&quot;bbbbb&quot;)) fmt.Println(lengthOfNonRepeatingSubStr(&quot;pwwkew&quot;)) fmt.Println(lengthOfNonRepeatingSubStr(&quot;&quot;)) fmt.Println(lengthOfNonRepeatingSubStr(&quot;b&quot;)) fmt.Println(lengthOfNonRepeatingSubStr(&quot;abcdef&quot;)) fmt.Println(lengthOfNonRepeatingSubStr(&quot;一二三二一&quot;)) fmt.Println(lengthOfNonRepeatingSubStr(&quot;一二三四五六&quot;))&#125;===================================output===================================31301636===================================output=================================== 其他字符串操作 Fields，Split，Join Contains，Index ToLower，ToUpper Trim，TrimRight，TrimLeft]]></content>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph-ansible project]]></title>
    <url>%2F2018%2F08%2F29%2Fceph-ansible-project%2F</url>
    <content type="text"><![CDATA[近期工作是调研teuthology，目标是把teuthology enable起来做ceph的测试使用，但没想到teuthology只是冰山一角，其关联了ceph社区的整套CI/CD，这套CI/CD关联项目多的力不从心，其关联的项目大致如下： 12345678https://github.com/ceph/shaman.githttps://github.com/ceph/teuthology.githttps://github.com/ceph/ceph-cm-ansible.githttps://github.com/ceph/pulpito.githttps://github.com/ceph/ceph-qa-suite.githttps://github.com/ceph/paddles.githttps://github.com/ceph/ceph.git还有很多未知的，shaman社区明确说是内部使用的，不提供文档，所以只能撸代码，从代码中找数据库的表结构，到目前还没时间找出来。https://github.com/ceph/shaman/issues/110 既然社区使用ansible，并且Sébastien Han一直维护，毕竟有他的道理，ansible在自动化运维领域也是很常用的（尤其在大规模部署的情况下），倒不如借着这个机会学习一下。 ansible基础语法看以下两个文章差不多够用，再有问题可以查ansible官网。 12https://www.w3cschool.cn/automate_with_ansible/https://www.cnblogs.com/f-ck-need-u/p/7567417.html 接着来看一下ceph-ansible项目，README中给出了文档，一个是master一个是stable-3.0。我没有那么傻，当然是看stable-3.0了，哈哈。 1https://github.com/ceph/ceph-ansible 首先是把ceph-ansible的playbooks clone下来。 12345[root@k8s-master ~]# yum install git -y[root@k8s-master ~]# git clone https://github.com/ceph/ceph-ansible.git[root@k8s-master ~]# cd ceph-ansible/# 当然branch也要对应文档，这样后续困难应该少一点[root@k8s-master ceph-ansible]# git checkout -b mystable-3.0 origin/stable-3.0 接着需要安装ansible，官方stable-3.0文档中说明了ceph-ansible项目各个branch，支持ansible版本和ceph版本。我们只看现在的branch，其他branch有需要再说。 1stable-3.0支持ceph版本jewel和luminous。该branch支持ansible版本2.3.1,2.3.2和2.4.2。 安装ansible 2.4.2使用yum安装指定版本的ansible 1234567891011121314151617181920212223242526[root@cephJ ~]# yum -h | grep show --showduplicates 在 list/search 命令下，显示源里重复的条目[root@cephJ ~]# yum --showduplicates list ansible已加载插件：fastestmirror, prioritiesLoading mirror speeds from cached hostfile * base: mirrors.cn99.com * epel: www.ftp.ne.jp * extras: mirrors.nwsuaf.edu.cn * nux-dextop: li.nux.ro * updates: mirrors.nwsuaf.edu.cn12 packages excluded due to repository priority protections可安装的软件包ansible.noarch 2.4.2.0-2.el7 extrasansible.noarch 2.7.5-1.el7 epel ### sudo yum install &lt;package name&gt;-&lt;version info&gt;[root@cephJ ~]# sudo yum install -y ansible-2.4.2.0[root@cephJ ~]# ansible --versionansible 2.4.2.0 config file = /etc/ansible/ansible.cfg configured module search path = [u&apos;/root/.ansible/plugins/modules&apos;, u&apos;/usr/share/ansible/plugins/modules&apos;] ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /usr/bin/ansible python version = 2.7.5 (default, Apr 11 2018, 07:36:10) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] 配置ceph-ansible准备四台机器 1234567ansible节点 ansible-masterceph节点 ansible-ceph-1 ansible-ceph-2 ansible-ceph-3 配置ansible 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101ansible节点执行1、首先clone代码[root@ansible-master ~]# git clone https://github.com/ceph/ceph-ansible.git &amp;&amp; cd ceph-ansible/2、创建本地分支并切换分支[root@ansible-master ceph-ansible]# git checkout -b myv3.2.0 v3.2.0 3、copy模板文件[root@ansible-master ceph-ansible]# cp site.yml.sample site.yml4、修改ansible机器清单(inventory)[root@ansible-master ~]# vim /etc/ansible/hosts[mons]ansible-ceph-[1:3] ansible_ssh_pass=yujiang2[osds]ansible-ceph-[1:3] ansible_ssh_pass=yujiang2[rgws]ansible-ceph-[1:3] ansible_ssh_pass=yujiang2[mgrs]ansible-ceph-[1:3] ansible_ssh_pass=yujiang25、批量推送sshkey[root@ansible-master ~]# ssh-keygen -t rsa[root@ansible-master ~]# cat push-ssh.yaml - hosts: all user: root tasks: - name: ssh-key-copy authorized_key: user=root key=&quot;&#123;&#123; lookup(&apos;file&apos;,&apos;/root/.ssh/id_rsa.pub&apos;)&#125;&#125;&quot; tags: - sshkey[root@ansible-master ~]# ansible-playbook push-ssh.yamlansible-ceph-1 : ok=2 changed=1 unreachable=0 failed=0 ansible-ceph-2 : ok=2 changed=1 unreachable=0 failed=0 ansible-ceph-3 : ok=2 changed=1 unreachable=0 failed=0 6、安装pip并安装ceph-ansible依赖[root@ansible-master ~]# curl &quot;https://bootstrap.pypa.io/get-pip.py&quot; -o &quot;get-pip.py&quot; &amp;&amp; python get-pip.py &amp;&amp; pip install --upgrade setuptools[root@ansible-master ceph-ansible]# pip install -r requirements.txt7、修改ansible变量[root@ansible-master group_vars]# pwd/root/ceph-ansible/group_vars[root@ansible-master group_vars]# cp all.yml.sample all.yml &amp;&amp; cp osds.yml.sample osds.yml &amp;&amp; cp mgrs.yml.sample mgrs.ymlall.yml修改如下：mon_group_name: monsosd_group_name: osdsntp_daemon_type: chronydceph_origin: repositoryceph_repository: communityceph_mirror: http://mirrors.163.com/cephceph_stable_key: http://mirrors.163.com/ceph/keys/release.ascceph_stable_release: luminousceph_stable_repo: &quot;&#123;&#123; ceph_mirror &#125;&#125;/rpm-&#123;&#123; ceph_stable_release &#125;&#125;/el7/x86_64&quot;monitor_interface: eth0public_network: 192.168.0.0/24cluster_network: 192.168.0.0/24radosgw_interface: eth0radosgw_address: 0.0.0.0osds.yml修改如下：copy_admin_key: truedevices: - /dev/vdbosd_scenario: collocatedmgrs.yml修改如下：ceph_mgr_modules: [status,dashboard]8、执行ansible开始部署ceph，部署哪些模块会在/etc/ansible/hosts中定义（下面是我们之前定义的），如果在该文件中没有对应模块定义，ansible会忽略该模块的部署。[mons]ansible-ceph-[1:3] ansible_ssh_pass=yujiang2[osds]ansible-ceph-[1:3] ansible_ssh_pass=yujiang2[rgws]ansible-ceph-[1:3] ansible_ssh_pass=yujiang2[mgrs]ansible-ceph-[1:3] ansible_ssh_pass=yujiang29、修改ansible host_key_checking[root@ansible-master ~]# vim /etc/ansible/ansible.cfg[defaults]host_key_checking = False[root@ansible-master ceph-ansible]# pwd/root/ceph-ansible[root@ansible-master ceph-ansible]# ansible-playbook site.ymlPLAY RECAP *******************************************************************************************************************************************************************************************************************************************************************ansible-ceph-1 : ok=310 changed=17 unreachable=0 failed=0 ansible-ceph-2 : ok=287 changed=17 unreachable=0 failed=0 ansible-ceph-3 : ok=289 changed=20 unreachable=0 failed=0 INSTALLER STATUS *************************************************************************************************************************************************************************************************************************************************************Install Ceph Monitor : Complete (0:00:37)Install Ceph Manager : Complete (0:01:05)Install Ceph OSD : Complete (0:00:38)Install Ceph RGW : Complete (0:00:30)]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7 虚拟机科学上网]]></title>
    <url>%2F2018%2F08%2F21%2Fcentos7-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[参考：https://cloud.tencent.com/developer/article/1122336 1234567891011121314151617181920212223242526[root@k8s-master ~]# yum groupinstall &quot;Development Tools&quot; -y &amp;&amp; yum install -y m2crypto python-setuptools &amp;&amp; easy_install pip &amp;&amp; python -m pip install --upgrade pip &amp;&amp; python -m pip install --upgrade setuptools &amp;&amp; pip install shadowsocks[root@k8s-master ~]# vi /etc/shadowsocks.json&#123; &quot;server&quot;: &quot;38.*.*.*&quot;, &quot;server_port&quot;: 7777, &quot;password&quot;: &quot;pwd&quot;, &quot;method&quot;: &quot;aes-256-cfb&quot;, &quot;local_address&quot;:&quot;127.0.0.1&quot;, &quot;local_port&quot;:1080&#125;[root@k8s-master ~]# sslocal -c /etc/shadowsocks.json[root@k8s-master ~]# yum install epel-release -y &amp;&amp; yum install -y privoxy[root@k8s-master ~]# vi /etc/privoxy/configforward-socks5t / 127.0.0.1:1080listen-address 127.0.0.1:8118[root@k8s-master ~]# vi ~/.bashrcexport http_proxy=http://127.0.0.1:8118export https_proxy=http://127.0.0.1:8118export ftp_proxy=http://127.0.0.1:8118[root@k8s-master ~]# source ~/.bashrc [root@k8s-master ~]# systemctl restart privoxy.service &amp;&amp; systemctl status privoxy.service]]></content>
      <tags>
        <tag>科学上网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s centos7 部署]]></title>
    <url>%2F2018%2F08%2F20%2Fk8s-centos7-%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[参考文章：针对以下文章的实际操作 【1】https://juejin.im/post/5b45d4185188251ac062f27c 【2】https://blog.qikqiak.com/post/use-kubeadm-install-kubernetes-1.10/ 虚拟机master节点和slave节点——Docker版本123456# 所有master节点和slave节点[root@k8s-master ~]# docker --versionDocker version 1.13.1, build dded712/1.13.1[root@k8s-master ~]# yum install -y docker &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker[root@k8s-slave ~]# yum install -y docker &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker 虚拟机master节点——安装etcd12345678910111213141516171819202122232425262728293031323334353637[root@k8s-master ~]# hostnamectl Static hostname: rhel7 Icon name: computer-vm Chassis: vm Machine ID: 5b097de8abcc4690b4cdd7ec9deefbc4 Boot ID: 89bd04baef274f4f9e8eceb107547e5a Virtualization: kvm Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 3.10.0-862.el7.x86_64 Architecture: x86-64[root@k8s-master ~]# yum install wget -y &amp;&amp; wget https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz &amp;&amp; tar zxvf etcd-v3.3.9-linux-amd64.tar.gz &amp;&amp; cd etcd-v3.3.9-linux-amd64[root@k8s-master etcd-v3.3.9-linux-amd64]# cp etcd /usr/bin/ &amp;&amp; cp etcdctl /usr/bin/[root@k8s-master ~]# vi /usr/lib/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confUser=root# set GOMAXPROCS to number of processorsExecStart=/bin/bash -c &quot;GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\&quot;$&#123;ETCD_NAME&#125;\&quot; --data-dir=\&quot;$&#123;ETCD_DATA_DIR&#125;\&quot; --listen-client-urls=\&quot;$&#123;ETCD_LISTEN_CLIENT_URLS&#125;\&quot;&quot;Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target[root@k8s-master ~]# systemctl daemon-reload &amp;&amp; systemctl start etcd &amp;&amp; systemctl enable etcd &amp;&amp; systemctl status etcd使用kubeadm安装要关闭etcd， 虚拟机master节点——安装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201# 所有master和slave节点，配置kubernetes源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF[root@k8s-master ~]# yum install -y kubelet kubeadm kubectl ipvsadm# 所有master和slave节点[root@k8s-master ~]# iptables -P FORWARD ACCEPT# 所有master和slave节点，配置转发相关参数cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1vm.swappiness=0EOF[root@k8s-master ~]# sysctl --system# 所有master和slave节点，加载ipvs相关内核模块[root@k8s-master ~]# modprobe ip_vs[root@k8s-master ~]# modprobe ip_vs_rr[root@k8s-master ~]# modprobe ip_vs_wrr[root@k8s-master ~]# modprobe ip_vs_sh[root@k8s-master ~]# modprobe nf_conntrack_ipv4[root@k8s-master ~]# lsmod | grep ip_vsip_vs_sh 12688 0 ip_vs_wrr 12697 0 ip_vs_rr 12600 0 ip_vs 141432 6 ip_vs_rr,ip_vs_sh,ip_vs_wrrnf_conntrack 133053 6 ip_vs,nf_nat,nf_nat_ipv4,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_ipv4libcrc32c 12644 4 xfs,ip_vs,nf_nat,nf_conntrack# 或持久化[root@k8s-master modules]# cat /etc/sysconfig/modules/k8s.modules#！/bin/sh/sbin/modinfo -F filename ip_vs &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe ip_vsfi/sbin/modinfo -F filename ip_vs_rr &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe ip_vs_rrfi/sbin/modinfo -F filename ip_vs_wrr &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe ip_vs_wrrfi/sbin/modinfo -F filename ip_vs_sh &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe ip_vs_shfi/sbin/modinfo -F filename nf_conntrack_ipv4 &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe nf_conntrack_ipv4fi[root@k8s-master modules]# chmod +x k8s.modules[root@k8s-master ~]# reboot[root@k8s-master ~]# lsmod | grep ip_vs# 所有master和slave节点，修改hosts文件[root@k8s-master ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.56.200 k8s-master k8s-master.localdomain192.168.56.201 k8s-slave k8s-slave.localdomain# 所有master和slave节点[root@k8s-master ~]# DOCKER_CGROUPS=$(docker info | grep &apos;Cgroup&apos; | cut -d&apos; &apos; -f3)[root@k8s-master ~]# echo $DOCKER_CGROUPS# 或持久化[root@k8s-master ~]# vi /etc/profileexport DOCKER_CGROUPS=$(docker info | grep &apos;Cgroup&apos; | cut -d&apos; &apos; -f3)[root@k8s-master ~]# source /etc/profile# 所有master和slave节点cat &gt;/etc/sysconfig/kubelet&lt;&lt;EOFKUBELET_EXTRA_ARGS=&quot;--cgroup-driver=$DOCKER_CGROUPS --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1&quot;EOF[root@k8s-master ~]# systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl start kubelet &amp;&amp; systemctl status kubelet# 永久关闭 注释/etc/fstab文件里swap相关的行[root@k8s-master ~]# swapoff -a# cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.2imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapi: advertiseAddress: 192.168.56.200controllerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16 kubeProxy: config: # mode: ipvs mode: iptablesEOF[root@k8s-master ~]# kubeadm config images pull --config kubeadm-master.config[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver-amd64:v1.11.2[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager-amd64:v1.11.2[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler-amd64:v1.11.2[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.11.2[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.2.18[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.1.3[root@k8s-master ~]# kubeadm init --config kubeadm-master.config[init] using Kubernetes version: v1.11.2[preflight] running pre-flight checksI0822 05:07:42.655181 8212 kernel_validator.go:81] Validating kernel versionI0822 05:07:42.655280 8212 kernel_validator.go:96] Validating kernel config[preflight] Some fatal errors occurred: [ERROR DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`[root@k8s-master ~]# rm -rf /var/lib/etcd/*[root@k8s-master ~]# kubeadm init --config kubeadm-master.config[init] using Kubernetes version: v1.11.2[preflight] running pre-flight checksI0822 05:08:00.364246 8239 kernel_validator.go:81] Validating kernel versionI0822 05:08:00.364505 8239 kernel_validator.go:96] Validating kernel config[preflight/images] Pulling images required for setting up a Kubernetes cluster[preflight/images] This might take a minute or two, depending on the speed of your internet connection[preflight/images] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[preflight] Activating the kubelet service[certificates] Generated ca certificate and key.[certificates] Generated apiserver certificate and key.[certificates] apiserver serving cert is signed for DNS names [k8s-master.localdomain kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.56.200][certificates] Generated apiserver-kubelet-client certificate and key.[certificates] Generated sa key and public key.[certificates] Generated front-proxy-ca certificate and key.[certificates] Generated front-proxy-client certificate and key.[certificates] Generated etcd/ca certificate and key.[certificates] Generated etcd/server certificate and key.[certificates] etcd/server serving cert is signed for DNS names [k8s-master.localdomain localhost] and IPs [127.0.0.1 ::1][certificates] Generated etcd/peer certificate and key.[certificates] etcd/peer serving cert is signed for DNS names [k8s-master.localdomain localhost] and IPs [192.168.56.200 127.0.0.1 ::1][certificates] Generated etcd/healthcheck-client certificate and key.[certificates] Generated apiserver-etcd-client certificate and key.[certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot;[controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;[controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;[controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot; [init] this might take a minute or longer if the control plane images have to be pulled[apiclient] All control plane components are healthy after 39.501843 seconds[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace[kubelet] Creating a ConfigMap &quot;kubelet-config-1.11&quot; in namespace kube-system with the configuration for the kubelets in the cluster[markmaster] Marking the node k8s-master.localdomain as master by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;[markmaster] Marking the node k8s-master.localdomain as master by adding the taints [node-role.kubernetes.io/master:NoSchedule][patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;k8s-master.localdomain&quot; as an annotation[bootstraptoken] using token: vyabge.9wi2cqgukrf3c2qh[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join 192.168.56.200:6443 --token vyabge.9wi2cqgukrf3c2qh --discovery-token-ca-cert-hash sha256:2224c75b200730114e0153aa7aea9014affd90ffc47cfe08b7f931ef8e58b6ab[root@k8s-master ~]# rm -rf $HOME/.kube[root@k8s-master ~]# mkdir -p $HOME/.kube[root@k8s-master ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config[root@k8s-master ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config[root@k8s-master ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master.localdomain NotReady master 2d v1.11.2 虚拟机master节点——安装flannel1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 下载配置文件[root@k8s-master ~]# mkdir flannel &amp;&amp; cd flannel[root@k8s-master flannel]# wget https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与上面kubeadm的pod-network一致 net-conf.json: | &#123; &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;Backend&quot;: &#123; &quot;Type&quot;: &quot;vxlan&quot; &#125; &#125;# 修改镜像 image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=enp0s8# 启动[root@k8s-master flannel]# kubectl apply -f kube-flannel.ymlclusterrole.rbac.authorization.k8s.io/flannel createdclusterrolebinding.rbac.authorization.k8s.io/flannel createdserviceaccount/flannel createdconfigmap/kube-flannel-cfg createddaemonset.extensions/kube-flannel-ds created# 查看[root@k8s-master flannel]# kubectl get pods --namespace kube-systemNAME READY STATUS RESTARTS AGEcoredns-777d78ff6f-2z75r 0/1 ContainerCreating 0 2dcoredns-777d78ff6f-rghp9 0/1 ContainerCreating 0 2detcd-k8s-master.localdomain 1/1 Running 1 2dkube-apiserver-k8s-master.localdomain 1/1 Running 1 2dkube-controller-manager-k8s-master.localdomain 1/1 Running 1 2dkube-flannel-ds-jftcs 1/1 Running 0 19skube-proxy-wvsxb 1/1 Running 1 2dkube-scheduler-k8s-master.localdomain 1/1 Running 1 2d[root@k8s-master flannel]# kubectl get svc --namespace kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP 2d 配置slave节点加入集群1[root@k8s-master ~]# kubeadm join 192.168.56.201:6443 --token vliohs.l5lz1treou1srvt1 --discovery-token-ca-cert-hash sha256:0c08eeb897ef635c188770ef42e82580326f5ff534be49cc1a80fd8785173495 极客时间部署方式 master节点 12345678910111213141516171819# 添加kubernetes源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=0 EOF# 安装kubelet、kubeadm、kubectl、kubernetes-cnisudo yum -y install epel-release &amp;&amp; yum clean all &amp;&amp; yum makecachesudo yum -y install kubelet-1.11.1 kubeadm-1.11.1 kubectl-1.11.1 kubernetes-cni# 安装dockersudo yum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.reposudo yum install docker-ce -y &amp;&amp; sudo systemctl start docker &amp;&amp; sudo systemctl enable docker]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker regsitry搭建]]></title>
    <url>%2F2018%2F08%2F18%2Fdocker-regsitry%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[参考文章：https://www.jianshu.com/p/fc36368b5c44 mac virtualbox 制作两台虚拟机进行测试 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879机器名：k8s-master[root@k8s-master ~]# cat /etc/hostname k8s-master.localdomain[root@k8s-master ~]# systemctl stop firewalld.service[root@k8s-master ~]# systemctl disable firewalld.service[root@k8s-master ~]# cat /etc/sysconfig/selinux # This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of three two values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection.SELINUXTYPE=targeted[root@k8s-master ~]# cat /etc/sysconfig/network-scripts/ifcfg-enp0s8 TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp0s8DEVICE=enp0s8ONBOOT=yesIPADDR=192.168.56.200 #静态IPGATEWAY=192.168.56.1 #默认网关NETMASK=255.255.255.0 #子网掩码DNS1=192.168.56.1 #DNS 配置机器名：k8s-slave[root@k8s-slave ~]# cat /etc/hostname k8s-slave.localdomain[root@k8s-slave ~]# systemctl stop firewalld.service[root@k8s-slave ~]# systemctl disable firewalld.service[root@k8s-slave ~]# cat /etc/sysconfig/selinux# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of three two values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection.SELINUXTYPE=targeted[root@k8s-slave ~]# cat /etc/sysconfig/network-scripts/ifcfg-enp0s8 TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp0s8DEVICE=enp0s8ONBOOT=yesIPADDR=192.168.56.201 #静态IPGATEWAY=192.168.56.1 #默认网关NETMASK=255.255.255.0 #子网掩码DNS1=192.168.56.1 #DNS 配置 两台虚拟机安装docker并启动服务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@k8s-master ~]# yum -y install docker[root@k8s-master ~]# systemctl start docker.service[root@k8s-master ~]# systemctl status docker.service● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled) Active: active (running) since 六 2018-08-18 11:10:58 EDT; 6s ago Docs: http://docs.docker.com Main PID: 1522 (dockerd-current) CGroup: /system.slice/docker.service ├─1522 /usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --default-runtime=docker-runc --exec-opt native.cgroupdriver=systemd ... └─1526 /usr/bin/docker-containerd-current -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/d...8月 18 11:10:58 k8s-master.localdomain dockerd-current[1522]: time=&quot;2018-08-18T11:10:58.149875557-04:00&quot; level=warning msg=&quot;Docker could not enable SELinux on the host system&quot;8月 18 11:10:58 k8s-master.localdomain dockerd-current[1522]: time=&quot;2018-08-18T11:10:58.178512067-04:00&quot; level=info msg=&quot;Graph migration to content-addressability took ... seconds&quot;8月 18 11:10:58 k8s-master.localdomain dockerd-current[1522]: time=&quot;2018-08-18T11:10:58.179123980-04:00&quot; level=info msg=&quot;Loading containers: start.&quot;8月 18 11:10:58 k8s-master.localdomain dockerd-current[1522]: time=&quot;2018-08-18T11:10:58.229107163-04:00&quot; level=info msg=&quot;Firewalld running: false&quot;8月 18 11:10:58 k8s-master.localdomain dockerd-current[1522]: time=&quot;2018-08-18T11:10:58.312308773-04:00&quot; level=info msg=&quot;Default bridge (docker0) is assigned with an IP... address&quot;8月 18 11:10:58 k8s-master.localdomain dockerd-current[1522]: time=&quot;2018-08-18T11:10:58.386055892-04:00&quot; level=info msg=&quot;Loading containers: done.&quot;8月 18 11:10:58 k8s-master.localdomain dockerd-current[1522]: time=&quot;2018-08-18T11:10:58.403922614-04:00&quot; level=info msg=&quot;Daemon has completed initialization&quot;8月 18 11:10:58 k8s-master.localdomain dockerd-current[1522]: time=&quot;2018-08-18T11:10:58.403960859-04:00&quot; level=info msg=&quot;Docker daemon&quot; commit=&quot;dded712/1.13.1&quot; graphdri...on=1.13.18月 18 11:10:58 k8s-master.localdomain systemd[1]: Started Docker Application Container Engine.8月 18 11:10:58 k8s-master.localdomain dockerd-current[1522]: time=&quot;2018-08-18T11:10:58.427377761-04:00&quot; level=info msg=&quot;API listen on /var/run/docker.sock&quot;Hint: Some lines were ellipsized, use -l to show in full.[root@k8s-slave ~]# yum -y install docker[root@k8s-slave ~]# systemctl start docker.service[root@k8s-slave ~]# systemctl status docker.service● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled) Active: active (running) since 六 2018-08-18 11:11:14 EDT; 3s ago Docs: http://docs.docker.com Main PID: 1517 (dockerd-current) CGroup: /system.slice/docker.service ├─1517 /usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --default-runtime=docker-runc --exec-opt native.cgroupdriver=systemd ... └─1521 /usr/bin/docker-containerd-current -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/d...8月 18 11:11:14 k8s-slave.localdomain dockerd-current[1517]: time=&quot;2018-08-18T11:11:14.647943371-04:00&quot; level=warning msg=&quot;Docker could not enable SELinux on the host system&quot;8月 18 11:11:14 k8s-slave.localdomain dockerd-current[1517]: time=&quot;2018-08-18T11:11:14.678117451-04:00&quot; level=info msg=&quot;Graph migration to content-addressability took 0.00 seconds&quot;8月 18 11:11:14 k8s-slave.localdomain dockerd-current[1517]: time=&quot;2018-08-18T11:11:14.678885165-04:00&quot; level=info msg=&quot;Loading containers: start.&quot;8月 18 11:11:14 k8s-slave.localdomain dockerd-current[1517]: time=&quot;2018-08-18T11:11:14.732225305-04:00&quot; level=info msg=&quot;Firewalld running: false&quot;8月 18 11:11:14 k8s-slave.localdomain dockerd-current[1517]: time=&quot;2018-08-18T11:11:14.822182514-04:00&quot; level=info msg=&quot;Default bridge (docker0) is assigned with an IP ... address&quot;8月 18 11:11:14 k8s-slave.localdomain dockerd-current[1517]: time=&quot;2018-08-18T11:11:14.903524638-04:00&quot; level=info msg=&quot;Loading containers: done.&quot;8月 18 11:11:14 k8s-slave.localdomain dockerd-current[1517]: time=&quot;2018-08-18T11:11:14.927271723-04:00&quot; level=info msg=&quot;Daemon has completed initialization&quot;8月 18 11:11:14 k8s-slave.localdomain dockerd-current[1517]: time=&quot;2018-08-18T11:11:14.927306463-04:00&quot; level=info msg=&quot;Docker daemon&quot; commit=&quot;dded712/1.13.1&quot; graphdriv...on=1.13.18月 18 11:11:14 k8s-slave.localdomain systemd[1]: Started Docker Application Container Engine.8月 18 11:11:14 k8s-slave.localdomain dockerd-current[1517]: time=&quot;2018-08-18T11:11:14.952806378-04:00&quot; level=info msg=&quot;API listen on /var/run/docker.sock&quot;Hint: Some lines were ellipsized, use -l to show in full. 下载registry镜像并启动registry容器 12345[root@k8s-master ~]# docker pull registry[root@k8s-master ~]# docker run -d --name registry-container --restart always -p 5000:5000 -v /data/docker/registry:/tmp/registry docker.io/registry98dfeb9dc851f54c725ee4a27f058f71ef926b3832f61a4baf8514d354f63531[root@k8s-master ~]# curl -X GET 127.0.0.1:5000/v2/_catalog&#123;&quot;repositories&quot;:[]&#125; 测试镜像仓库 123456789101112131415161718192021[root@k8s-master ~]# docker pull centos:7.5.1804[root@k8s-master ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/centos 7.5.1804 fdf13fa91c6e 11 days ago 200 MBdocker.io/registry latest b2b03e9146e1 6 weeks ago 33.3 MB[root@k8s-master ~]# docker tag fdf13fa91c6e 192.168.56.200:5000/centos[root@k8s-master ~]# vi /etc/sysconfig/dockerOPTIONS=&apos;--selinux-enabled --log-driver=journald --signature-verification=false --insecure-registry 192.168.56.200:5000&apos;[root@k8s-master ~]# systemctl restart docker[root@k8s-master ~]# docker push 192.168.56.200:5000/centosThe push refers to a repository [192.168.56.200:5000/centos]bcc97fbfc9e1: Pushed latest: digest: sha256:7c14180942615fef85cb5c8b1388e028be1a8f79694a5fa30a4025173e42ad61 size: 529[root@k8s-master ~]# curl -X GET http://192.168.56.200:5000/v2/_catalog&#123;&quot;repositories&quot;:[&quot;centos&quot;]&#125;[root@k8s-master ~]# curl -X GET http://192.168.56.200:5000/v2/centos/tags/list&#123;&quot;name&quot;:&quot;centos&quot;,&quot;tags&quot;:[&quot;latest&quot;]&#125; 客户端测试 12345678910111213[root@k8s-slave ~]# vi /etc/sysconfig/dockerOPTIONS=&apos;--selinux-enabled --log-driver=journald --signature-verification=false --insecure-registry 192.168.56.200:5000&apos;[root@k8s-slave ~]# systemctl restart docker[root@k8s-slave ~]# docker pull 192.168.56.200:5000/centosUsing default tag: latestTrying to pull repository 192.168.56.200:5000/centos ... latest: Pulling from 192.168.56.200:5000/centos7dc0dca2b151: Extracting [================================================&gt; ] 71.86 MB/74.69 MB[root@k8s-slave ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE192.168.56.200:5000/centos latest fdf13fa91c6e 11 days ago 200 MB 制作nginx镜像并上传到私有仓库 1234567891011121314151617181920212223[root@k8s-master ~]# docker pull ubuntu:18.04[root@k8s-master ubuntu-nginx-dockerfile]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE192.168.56.200:5000/centos latest fdf13fa91c6e 11 days ago 200 MBdocker.io/centos 7.5.1804 fdf13fa91c6e 11 days ago 200 MBdocker.io/ubuntu 18.04 735f80812f90 3 weeks ago 83.5 MBdocker.io/registry latest b2b03e9146e1 6 weeks ago 33.3 MB[root@k8s-master ubuntu-nginx-dockerfile]# vi Dockerfile# 指定基于的基础镜像FROM ubuntu:18.04# 维护者信息MAINTAINER yujiang# 更新软件RUN sed -i &apos;s/http:\/\/archive\.ubuntu\.com\/ubuntu\//http:\/\/mirrors\.aliyun\.com\/ubuntu\//g&apos; /etc/apt/sources.listRUN apt-get update# 安装软件RUN apt-get install nginx net-tools curl vim -y# 允许指定的端口EXPOSE 80CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;][root@k8s-master ubuntu-nginx-dockerfile]# docker build -t 192.168.56.200/nginx:v1.0 .]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crontab]]></title>
    <url>%2F2018%2F08%2F04%2Fcrontab%2F</url>
    <content type="text"><![CDATA[课程地址：https://www.imooc.com/learn/1009 安装1yum install cronie crontabs -y 验证CROND服务验证crond服务和crontab工具（centos7) 检查crond服务是否安装及启动： 1yum list cronie &amp;&amp; systemctl status crond 检查crontab工具是否安装： 1yum list crontabs &amp;&amp; which crontab &amp;&amp; crontab -l CRONTAB架构12345 文件 解析 守护进程 crontab工具* * * * * ==========&gt; CRONDmy command按照格式编写定时任务 定时检查是否有任务需要执行 例如： 1234567(1)编辑任务列表crontab -e(2)查看任务列表crontab -l(3)重启crond服务，查看crond服务状态systemctl restart crondsystemctl status crond CRONTAB文件格式123456789101112131415161718* * * * * my command分 时 日 月 周 要运行的命令分：范围0-59时：范围0-23日：范围1-31月：范围1-12周：范围0-6my command：范围命令或脚本*：取值范围内的数字，通常代表对应时间区间内所涵盖的所有数字/：代表每，通常与*组合。例如2/*在&quot;分&quot;这个占位符中代表每两分钟，5/*在&quot;时&quot;这个占位符代表每5个小时-：代表某个数字到某个数字之间的区间，2-10在&quot;分&quot;占位符中代表第2分钟到第10分钟,：分开几个离散的数字。2,10 代表第2分钟和第10分钟例子： crontab -e * * * * * echo -e &quot;Hello&quot; &gt; /root/crontabtest.output CRONTAB配置文件系统配置文件 123456789101112131415############### /etc/crontab ###############SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=root# For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed 系统用户crontab配置文件保存目录（crontab -e） 1234############### /var/spool/cron/ ###############文件以linux用户区分root：/var/spool/cron/rootuser01：/var/spool/cron/user01 CRONTAB环境变量添加PATH到/etc/crontab 1234############### /etc/crontab ###############PATH=/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/jdk1.8.0_111/bin* * * * * root java -version 2&gt; /root/temp.out 在执行具体任务前引入系统/用户环境变量（推荐） 12330 2 * * * source /etc/profile;sh /root/test.sh30 2 * * * source ~/.bash_profile;sh /root/test.shsystemctl restart crond CRONTAB日志Cron日志保存在系统目录/var/log/cron 1tail -n 2 /var/log/cron 实战CRONTAB清理系统日志 123查看当前目录所有文件大小# du -sh ** 1 * * * cat /dev/null &gt; /var/log/messages]]></content>
      <tags>
        <tag>crontab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[setup a Ceph cluster using ceph-ansible]]></title>
    <url>%2F2018%2F07%2F29%2Fsetup-a-Ceph-cluster-using-ceph-ansible%2F</url>
    <content type="text"><![CDATA[原文地址：https://gist.github.com/elsonrodriguez/a26cdb7f04a9e94c480457e12458954a Quick and Dirty Ceph Cluster这是一个关于如何使用ceph-ansible设置Ceph集群的简明指南。 本指南假设： Ansible安装在您的本地计算机上 8个Centos 7.2节点使用XFS进行配置和格式化 您对节点有ssh和sudo访问权限 Ansible Setup首先，克隆ceph-ansible repo。 123git clone https://github.com/ceph/ceph-ansible.gitcd ceph-ansiblegit checkout a41eddf6f0d840580bc842fa59751defef10fa69 接下来，在repo的基础上添加一个inventory文件。使用以下内容作为指南，将示例IP替换为您自己的计算机IP。 12345678910111213[mons]10.0.0.110.0.0.210.0.0.3[osds]10.0.0.410.0.0.510.0.0.610.0.0.7[clients]10.0.0.8 我们需要至少3个MON（cluster state daemons）4个OSD（object storage daemons）和1个client（to mount the storage）。 接下来，我们需要复制samples。 1234cp site.yml.sample site.ymlcp group_vars/all.sample group_vars/allcp group_vars/mons.sample group_vars/monscp group_vars/osds.sample group_vars/osds 现在，我们需要修改group_vars/all中的一些变量： 1234567ceph_origin: &apos;upstream&apos; ceph_stable: truemonitor_interface: eth0public_network: 172.20.0.0/16journal_size: 5120 在groups_vars/osd中： 123osd_directory: trueosd_directories: - /var/lib/ceph/osd/mydir1 Running ceph-ansible playbook创建inventory文件并修改变量后： 1ansible-playbook site.yml -vvvv -i inventory -u centos 完成后，您应该有一个正常运行的Ceph集群。 Interacting with Ceph]]></content>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph-ansible]]></title>
    <url>%2F2018%2F07%2F27%2Fceph-ansible%2F</url>
    <content type="text"><![CDATA[原文地址：http://docs.ceph.com/ceph-ansible/master/ ceph-ansibleAnsible playbooks for Ceph, the distributed filesystem. 分布式文件系统，Ceph Ansible playbooks。 Installation您可以按照以下步骤直接从GitHub上的源安装： Clone the repository: 1git clone https://github.com/ceph/ceph-ansible.git 接下来，您必须决定使用哪个ceph-ansible branch。 有稳定的branch可供选择，或者您可以使用master branch： 1git checkout $branch 接下来，使用pip和提供的requirements.txt来安装ansible和其他所需的python库： 1pip install -r requirements.txt Ansible on RHEL and CentOS您可以从 Ansible channel 获取并安装RHEL和CentOS上的Ansible。 On RHEL: 1subscription-manager repos --enable=rhel-7-server-ansible-2-rpms （CentOS不使用subscription-manager，默认情况下已启用“Extras” 。） 1sudo yum install ansible Ansible on Ubuntu您可以使用 Ansible PPA 在Ubuntu上获取Ansible。 123sudo add-apt-repository ppa:ansible/ansiblesudo apt updatesudo apt install ansible Releases应根据您的需求使用以下branches。stable-* branches已经过QE测试，并且在lifecycle内会得到backport修复。master branch应该被认为是实验性的，谨慎使用。 stable-3.0 支持ceph jewel和luminous版本。该branch需要Ansible 2.4版本。 stable-3.1 支持ceph luminous和mimic版本。该branch需要Ansible 2.4版本。 stable-3.2支持ceph luminous和mimic版本。该branch需要Ansible 2.6版本。 stable-4.0支持ceph nautilus版本。该branch需要Ansible 2.8版本。 master 支持Ceph@master版本。该branch需要Ansible 2.8版本。 Configuration and Usage该项目假设您具有ansible的基本知识，并已经准备好主机以供Ansible配置。在clone了ceph-ansible repository后，选择你的branch并且安装了ansible，然后你需要为你的ceph集群创建你的inventory文件，playbook和configuration。 Inventoryansible inventory文件定义集群中的主机以及每个主机在ceph集群中扮演的角色（role）。inventory文件的默认位置是/etc/ansible/hosts，但此文件可以放在任何位置，并与ansible-playbook的-i flag一起使用。 示例inventory文件如下所示： 123456789[mons]mon1mon2mon3[osds]osd1osd2osd3 123注意有关ansible inventories的更多信息，请参阅ansible文档：http://docs.ansible.com/ansible/latest/intro_inventory.html Playbook在部署群集时，必须有一个playbook传递给ansible-playbook命令。在ceph-ansible项目的根目录中有一个名为site.yml.sample的示例playbook。这个playbook应该适用于大多数用法，但它默认包含每个守护进程组（every daemon group），这可能不适合你的群集设置。执行以下步骤准备您的playbook： 重命名示例playbook：mv site.yml.sample site.yml 根据群集需求修改playbook 12注意重要的是你使用的playbook应放在ceph-ansible项目的root目录。这就是ansible能够找到ceph-ansible提供的roles的方式。 Configuration Validationceph-ansible项目通过ceph-validate 角色（role）提供配置验证（config validation）。如果您使用的是所提供的playbooks，此role将在部署前运行，以确保您的ceph-ansible配置正确。此检查仅确保您为群集提供了正确的配置，而不是生成healthy集群。例如，如果为monitor_address提供了错误的地址，则mon仍然无法加入群集。 验证失败的示例可能如下所示： 123456789101112TASK [ceph-validate : validate provided configuration] *************************task path: /Users/andrewschoen/dev/ceph-ansible/roles/ceph-validate/tasks/main.yml:3Wednesday 02 May 2018 13:48:16 -0500 (0:00:06.984) 0:00:18.803 ********* [ERROR]: [mon0] Validation failed for variable: osd_objectstore [ERROR]: [mon0] Given value for osd_objectstore: foo [ERROR]: [mon0] Reason: osd_objectstore must be either &apos;bluestore&apos; or &apos;filestore&apos; fatal: [mon0]: FAILED! =&gt; &#123; &quot;changed&quot;: false &#125; Supported Validationceph-validate角色（role）当前支持验证以下osd场景的配置： collocated（journal与data一块盘） non-collocated lvm ceph-validate角色（role）还验证了以下安装选项： ceph_origin 设置为 distro ceph_origin 设置为 repository ceph_origin 设置为 local ceph_repository 设置为 rhcs ceph_repository 设置为 dev ceph_repository 设置为 community Installation methodsCeph可以通过几种方法安装。 Installation methods Configurationceph集群的配置将通过ceph-ansible提供的ansible变量来设置。所有这些选项及其默认值都在ceph-ansible项目root目录下的group_vars/子目录中定义。Ansible将在group_vars/子目录中使用您的inventory文件或playbook配置。在group_vars/子目录中有许多ansible配置文件示例，它们通过文件名与每个ceph守护进程组（ceph daemon groups）关联。例如，osds.yml.sample包含OSD守护进程的所有默认配置。all.yml.sample文件是一个特殊的group_vars文件，适用于群集中的所有主机。 12注意有关设置group或主机特定配置的更多信息，请参阅ansible文档：http://docs.ansible.com/ansible/latest/intro_inventory.html#splitting-out-host-and-group-specific-data 最基本的，您必须告诉ceph-ansible要安装的ceph版本，安装方法，群集网络设置以及希望如何配置OSD。开始配置，请重命名group_vars/目录中的每个文件，让它不包含文件名末尾的.sample，取消要更改选项的注释并提供自己的值。 使用lvm批处理方法部署Ceph upstream的octopus版本，在group_vars/all.yml中如下配置： 123456789ceph_origin: repositoryceph_repository: communityceph_stable_release: octopuspublic_network: &quot;192.168.3.0/24&quot;cluster_network: &quot;192.168.4.0/24&quot;monitor_interface: eth1devices: - &apos;/dev/sda&apos; - &apos;/dev/sdb&apos; 所有安装都需要更改以下配置选项，但可能还有其他必需选项，具体取决于OSD方案选择或群集的其他方面。 ceph_origin ceph_stable_release public_network monitor_interface or monitor_address 部署RGW实例时，需要设置radosgw_interface或radosgw_address`选项。 ceph.conf Configuration File覆盖已定义 ceph.conf 中内容的方法是使用 ceph_conf_overrides 变量。这允许您使用 INI 格式指定配置选项。此变量可用于覆盖已在 ceph.conf 中定义的sections（请参见：roles/ceph-config/templates/ceph.conf.j2）或提供新的配置选项。 支持ceph.conf中的以下部分： [global] [mon] [osd] [mds] [client.rgw.{instance_name}] 例如： 123456ceph_conf_overrides: global: foo: 1234 bar: 5678 osd: osd_mkfs_type: ext4 12注意我们将不再接受修改ceph.conf模板的pull requests，除非它有助于部署。对于简单的配置调整，请使用ceph_conf_overrides变量。 用于配置每个ceph daemon types的完整文档，在以下各sections中。 OSD Configuration通过选择OSD场景并提供该场景所需的配置来设置OSD配置。从稳定4.0版的nautilus开始，唯一可用的场景是lvm。 OSD Scenarios 从stable-4.0开始，不再支持下列场景，因为他们与ceph-disk相关联： collocated non-collocated 自Ceph luminous发布以来，首选使用ceph-volume工具的lvm场景。任何其他方案都将导致弃用警告。 ceph-disk在ceph-ansible 3.2中被弃用，并且已经在Nautilus版本完全从Ceph本身中删除。 目前（从稳定版4.0开始），只有一个场景，默认为lvm，参见： lvm 因此不再需要配置osd_scenario，它默认为lvm。 上面提到的lvm场景支持containerized和non-containerized集群。提醒一下，可以通过将containerized_deployment设置为True来完成部署容器化集群。 lvm此场景使用ceph-volume来创建LVM的OSD，并且仅在Ceph luminous或更新的版本才可用。它会自动启用。 其他（可选）支持的设置： osd_objectstore：为OSD设置Ceph objectstore。可用选项是filestore或bluestore。你只能选择luminous或更高Ceph版本的bluestore。如果未设置，则默认为bluestore。 dmcrypt：使用dmcrypt在OSD上启用Ceph加密。如果未设置，则默认为false。 osds_per_device：为每个设备提供多个OSD（如果未设置，默认为1，也就是说1个设备1个OSD）。 Simple configuration使用这种方法，有关设备如何配置为预配 OSD 的决策都由 Ceph 工具完成的（本例中为ceph-volume lvm批处理）。在给定输入设备的情况下，几乎无法修改OSD的组合方式。 要使用此配置，必须配置使用OSD的原始设备路径填充devices选项。 123456789101112131415161718192021注意原始设备必须“干净”，没有gpt分区表或逻辑卷存在。[root@dev ~]# yum install gdisk -y[root@dev ~]# gdisk /dev/sda GPT fdisk (gdisk) version 0.8.10Partition table scan: MBR: not present BSD: not present APM: not present GPT: not presentCreating new GPT entries.Command (? for help): xExpert command (? for help): zAbout to wipe out GPT on /dev/sda. Proceed? (Y/N): YGPT data structures destroyed! You may now partition the disk using fdisk orother utilities.Blank out MBR? (Y/N): Y 例如，对于具有/dev/sda和/dev/sdb的节点，配置将是： 123devices: - /dev/sda - /dev/sdb 在上述情况下，如果两个设备都是spinning（旋转）设备（HDD），则会创建2个OSD，每个OSD都有自己的collocated journal（journal与data同盘）。 其他方式可能是spinning设备与solid state设备混合部署（HDD+SSD），例如： 1234devices: - /dev/sda - /dev/sdb - /dev/nvme0n1 与最初的示例类似，最终会生成2个OSD，但数据将放在较慢的spinning设备（/dev/sda, 和/dev/sdb）上，而journals将放在速度更快的solid state设备/dev/nvme0n1上。 ceph-volume工具在“batch”子命令部分详细描述了这一点 此选项也可以与osd_auto_discovery一起使用，这意味着您不需要直接填充devices ，而是使用ansible找到的任何适当的设备。 1osd_auto_discovery: true 其他（可选）支持的设置： crush_device_class：为使用此方法创建的所有OSD设置CRUSH device class（使用simple configuration方法不可能每个OSD都有CRUSH device class）。 值必须是字符串，如crush_device_class：&quot;ssd&quot; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990未使用crush_device_class# begin crush maptunable choose_local_tries 0tunable choose_local_fallback_tries 0tunable choose_total_tries 50tunable chooseleaf_descend_once 1tunable chooseleaf_vary_r 1tunable chooseleaf_stable 1tunable straw_calc_version 1tunable allowed_bucket_algs 54# devicesdevice 0 osd.0 class hdddevice 1 osd.1 class hdddevice 2 osd.2 class hdddevice 3 osd.3 class hdddevice 4 osd.4 class hdddevice 5 osd.5 class hdddevice 6 osd.6 class hdddevice 7 osd.7 class hdddevice 8 osd.8 class hdd# typestype 0 osdtype 1 hosttype 2 chassistype 3 racktype 4 rowtype 5 pdutype 6 podtype 7 roomtype 8 datacentertype 9 zonetype 10 regiontype 11 root# bucketshost community-ceph13-1 &#123; id -3 # do not change unnecessarily id -4 class hdd # do not change unnecessarily # weight 0.144 alg straw2 hash 0 # rjenkins1 item osd.0 weight 0.048 item osd.3 weight 0.048 item osd.6 weight 0.048&#125;host community-ceph13-2 &#123; id -5 # do not change unnecessarily id -6 class hdd # do not change unnecessarily # weight 0.144 alg straw2 hash 0 # rjenkins1 item osd.1 weight 0.048 item osd.4 weight 0.048 item osd.8 weight 0.048&#125;host community-ceph13-3 &#123; id -7 # do not change unnecessarily id -8 class hdd # do not change unnecessarily # weight 0.144 alg straw2 hash 0 # rjenkins1 item osd.2 weight 0.048 item osd.5 weight 0.048 item osd.7 weight 0.048&#125;root default &#123; id -1 # do not change unnecessarily id -2 class hdd # do not change unnecessarily # weight 0.431 alg straw2 hash 0 # rjenkins1 item community-ceph13-1 weight 0.144 item community-ceph13-2 weight 0.144 item community-ceph13-3 weight 0.144&#125;# rulesrule replicated_rule &#123; id 0 type replicated min_size 1 max_size 10 step take default step chooseleaf firstn 0 type host step emit&#125;# end crush map Advanced configuration当在设置devices需要更精细的控制OSD时，非常有用。它需要已存在的volume groups和logical volumes设置（ceph-volume不会创建这些卷）。 要使用此配置，必须使用logical volumes和volume groups填充lvm_volumes选项。此外，分区的绝对路径可用于journal，block.db和block.wal。 12注意此配置使用ceph-volume lvm create来设置OSD 支持的lvm_volumes配置设置： data：logical volume名称或raw device（原始设备）的绝对路径（使用 100% 的原始设备创建 LV） data_vg：volume group名称，如果data是logical volume，则为必需。 crush_device_class：生成OSD的CRUSH device class名，允许为每个OSD设置device class，而不像全局crush_device_class那样为所有OSD设置它们。 12注意如果在使用devices时为OSD设置crush_device_class，则必须使用全局crush_device_class选项设置它，如上所示。 当使用像lvm_volumes这样的devices时，无法为每个OSD定义特定的CRUSH device class。 filestore objectstore变量： journal：logical volume名称或分区的绝对路径。 journal_vg：volume group名称，如果journal是logical volume，则是必选。 12警告每个entry（条目）必须是唯一的，不允许重复的值 bluestore objectstore变量： db： logical volume名称或分区的绝对路径。 db_vg： volume group名称，如果db是logical volume，则是必选。 wal： logical volume名称或分区的绝对路径。 wal_vg： volume group名称，如果wal是logical volume，则是必选。 12注意这些bluestore变量是可选的优化选项。 Bluestore的db和wal只会从更快的设备中受益。可以使用单个raw device（原始设备）创建bluestore OSD。 12警告每个entry（条目）必须是唯一的，不允许重复的值 使用 raw devices（原始设备）的bluestore示例： 1234osd_objectstore: bluestorelvm_volumes: - data: /dev/sda - data: /dev/sdb 12注意在这种情况下，将使用100％的devices创建volume groups和logical volumes。 具有logical volumes的bluestore示例： 123456osd_objectstore: bluestorelvm_volumes: - data: data-lv1 data_vg: data-vg1 - data: data-lv2 data_vg: data-vg2 12注意必须存在Volume groups和logical volumes。 定义wal和dblogical volumes的bluestore示例： 1234567891011121314osd_objectstore: bluestorelvm_volumes: - data: data-lv1 data_vg: data-vg1 db: db-lv1 db_vg: db-vg1 wal: wal-lv1 wal_vg: wal-vg1 - data: data-lv2 data_vg: data-vg2 db: db-lv2 db_vg: db-vg2 wal: wal-lv2 wal_vg: wal-vg2 12注意必须存在volume groups和logical volumes。 具有logical volumes的filestore示例： 12345678910osd_objectstore: filestorelvm_volumes: - data: data-lv1 data_vg: data-vg1 journal: journal-lv1 journal_vg: journal-vg1 - data: data-lv2 data_vg: data-vg2 journal: journal-lv2 journal_vg: journal-vg2 12注意必须存在volume groups和logical volumes。 Contribution有关如何为ceph-ansible做出贡献的指导，请参阅以下部分。 Contribution Guidelines Testing用于编写 ceph-ansible 功能测试方案的文档。 Testing with ceph-ansible Glossary DemosVagrant Demo在裸机上从头开始部署：https://youtu.be/E8-96NamLDo Bare metal demo在裸机上从头开始部署：https://youtu.be/dv_PEp9qAqg ansible不配置ssh免密钥,使用密码登录123456789101112131415161718192021221、编辑/etc/ansible/ansible.cfg取消注释：#host_key_checking = False2、编辑/etc/ansible/hosts[ceph_exporter]10.121.136.110 ansible_user=root ansible_ssh_pass=root10.121.136.111 ansible_user=root ansible_ssh_pass=root10.121.136.112 ansible_user=root ansible_ssh_pass=root10.121.136.113 ansible_user=root ansible_ssh_pass=root10.121.136.114 ansible_user=root ansible_ssh_pass=root10.121.136.115 ansible_user=root ansible_ssh_pass=root[node_exporter]10.121.136.110 ansible_user=root ansible_ssh_pass=root10.121.136.111 ansible_user=root ansible_ssh_pass=root10.121.136.112 ansible_user=root ansible_ssh_pass=root10.121.136.113 ansible_user=root ansible_ssh_pass=root10.121.136.114 ansible_user=root ansible_ssh_pass=root10.121.136.115 ansible_user=root ansible_ssh_pass=root3、测试ansible all -m ping 清除集群中的LVM123456789101112131415161718192021222324252627282930[root@dev yujiang]# cat remove_ceph_lvm.sh #!/bin/bashset -xDEVICE_LIST=(a b c d e f g h i j k l)DEVICE_PATH=&quot;/dev&quot;DEVICE_PREFIX=&quot;sd&quot;VG=`ls -l /dev/ | grep ceph- | awk &apos;&#123;print $9&#125;&apos; `for name in $&#123;DEVICE_LIST[@]&#125;do echo $&#123;name&#125;donefor name in $&#123;VG[@]&#125;do LV=`ls -l $&#123;DEVICE_PATH&#125;/$&#123;name&#125; | grep osd- | awk &apos;&#123;print $9&#125;&apos;` for lv in $&#123;LV&#125; do LV_PATH=$&#123;DEVICE_PATH&#125;/$&#123;name&#125;/$&#123;lv&#125; lvremove $&#123;LV_PATH&#125; done vgremove $&#123;DEVICE_PATH&#125;/$&#123;name&#125;donefor name in $&#123;DEVICE_LIST[@]&#125;do pvremove $&#123;DEVICE_PATH&#125;/$&#123;DEVICE_PREFIX&#125;$&#123;name&#125;done]]></content>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible]]></title>
    <url>%2F2018%2F07%2F24%2FAnsible%2F</url>
    <content type="text"><![CDATA[环境准备123PythonSetuptoolspip(可选) Ansible快速安装12安装pip：easy_install pip安装Ansible：pip install ansible Ansible源码安装1234获取源码解压源码进入源码目录运行source ./hacking/env-setup Ansible系统源安装1234567Centos - yum install ansibleUbuntu - apt-get install software-properties-common - apt-add-repository ppa:ansible/ansible - apt-get update - apt-get install ansible Ansible运行12 client serverAnsible --&gt; ssh =====&gt; ssh --&gt; shell Ansible配置文件路径搜索优先级1234export ANSIBLE_CONFIG./ansible.cfg~/.ansible.cfg/etc/ansible/ansible.cfg Ansible配置文件获取12/etc/ansible/目录下https://raw.github.com/ansible/ansible/devel/examples/ansible.cfg Ansible配置详细123456defaults默认配置项privilege_escalation执行命令的用户权限设置(ssh到客户机以哪个用户执行命令，关于这个用户的权限设置)paramiko_connection paramika插件设置(python插件实现的ssh协议，如果不设置会走Ansible自己的ssh协议通道)ssh_connection ssh连接设置accelerateselinux &amp; colors 123ask_pass &amp; ask_sudo_passask_pass：可以控制，Ansible剧本playbook是否会自动默认弹出密码输入框(遇到输入密码的情况下)ask_sudo_pass：用户使用的系统平台开启了sudo密码的话，应该开启这一参数(如果目标机开启了sudo免密码的话，应该开启该项，否则报错) 12gather_subsetgather_subset：设置手机的内容，包括all，network，hardware，virtual，facter，ohai 1234remote_port &amp; remote_tmp &amp; remote_userremote_port：设置远程客户机ssh监听的端口remote_tmp：设置远程客户机临时目录remote_user：设置远程客户机执行命令的用户(以哪个用户执行shell) 1234sudo_exe &amp; sudo_flags &amp; sudo_usersudo_exe：sudo命令路径(默认/usr/bin)sudo_flags：sudo参数sudo_user：能够使用sudo的user 12345678action_plugins &amp; callback_plugins &amp; connection_plugins &amp; filter_plugins &amp; lookup_plugins &amp; vars_plugins开发者中心的插件相关功能，开发者可以开发相应的插件，来完成自己的功能。action_plugins：激活事件(在某个事件发生的时候，去执行插件中的内容)callback_plugins：回调connection_plugins：连接filter_plugins：过滤器lookup_plugins：加载路径vars_plugins：任何地方加载 12forksforks：最大开辟的进程数，这个数不宜过大，过大性能损耗高，不易过小，过小并发性能低，一般的设置方法是cpu核数*2 12module_namemodule_name：这个是/usr/bin/ansible的默认模块名(-m)。默认是&apos;command&apos;模块。之前提到过，command模块不支持shell变量，管道，配额。所以也许你希望把这个参数改为&apos;shell&apos;。 1234vault_password_filevault_password_file：这个文件也可以是脚本的形式。如果你使用脚本而不是单纯文件的话，请确保它可以执行并且密码可以在标准输出上打印出来。如果你的脚本需要提示请求数据，请求将会发到标准错误输出中。存放远程客户机密码的文件，因为有时候sudo是需要输入密码的，而我们希望Ansible执行比较流畅(不需要人工干预)，这个时候可以通过机器代填密码的方式。这个文件可以是个脚本，也可以是单独的一条密码。如果是脚本，确保该脚本是可运行的，并且运行的，结果输出到标准输出中stdout。 12patternpattern：如果没有提供&quot;hosts&quot;节点，这是playbook要通信的默认主机组。默认值是对所有主机通信，如果不想被惊吓到，最好还是设置这个选项 123inventory &amp; libraryinventory：可通信主机的目录(默认/etc/ansible/hosts)library：Ansible默认搜索模块路径(默认/usr/share/my_modules/) 新手上路如何添加一台机器？ 12341、编辑/etc/ansible/hosts2、添加本机的public SSH key到目标机器的authorized_keys3、添加本机的私钥到Ansible4、运行ansible all -m ping测试是否添加成功 Ansible命令格式——ansible all -m ping 12341、ansible命令主体 —— ansible/ansible-playbook2、被操作的目标机器的正则表达式 —— all3、指定要使用的模块 —— -m ping4、传入的参数（ ansible all -a &apos;ls&apos; ） 执行ping模块发生了什么？ 1唤起Ansible --&gt; 查询hosts，获取指定的目标机器 --&gt; 调用指定模块 --&gt; 传入参数 --&gt; 返回执行结果 Ansible命令详解 1234567-a 指定传入模块的参数-C -D 两个一起使用，检查hosts规则文件的修改-l 限制匹配规则的主机数--list-hosts 显示所有匹配规则的主机数-m -M 指定所使用的模块和模块的路径--syntax-check 检查语法-v 显示详细的日志 12ansible all -a &apos;ls&apos;ansible all -a &apos;ls&apos; --list-hosts Ansible——Inventory（分组） 什么是Inventory？ 123456789101112131415161718192021222324252627282930313233343536373839404142Ansible可同时操作属于一个组的多台主机，组和主机之间的关系通过inventory文件配置。默认的文件路径为/etc/ansible/hosts中文：库存、清单作用：分组一个简单的实例one.example.com[webservers]two.example.comthree.example.com方括号[]中是组名，一个主机可以属于不同的组，比如一台服务器可以同时属于webserver组和vim组。这时，指定webserver或者vim组，都可以操作这台主机。ansible webservers -a &apos;ls&apos;ansible &lt;IP&gt; -a &apos;ls&apos;如何自定义连接端口？在编写hosts文件时，在域名或者ip后加上端口号，例如：192.168.100.101:5000one.example.com:6000IP域名写起来太长？起个别名吧jumper ansible_ssh_port=5555 ansible_ssh_host=192.168.1.50不想以root用户登录例如：jumper ansible_ssh_port=5555 ansible_ssh_host=192.168.1.50 ansible_ssh_user=papa机器太多，但是连续的，写起来好麻烦[vim]vim[1:50].example.comvim[a-f].example.comInventory参数选项ansible_ssh_hostansible_ssh_portansible_ssh_useransible_ssh_pass ssh密码(非公钥免密方式)ansible_sudo_pass sudo密码ansible_sudo_exeansible_connection 与主机的连接类型。比如：local，ssh或者paramiko。ansible_ssh_private_key_file 私钥文件ansible_shell_type 目标系统的shell类型ansible_python_interpreter 分文件管理 123456789分文件 —— 例如：/etc/ansible/group_vars/vim (一组)/etc/ansible/group_vars/webservers (一组)/etc/ansible/host_vars/foosball (不分组)vim文件中的内容大致这样： --- ntp_server:acme.example.org database_server:storage.example.org这时候文件名就是组名 分文件夹管理 123456789分文件 —— 例如：/etc/ansible/group_vars/vim/install/etc/ansible/group_vars/vim/init/etc/ansible/host_vars/foosballinstall文件中的内容大致这样： --- ntp_server:acme.example.org database_server:storage.example.org这时候文件夹名就是组名 Ansible——Patterns 1234567891011121314151617181920212223242526什么是Patterns？ 在Ansible中，Patterns是指我们通过类正则表达式的方式，决定与哪台主机进行交互 1、所有hosts机器：all或者星号(*) 例如： ansible all -m ping2、通过域名或者IP指定一台特定的机器 例如： ansible one.example.com -m ping ansible 192.168.1.2 -m ping3、指定一组连续机器 例如： ansible 192.168.1.* -m ping 指定192.168.1.0/24段的所有机器4、指定一组不相关的机器 例如： ansible one.example.com:two.example.com -m ping 指定one two两台机器5、组也支持Patterns webservers -- 指定webserver组 webservers:nginx -- 指定webservers和nginx两个组6、与和非 webservers:!phoenix -- 指定在webserver组但不在phoenix组 webservers:&amp;nginx -- 指定同时在webservers和nginx组问题：webservers:nginx:&amp;vim:!python:!mysql如上的语句是什么意思？温馨提示：匹配时从左到右依次匹配！ Ansible——Ad-Hoc 1234567891011121314151617181920212223242526什么是Ad-Hoc 这其实是一个概念性的名字，是相对于写Ansible playbook来说的。类似于在命令行敲入shell命令和写shell scripts两者之间的关系 如果我们敲入一些命令去比较快的完成一些事情，而不需要将这些执行的命令保存下来，这样的命令就叫做ad-hoc命令。执行shell命令： ansible atlanta -a &quot;/sbin/reboot&quot; -f 10 这个命令中，atlanta是一个组，这个组里面有很多服务器，&quot;/sbin/reboot&quot;命令会在atlanta组下的所有机器上执行。这里ssh-agent会fork出10个子进程(bash)以并行的方式执行reboot命令。如前所说&quot;每次重启10个&quot;即是以这种方式实现。 指定用户执行shell命令： ansible atlanta -a &quot;/usr/bin/foo&quot; -u username使用sudo： ansible atlanta -a &quot;/usr/bin/foo&quot; -u username --sudo [--ask-sudo-pass]文件管理： 创建文件夹：ansible webservers -m file -a &quot;dest=/path/to/c mode=755 owner=mdehaan group=mdehaan state=directory&quot; 拷贝文件：ansible atlanta -m copy -a &quot;src=/etc/hosts dest=/tmp/hosts&quot;软件包管理： 确认一个软件包已经安装，但不去升级它：ansible webservers -m yum -a &quot;name=acme state=present&quot; 确认一个软件包的安装版本：ansible webservers -m yum -a &quot;name=acme-1.5 state=present&quot; 确认一个软件包还没有安装：ansible webservers -m yum -a &quot;name=acme state=absent&quot;其他模块 git模块：git相关 service模块：系统服务相关 setup模块：系统环境相关 如何执行一个耗时的任务 1234567ansible all -B 3600 -P 0 -a &quot;/usr/bin/long_running_operation --do-stuff&quot;-B 1800 表示最多运行60分钟，-P 0表示不获取状态ansible all -B 1800 -P 60 -a &quot;/usr/bin/long_running_operation --do-stuff&quot;-B 1800表示最多运行30分钟，-P 60表示每隔一分钟获取一次状态ansible web1.example.com -m async_status -a &quot;jid=488359678239.2844&quot; Ansible——Playbook简介什么是playbook？ 123简单来说，playbook是一种简单的配置管理系统与多机器部署系统的基础。与现有的其他系统有不同之处，且非常适合于复杂应用的部署。playbook可用于声明配置，更强大的地方在于，在playbooks中可以编排有序的执行过程，甚至于做到在多组机器间，来回有序的执行特别指定的步骤。并且可以同步或异步的发起任务。 Ansible APIAPI提供了哪些功能？ 1231、调用Ansible的模块2、开发动态的Inventory数据源3、更好的控制playbook等功能的运行 如何调用Ansible模块？ 1234567891011121314151、引入Ansible runner库2、初始化runner对象，传入相关参数3、运行runner对象的run函数示例代码（Ansible2.0之前）：import ansible.runnerrunner = ansible.runner.Runner( module_name=&apos;ping&apos;, module_args=&apos;&apos;, pattern=&apos;web*&apos;, forks=10)datastructrue = runner.run() Directory Layout 目录布局12345678910111213141516171819202122232425262728293031323334353637383940414243production # inventory file for production servers（生产服务器的inventory文件）staging # inventory file for staging environment（临时环境的inventory文件）group_vars/ group1.yml # here we assign variables to particular groups（将变量分配给特定的groups） group2.ymlhost_vars/ hostname1.yml # here we assign variables to particular systems（将变量分配给特定systems） hostname2.ymllibrary/ # if any custom modules, put them here (可选)（放置自定义modules）module_utils/ # if any custom module_utils to support modules, put them here (可选)（放置支持modules的自定义module_utils）filter_plugins/ # if any custom filter plugins, put them here (可选)（放置自定义filter plugins）site.yml # master playbook（playbook入口）webservers.yml # playbook for webserver tier（webserver层playbook）dbservers.yml # playbook for dbserver tier（dbserver层playbook）roles/ common/ # this hierarchy represents a &quot;role&quot;（这个层次代表role） tasks/ # main.yml # &lt;-- tasks file can include smaller files if warranted（tasks文件） handlers/ # main.yml # &lt;-- handlers file（处理程序文件） templates/ # &lt;-- files for use with the template resource（用于模板资源的文件） ntp.conf.j2 # &lt;------- templates end in .j2（模板以.j2结尾） files/ # bar.txt # &lt;-- files for use with the copy resource（用于复制资源的文件） foo.sh # &lt;-- script files for use with the script resource（用于脚本资源的脚本文件） vars/ # main.yml # &lt;-- variables associated with this role（与此role关联的变量） defaults/ # main.yml # &lt;-- default lower priority variables for this role（此role的默认优先级较低的变量） meta/ # main.yml # &lt;-- role dependencies（role依赖） library/ # roles can also include custom modules（role还可以包括自定义modules） module_utils/ # roles can also include custom module_utils（role还可以包括自定义module_utils） lookup_plugins/ # or other types of plugins, like lookup in this case（或其他类型的plugins，例如lookup） webtier/ # same kind of structure as &quot;common&quot; was above, done for the webtier role（与上面的&quot;common&quot;相同的结构，为webtier role定义） monitoring/ # &quot;&quot; fooapp/ # &quot;&quot; 常用操作查看环境变量 1ansible-config dump 课程地址https://www.imooc.com/video/15236]]></content>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golangpkg被墙解决办法]]></title>
    <url>%2F2018%2F07%2F08%2Fgolangpkg%E8%A2%AB%E5%A2%99%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[原文：http://www.10tiao.com/html/489/201702/2247483836/1.html 1234567$mkdir -p $GOPATH/src/golang.org/x/$cd $GOPATH/src/golang.org/x/$git clone https://github.com/golang/net.git net $go install net 或者$git clone https://github.com/golang/text.git $GOPATH/src/golang.org/x/text$git clone https://github.com/golang/net.git $GOPATH/src/golang.org/x/net go get 获取第三方库 go get命令演示 使用gopm来获取无法下载的包 1234567891011121314151617181920yujiangdeMBP-13:~ yujiang$ go get -v github.com/gpmgo/gopmyujiangdeMBP-13:~ yujiang$ cat ~/.bashrcexport PATH=$PATH:/home/go/bin:/root/go/binexport GOPATH=&quot;/root/go&quot;yujiangdeMBP-13:~ yujiang$ source ~/.bashrcyujiangdeMBP-13:~ yujiang$ gopm get -g -v golang.org/x/tools/cmd/goimportsyujiangdeMBP-13:~ yujiang$ ls go/src/golang.org/x/tools/imports/fix.go fix_test.go imports.go mkindex.go mkstdlib.go sortimports.go zstdlib.go# 编译出goimportsyujiangdeMBP-13:~ yujiang$ go build golang.org/x/tools/cmd/goimports# 编译并安装到$GOPATH/bin/目录下yujiangdeMBP-13:~ yujiang$ go install golang.org/x/tools/cmd/goimportsgo get -v github.com/golang/tools/go/...go get -v github.com/golang/tools/...go get -u golang.org/x/tools/... go build编译 go install产生pkg文件和可执行文件 go run直接编译运行]]></content>
  </entry>
  <entry>
    <title><![CDATA[jenkins安装]]></title>
    <url>%2F2018%2F06%2F18%2Fjenkins%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[centos install安装文档 12https://pkg.jenkins.io/redhat-stable/https://www.vultr.com/docs/how-to-install-jenkins-on-centos-7 启动jenkins 12systemctl start jenkins.servicesystemctl enable jenkins.service 中文插件 1Locale plugin 多任务插件 1Multijob plugin]]></content>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Repo for Microsoft Windows and Linux]]></title>
    <url>%2F2018%2F06%2F15%2FRepo-for-Microsoft-Windows-and-Linux%2F</url>
    <content type="text"><![CDATA[这是E.S.R.Labs Repo —— 就像Google Repo，但它也可以在Microsoft Windows运行。Repo是Google建立在Git之上的repository管理工具。必要时，Repo会统一许多Git repositories上传到版本控制系统，并使部分开发工作流程自动化。Repo并不意味着取代Git，只是为了更容易在多个repositories的环境中使用Git。repo command是一个可执行的Python脚本，您可以将其放在任何路径。在处理源文件时，您将使用Repo进行跨网络操作。例如，使用Repo命令，可以将多个repositories中的文件下载到本地工作目录中。 Setup steps for Microsoft Windows修复权限以允许创建符号链接 如果您是Administrators group的member，则必须关闭User Access Control (UAC)，然后重新启动计算机。 否则，您必须调整您的用户权限才能获得SeCreateSymbolicLinkPrivilege权限。editrights工具是作为Microsoft Windows的git-repo的一部分提供的。 Highly experimental（除开发此功能外，请勿使用！）：如果您不想使用符号链接，而是文件夹和文件的硬链接（hardlinks），则必须在~/.gitconfig中设置以下内容: 12[portable] windowsNoSymlinks = true 这不需要上述所说的设置权限。 已知问题：硬链接被git销毁，例如，当你删除一个branch时（breaks .git/config）。这会破坏project workspace！ Download and install Git下载Git（http://git-scm.com/downloads） 将Git添加到path环境变量中：例如C:\Program Files (x86)\Git\cmd; 将MinGW添加到path环境变量中：例如C:\Program Files (x86)\Git\bin; Download and install Python下载Python 2.7+（https://www.python.org/downloads/） 将Python添加到path环境变量中：例如C:\Python27; 注意：git-repo在实验环境支持Python 3.x 使用Windows Command Shell或Git Bash下载并安装RepoWindows Command Shell 123md %USERPROFILE%\bincurl https://raw.githubusercontent.com/esrlabs/git-repo/stable/repo &gt; %USERPROFILE%/bin/repocurl https://raw.githubusercontent.com/esrlabs/git-repo/stable/repo.cmd &gt; %USERPROFILE%/bin/repo.cmd Windows PowerShell 123md $env:USERPROFILE\bincurl https://raw.githubusercontent.com/esrlabs/git-repo/stable/repo &gt; $env:USERPROFILE/bin/repocurl https://raw.githubusercontent.com/esrlabs/git-repo/stable/repo.cmd &gt; $env:USERPROFILE/bin/repo.cmd Git Bash 123mkdir ~/bincurl https://raw.githubusercontent.com/esrlabs/git-repo/stable/repo &gt; ~/bin/repocurl https://raw.githubusercontent.com/esrlabs/git-repo/stable/repo.cmd &gt; ~/bin/repo.cmd 将Repo添加path环境变量：%USERPROFILE%\bin; 创建一个指向％USERPROFILE％的HOME环境变量（OpenSSH需要找到它的.ssh目录）。 创建一个GIT_EDITOR环境变量，其中包含一个editor可执行文件作为值。为此，首先将editor可执行文件的主目录添加到path环境变量中。例如，GIT_EDITOR可以设置为”notepad++.exe”，”gvim.exe”。 Setup steps for Linux下载并安装Git和Python 12sudo apt-get install git-coresudo apt-get install python 下载并安装Repo 1234$ mkdir ~/bin$ PATH=~/bin:$PATH$ curl https://raw.githubusercontent.com/esrlabs/git-repo/stable/repo &gt; ~/bin/repo$ chmod a+x ~/bin/repo 用法有关git-repo用法的更多详细说明，请访问git-repo。 常见问题解答 我无法看到repo状态的任何颜色 用于repo状态的pager默认设置为less，除非另外配置，否则将忽略转义序列。您可以将环境变量LESS设置为-FRSX以减少处理颜色。 （您可能还想将LESSCHARSET设置为utf-8） 它仍然不起作用！ 对pager有更多的选择。它的行为并不像less一样，但它对你来说可能已经足够了。 1git config --global core.pager more 或者，环境变量GIT_PAGER可以设置为more。 在使用repo init初始化我的repo时，我在Windows Command Shell中遇到WindowsError 如果在输出开始时出现警告，表示GPG不可用 12warning: gpg (GnuPG) is not available.warning: Installing it is strongly encouraged. 你必须添加gpg.exe到你的PATH变量。可执行文件可以在Git安装文件夹$GIT\usr\bin中找到。当你使用Git Bash时，$Git\usr\bin文件夹已经被添加到你的PATH中。 Developer Information与official google repo同步 为了与official google repo git同步，这里是与official google repo的tag v1.12.33同步的命令： 123456789101112131415161718# add google git-repo remote with taggit remote add googlesource https://android.googlesource.com/tools/repo/git checkout v1.12.33 -b google-latest# checkout basis for resyncgit checkout google-git-repo-base -b updategit merge --allow-unrelated-histories -Xtheirs --squash google-latestgit commit -m &quot;Update: google git-repo v1.12.33&quot;git rebase stable# solve conflicts; keep portability in mindgit checkout stablegit rebase update# cleanupgit branch -D updategit branch -D google-latest 创建一个新的签名版本用于创建新版本repo的命令： 123git tag -s -u KEYID v0.4.16 -m &quot;COMMENT&quot;git push origin stable:stablegit push origin v0.4.16 替换KEYID（类似0x ..） v0.4.16（两次） 用更多的解释替换COMMENT]]></content>
      <tags>
        <tag>git-repo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LTP C Test Case教程]]></title>
    <url>%2F2018%2F06%2F11%2FLTP-C-Test-Case%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[LTP测试可以用C或Shell脚本编写。 Getting StartedGit-clone主LTP repository，如自述文件中所述, 并更改目录到checked-out的Git repository。我们建议安装LTP并运行Quick guide（在自述文件中）中提到的测试之一，以确保您从良好的状态开始。我们还建议克隆Linux kernel repository以供参考，本指南将涉及主线kernel 4.12内的文件和目录。 1$ git clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git 还有许多其他的repositories可以参考，包括GNU C library glibc和可选的C library musl。一些system calls被部分或完全实现在user land作为标准C library的一部分。所以在这些情况下，C library是一个重要的参考。glibc是Linux最常用的C library，然而musl通常更容易理解。 实现system calls的方式因体系结构，以及内核版本和C library版本而异。要找出system calls是否真的在任何机器上访问内核，可以使用strace。这会拦截system calls并打印它们。我们将在后面的教程中使用它。 Choose a System Call to test我们将使用statx() system call来提供测试的具体示例。在撰写本文时，没有测试Linux内核版本4.11中的system call。Linux system call特定的测试主要包含在testcases/kernel/syscalls目录，但你也可以grep整个LTP repository以检查任何现有system call的用法。 找到当前未被LTP测试的system call的方法是在内核树中查看include/linux/syscalls.h。 Find an untested System call尝试找到一个未经测试的系统调用，它有一个手册页（即man syscall产生一个结果）。Git-clone最新的kernel man-pages repository是一个好主意。 1$ git clone git://git.kernel.org/pub/scm/docs/man-pages/man-pages.git 在写这篇文章时，最新的man-pages release和repository HEAD之间的差异超过了100次commits。这代表了大约9周的变化。如果你使用的是稳定的Linux发行版，那么你的man-pages package可能已经有几年了。就像kernel一样，最好有Git repository作为参考。 Create the test skeleton我将调用我的测试statx01.c，在你读这个文件名的时候，可能会采用这个文件名，所以可以根据需要增加文件名中的数字，或者用练习2.1中选择的system call替换statx。（怕被占用） 123$ mkdir testcases/kernel/syscalls/statx$ cd testcases/kernel/syscalls/statx$ echo statx &gt;&gt; .gitignore 接下来打开statx01.c并添加如下。确保将right notice更改为您的name/company，如有必要，更正test name和最低kernel version。我将在下面解释代码的作用。 12345678910111213141516171819202122232425262728293031323334353637/* * Copyright (c) 2017 Instruction Ignorer &lt;&quot;can&apos;t&quot;@be.bothered.com&gt; * * This program is free software: you can redistribute it and/or modify * it under the terms of the GNU General Public License as published by * the Free Software Foundation, either version 2 of the License, or * (at your option) any later version. * * This program is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the * GNU General Public License for more details. * * You should have received a copy of the GNU General Public License * along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;. *//* * Test statx * * All tests should start with a description of _what_ we are testing. * Non-trivial explanations of _how_ the code works should also go here. * Include relevant links, Git commit hashes and CVE numbers. * Inline comments should be avoided. */#include &quot;tst_test.h&quot;static void run(void)&#123; tst_res(TPASS, &quot;Doing hardly anything is easy&quot;);&#125;static struct tst_test test = &#123; .test_all = run, .min_kver = &quot;4.11&quot;,&#125;; 从#include语句开始，我们将复制main LTP test library headers。这包括最常见的test API函数和test harness初始化代码。需要注意的是，这是一个完全普通的，独立的C程序，但缺少main()，因为它在tst_test.h中实现。 我们使用tst_test结构指定运行一部分测试代码。测试编写器可以设置各种回调，包括我们已经设置为运行（）的test.test_all。test writer可以设置各种回调, 包括test.test_all，我们已经设置run()。测试工具将在一个单独的进程（使用fork()）中执行此回调，如果在test.timeout秒后没有返回，则强制终止它。 原文【1】https://github.com/linux-test-project/ltp/wiki/C-Test-Case-Tutorial]]></content>
      <tags>
        <tag>LTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用配置命令]]></title>
    <url>%2F2018%2F06%2F08%2FLinux%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[系统环境12345LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.4.1708 (Core) Release: 7.4.1708Codename: Core ip12345678910111213[root@cephL ~]# ip -hUsage: ip [ OPTIONS ] OBJECT &#123; COMMAND | help &#125; ip [ -force ] -batch filenamewhere OBJECT := &#123; link | address | addrlabel | route | rule | neigh | ntable | tunnel | tuntap | maddress | mroute | mrule | monitor | xfrm | netns | l2tp | macsec | tcp_metrics | token &#125; OPTIONS := &#123; -V[ersion] | -s[tatistics] | -d[etails] | -r[esolve] | -h[uman-readable] | -iec | -f[amily] &#123; inet | inet6 | ipx | dnet | bridge | link &#125; | -4 | -6 | -I | -D | -B | -0 | -l[oops] &#123; maximum-addr-flush-attempts &#125; | -o[neline] | -t[imestamp] | -ts[hort] | -b[atch] [filename] | -rc[vbuf] [size] | -n[etns] name | -a[ll] &#125; 获取系统上所有网络接口的信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@cephL ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 08:00:27:3a:98:1d brd ff:ff:ff:ff:ff:ff3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 08:00:27:52:03:aa brd ff:ff:ff:ff:ff:ff inet 192.168.56.102/24 brd 192.168.56.255 scope global dynamic enp0s8 valid_lft 945sec preferred_lft 945sec inet6 fe80::4b2b:b766:22c:f503/64 scope link valid_lft forever preferred_lft forever4: enp0s9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 08:00:27:3e:3e:e2 brd ff:ff:ff:ff:ff:ff5: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN qlen 1000 link/ether 52:54:00:66:bf:e6 brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever6: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN qlen 1000 link/ether 52:54:00:66:bf:e6 brd ff:ff:ff:ff:ff:ffscope解释，ip地址和路由都有一个域的概念，主要是告诉内核他们起作用的上下文。在linux中，路由的域表明了目的网络和本机的距离。而ip地址的域说明了这个ip地址的作用域。在linux中一共有3种作用域，分别是HOST，LINK(对于ip地址来说说明这个地址是只在lan中使用，而对于route来说目的的地址在lan中),Universe/global(广域网的地址,对于route来说要到达这个地址肯定会多于1跳)。要注意，域的默认值是Universe，也就是说你不显示指明，则默认都是Universe。查看IP地址[root@cephL ~]# ip addr show enp0s83: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 08:00:27:52:03:aa brd ff:ff:ff:ff:ff:ff inet 192.168.56.102/24 brd 192.168.56.255 scope global dynamic enp0s8 valid_lft 847sec preferred_lft 847sec inet6 fe80::4b2b:b766:22c:f503/64 scope link valid_lft forever preferred_lft forever查看IPv6地址[root@cephL ~]# ip -6 addr show dev enp0s83: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 state UP qlen 1000 inet6 fe80::4b2b:b766:22c:f503/64 scope link valid_lft forever preferred_lft forever添加IP地址[root@cephL ~]# ip addr add 192.168.56.103/24 dev enp0s8删除IP地址[root@cephL ~]# ip addr del 192.168.56.103/24 dev enp0s8 路由 1234567891011121314[root@cephL ~]# ip route show192.168.56.0/24 dev enp0s8 proto kernel scope link src 192.168.56.102 metric 100 192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1让kernel报告路由，它会发送一个数据包到指定的地址[root@cephL ~]# ip route get 61.135.169.12561.135.169.125 via 10.116.21.254 dev enp0s9 src 10.116.21.135 cache（解释：61.135.169.125是访问的地址，10.116.21.254是默认路由，10.116.21.135是enp0s9设备IP地址）添加默认路由[root@cephL ~]# ip route add default via 10.116.21.253 dev enp0s9删除默认路由[root@cephL ~]# ip route del default via 10.116.21.253 dev enp0s9 不同网络接口的统计信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546[root@cephL ~]# ip -s link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 RX: bytes packets errors dropped overrun mcast 189813592 61558 0 0 0 0 TX: bytes packets errors dropped carrier collsns 189813592 61558 0 0 0 0 2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000 link/ether 08:00:27:3a:98:1d brd ff:ff:ff:ff:ff:ff RX: bytes packets errors dropped overrun mcast 0 0 0 0 0 0 TX: bytes packets errors dropped carrier collsns 0 0 0 0 0 0 3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000 link/ether 08:00:27:52:03:aa brd ff:ff:ff:ff:ff:ff RX: bytes packets errors dropped overrun mcast 104188 1104 0 0 0 0 TX: bytes packets errors dropped carrier collsns 159594 673 0 0 0 0 4: enp0s9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000 link/ether 08:00:27:3e:3e:e2 brd ff:ff:ff:ff:ff:ff RX: bytes packets errors dropped overrun mcast 53777 574 0 0 0 0 TX: bytes packets errors dropped carrier collsns 60106 623 0 0 0 0 5: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT qlen 1000 link/ether 52:54:00:66:bf:e6 brd ff:ff:ff:ff:ff:ff RX: bytes packets errors dropped overrun mcast 0 0 0 0 0 0 TX: bytes packets errors dropped carrier collsns 0 0 0 0 0 0 6: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN mode DEFAULT qlen 1000 link/ether 52:54:00:66:bf:e6 brd ff:ff:ff:ff:ff:ff RX: bytes packets errors dropped overrun mcast 0 0 0 0 0 0 TX: bytes packets errors dropped carrier collsns 0 0 0 0 0 0 查询特定接口统计信息[root@cephL ~]# ip -s link ls enp0s94: enp0s9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000 link/ether 08:00:27:3e:3e:e2 brd ff:ff:ff:ff:ff:ff RX: bytes packets errors dropped overrun mcast 54527 583 0 0 0 0 TX: bytes packets errors dropped carrier collsns 60946 633 0 0 0 0 ARP，查看接入的MAC地址 123456[root@cephL ~]# ip neighbour192.168.56.1 dev enp0s8 lladdr 0a:00:27:00:00:00 DELAY10.116.21.254 dev enp0s9 lladdr 00:00:0c:07:ac:0c DELAY10.116.21.253 dev enp0s9 lladdr b4:de:31:c7:e7:47 STALE192.168.56.100 dev enp0s8 lladdr 08:00:27:3e:18:34 STALE10.116.21.252 dev enp0s9 lladdr b4:de:31:c7:d8:e7 STALE ip工具可以连续监视devices, addresses和routes的状态。 1234567[root@cephL ~]# ip monitor all[nsid current]192.168.56.1 dev enp0s8 lladdr 0a:00:27:00:00:00 REACHABLE[nsid current]10.116.21.254 dev enp0s9 lladdr 00:00:0c:07:ac:0c STALE[nsid current]192.168.56.1 dev enp0s8 lladdr 0a:00:27:00:00:00 REACHABLE[nsid current]10.116.21.254 dev enp0s9 lladdr 00:00:0c:07:ac:0c REACHABLE[nsid current]3: enp0s8 inet 192.168.56.102/24 brd 192.168.56.255 scope global dynamic enp0s8 valid_lft 1200sec preferred_lft 1200sec up和down网络设备 12345678910111213141516[root@cephL ~]# ip link set enp0s9 down [root@cephL ~]# ip route show192.168.56.0/24 dev enp0s8 proto kernel scope link src 192.168.56.102 metric 100 192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1[root@cephL ~]# ping www.baidu.comping: www.baidu.com: 未知的名称或服务[root@cephL ~]# ip link set enp0s9 up[root@cephL ~]# ip route showdefault via 10.116.21.254 dev enp0s9 proto static metric 100 10.116.20.0/23 dev enp0s9 proto kernel scope link src 10.116.21.135 metric 100 192.168.56.0/24 dev enp0s8 proto kernel scope link src 192.168.56.102 metric 100 192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1[root@cephL ~]# ping www.baidu.comPING www.a.shifen.com (61.135.169.121) 56(84) bytes of data.64 bytes from 61.135.169.121 (61.135.169.121): icmp_seq=1 ttl=50 time=4.35 ms 获取帮助信息 12345678910111213141516171819202122232425262728293031323334[root@cephL ~]# ip route helpUsage: ip route &#123; list | flush &#125; SELECTOR ip route save SELECTOR ip route restore ip route showdump ip route get ADDRESS [ from ADDRESS iif STRING ] [ oif STRING ] [ tos TOS ] [ mark NUMBER ] ip route &#123; add | del | change | append | replace &#125; ROUTESELECTOR := [ root PREFIX ] [ match PREFIX ] [ exact PREFIX ] [ table TABLE_ID ] [ proto RTPROTO ] [ type TYPE ] [ scope SCOPE ]ROUTE := NODE_SPEC [ INFO_SPEC ]NODE_SPEC := [ TYPE ] PREFIX [ tos TOS ] [ table TABLE_ID ] [ proto RTPROTO ] [ scope SCOPE ] [ metric METRIC ]INFO_SPEC := NH OPTIONS FLAGS [ nexthop NH ]...NH := [ via ADDRESS ] [ dev STRING ] [ weight NUMBER ] NHFLAGSOPTIONS := FLAGS [ mtu NUMBER ] [ advmss NUMBER ] [ rtt TIME ] [ rttvar TIME ] [reordering NUMBER ] [ window NUMBER ] [ cwnd NUMBER ] [ initcwnd NUMBER ] [ ssthresh NUMBER ] [ realms REALM ] [ src ADDRESS ] [ rto_min TIME ] [ hoplimit NUMBER ] [ initrwnd NUMBER ] [ features FEATURES ] [ quickack BOOL ] [ congctl NAME ] [ expires TIME ]TYPE := &#123; unicast | local | broadcast | multicast | throw | unreachable | prohibit | blackhole | nat &#125;TABLE_ID := [ local | main | default | all | NUMBER ]SCOPE := [ host | link | global | NUMBER ]NHFLAGS := [ onlink | pervasive ]RTPROTO := [ kernel | boot | static | NUMBER ]TIME := NUMBER[s|ms]BOOL := [1|0]FEATURES := ecn yum常用列表 1yum provides 找到缺少的RPM依赖包，例如yum provides &apos;*/dirent.h&apos; 查看ssh登录信息 1utmpdump /var/log/wtmp | cat 参考链接【1】https://linux.cn/article-3144-1.html 【2】https://www.jianshu.com/p/d89cc7ecb305 【3】http://www.pagefault.info/?p=240]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[近距离探索memcache缓存]]></title>
    <url>%2F2018%2F05%2F31%2F%E8%BF%91%E8%B7%9D%E7%A6%BB%E6%8E%A2%E7%B4%A2memcache%2F</url>
    <content type="text"><![CDATA[近距离探索memcache缓存笔记 课程地址：https://www.imooc.com/learn/527 课程教师：李虎头 memcache解析1-1内容解析 1231.知道什么是Memcache2.知道在什么时候使用Memcache3.知道怎样使用Memcache优化网站或API 1-2基础介绍 123456Memcache介绍1.什么是Memcache？ 分布式高速缓存系统2.Memcache有什么用？ 缓存层，前端频繁操作后端数据库时，减轻后端数据库压力，前端直接访问缓存层 1-3理解Memcache 1只有一张表的数据库 # 名字 类型 整理 1 key varchar(255) utf8_general_ci 2 value text utf8_general_ci 1-4使用场景 1231.非持久化存储：对数据存储要求不高2.分布式存储：不适合单机使用3.Key/Value存储：格式简单，不支持List、Array数据格式 memcache的安装2-1安装前的说明 1231.编译安装 Libevent Memcache(需要先安装Libevent依赖)2.使用依赖管理工具yum、apt-get-PS：Memcache和Memcached的区别(建议使用Memcached，它是Memcache的升级版) 2-2安装服务端实际操作 123456789101112131415161718192021222324252627282930313233343536373839401.编译安装yum -y install automake libtool wget gcc gcc-c++ &amp;&amp; wget https://github.com/libevent/libevent/archive/release-2.1.8-stable.tar.gz &amp;&amp; tar zxvf release-2.1.8-stable.tar.gz &amp;&amp; cd libevent-release-2.1.8-stable/ &amp;&amp; ./autogen.sh &amp;&amp; ./configure &amp;&amp; make -j 2 &amp;&amp; make installwget http://www.memcached.org/files/memcached-1.5.8.tar.gz &amp;&amp; cd memcached-1.5.8 &amp;&amp; ./configure &amp;&amp; make -j 2 &amp;&amp; make install[root@cephL memcached-1.5.8]# /usr/local/bin/memcached -h/usr/local/bin/memcached: error while loading shared libraries: libevent-2.1.so.6: cannot open shared object file: No such file or directory[root@cephL memcached-1.5.8]# ldd /usr/local/bin/memcached linux-vdso.so.1 =&gt; (0x00007ffdd7d77000) libevent-2.1.so.6 =&gt; not found libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007f2d78950000) libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f2d78583000) /lib64/ld-linux-x86-64.so.2 (0x000055b9e254c000)[root@cephL memcached-1.5.8]# LD_DEBUG=libs /usr/local/bin/memcached -v 51494: find library=libevent-2.1.so.6 [0]; searching 51494: search cache=/etc/ld.so.cache 51494: search path=/lib64/tls/x86_64:/lib64/tls:/lib64/x86_64:/lib64:/usr/lib64/tls/x86_64:/usr/lib64/tls:/usr/lib64/x86_64:/usr/lib64 (system search path) 51494: trying file=/lib64/tls/x86_64/libevent-2.1.so.6 51494: trying file=/lib64/tls/libevent-2.1.so.6 51494: trying file=/lib64/x86_64/libevent-2.1.so.6 51494: trying file=/lib64/libevent-2.1.so.6 51494: trying file=/usr/lib64/tls/x86_64/libevent-2.1.so.6 51494: trying file=/usr/lib64/tls/libevent-2.1.so.6 51494: trying file=/usr/lib64/x86_64/libevent-2.1.so.6 51494: trying file=/usr/lib64/libevent-2.1.so.6 51494: /usr/local/bin/memcached: error while loading shared libraries: libevent-2.1.so.6: cannot open shared object file: No such file or directory[root@cephL memcached-1.5.8]# ll /usr/local/lib/libevent-2.1.so.6lrwxrwxrwx. 1 root root 21 5月 29 18:48 /usr/local/lib/libevent-2.1.so.6 -&gt; libevent-2.1.so.6.0.2[root@cephL memcached-1.5.8]# ln -s /usr/local/lib/libevent-2.1.so.6 /usr/lib64/libevent-2.1.so.6[root@cephL memcached-1.5.8]# /usr/local/bin/memcached -d -l 127.0.0.1 -p 11211 -m 150 -u root[root@cephL memcached-1.5.8]# ps -ef | grep memcacheroot 51810 1 0 19:04 ? 00:00:00 /usr/local/bin/memcached -d -l 127.0.0.1 -p 11211 -m 150 -u root2.使用yum安装[root@cephL memcache]# yum install memcached[root@cephL memcache]# /usr/bin/memcached -d -l 127.0.0.1 -p 11211 -m 150 -u root[root@cephL memcache]# ps -ef | grep memcacheroot 40789 1 0 18:34 ? 00:00:00 /usr/bin/memcached -d -l 127.0.0.1 -p 11211 -m 150 -u root 2-3客户端的安装 123451.安装libmemcachedwget https://launchpad.net/libmemcached/1.0/1.0.18/+download/libmemcached-1.0.18.tar.gztar zxvf libmemcached-1.0.18.tar.gz &amp;&amp; cd libmemcached-1.0.18./configure --prefix=/usr/lib/libmemcachedmake &amp;&amp; make install]]></content>
      <tags>
        <tag>memcache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph纠删码bluestore与filestore对比调研]]></title>
    <url>%2F2018%2F05%2F25%2FCeph%E7%BA%A0%E5%88%A0%E7%A0%81bluestore%E4%B8%8Efilestore%E5%AF%B9%E6%AF%94%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[最近学习Ceph Erasure code，查阅了网络上有许多资料。Erasure code的概念与理论相对成熟，但目前在Ceph中实现与应用上有诸多困难。所以这里只对Ceph Erasure code应用场景做比较。 使用Erasure code的目的是为了节省存储空间（以牺牲计算资源换取存储资源）。 bluestore OSD 在luminous版本之前，只适用于像RGW那样的应用场景（full object writes and appends），不支持partial writes。 （1）用法一 在luminous版本，为了适用于RBD和CephFS的场景，增加了partial writes功能，但是只有在bluestore OSD上可以启用。 Erasure coded pools不支持omap，因此要将它们与RBD和CephFS一起使用，必须明确指示它们将data存储在ec pool中，将metadata存储在replicated pool中。 对于CephFS，在创建file system时通过file layouts设定一个erasure coded pool作为默认的data pool。 （2）用法二 在erasure coded pool之前设置一个cache tier。 filestore OSD （1）用法一 Erasure coded pools需要比replicated pools更多的资源，并且缺少一些功能，如omap。为了克服这些限制，可以在erasure coded pool之前设置一个cache tier。 由于使用cache tier并且与Erasure coded相关联，那么Erasure coded的性能同时与cache tier特性相关。这是一个纠结的问题！ Ceph Cache Tiering 如上图，cache tier为客户端读取存储在后端的数据子集提供更好的I/O性能。在创建pool时，Cache Tier层pool作为Storage Tier层pool的缓存。 Ceph objecter handles 负责处理objects放置位置。 tiering agent 决定何时将缓存中的objects刷新到后端存储层。所以cache tier和后端存储层对Ceph clients来说是完全透明的。 但是，管理员可以配置此迁移的发生方式。 主要有两种情况： Writeback Mode: 当管理员以writeback mode配置tiers时，Ceph clients将数据写入缓存层并从缓存层接收ACK。经过一段时间后，写入缓存层的数据将迁移到存储层，并从缓存层中清除。从概念上讲，缓存层被覆盖在后端存储层的“前面”。当Ceph client需要驻留在存储层中的数据时，cache tiering agent会在读取数据时将数据迁移到缓存层，然后将其发送到Ceph client。此后，Ceph client可以使用缓存层执行I/O，直到数据变为非活动状态。这对于易变数据（例如照片/视频编辑，交易数据等）是理想的。 Read-proxy Mode: 此模式将使用已存在于缓存层中的任何objects，但如果缓存中没有objects，则将请求代理到存储层。这对于从writeback mode转换为禁用缓存非常有用，因为它允许负载在缓存耗尽时正常运行，而无需向缓存中添加任何新objects。 cache tier所有迁移模式： 官方文档明确指出： A WORD OF CAUTION Cache tiering会降低大多数workloads的性能。用户在使用此功能之前应特别小心。 Workload dependent: 缓存是否会提高性能，高度依赖于负载。由于将objects移入或移出缓存会产生额外成本，因此只有在数据集中访问存在较大偏差时才会生效，这样大多数请求才会命中少量objects。缓存池应该大到足以捕获你的负载，以避免抖动。 Difficult to benchmark: 使用cache tiering，用户常规衡量性能的基准测试将显得很糟糕，部分原因是由于很少有人将请求集中在一小部分objects上，所以缓存“预热”可能需要很长时间，同时预热可能带来更高的成本。 Usually slower: 对于缓存分层不友好的负载，性能通常比没有设置cache tiering enabled的普通RADOS pool慢。 librados object enumeration: 在这种情况下，librados级别的object enumeration API并不一致。如果您的应用程序直接使用librados并依赖于object enumeration，则缓存分层可能无法按预期工作。 Complexity: 启用缓存分层会带来额外的复杂性。这会增加其他用户尚未遇到的错误的可能性，并且会使您的部署处于更高级别的风险中。 KNOWN GOOD WORKLOADS RGW time-skewed: 如果RGW负载几乎所有读取操作都针对最近写入的objects，可配置为一段时间后，将最近写入的对象从缓存层迁移到存储层，这种场景可以很好地工作。 KNOWN BAD WORKLOADS 已知下列配置对cache tiering效果不佳。 RBD with replicated cache and erasure-coded base: 这是一个常见的要求，但通常表现不佳。即使合理偏差的负载仍然会向cold objects发送一些small writes操作，由于erasure-coded pool尚不支持small writes操作，因此必须将整个（通常为4 MB）objects迁移到缓存中以满足small write（通常为4 KB）。只有少数用户成功部署了此配置，并且仅适用于他们，因为他们的数据extremely cold（备份）并且对性能没有任何要求。 RBD with replicated cache and base: 具有replicated base tier（存储层）的RBD效果要好于erasure coded base tier（存储层）时的RBD，但它仍高度依赖于负载中的偏差量，而且很难验证。用户需要很好地理解他们的负载，并需要调整缓存分层参数。 总结 1、 架构角度，如果想用Ceph Erasure code，推荐使用bluestore OSD，而不推荐filestore OSD。 抛开bluestore的稳定性不谈（待验证），filestore OSD的Ceph Erasure code依赖Ceph Cache Tiering会带来额外的复杂性，并且限制了应用场景。 摘自李运华： 组件越多，就越有可能其中某个组件出现故障，从而导致系统故障。这个概率可以算出来，假设组件的故障率是10%（有10%的时间不可用），那么有1个组件的系统可用性是(1-10%)=90%，有2个组件的系统的可用性是(1-10%)*(1-10%)=81%，两者的可用性相差9%。 2、如果使用filestore OSD的Ceph Erasure code，推荐应用在RGW time-skewed场景（官方推荐）。而Ceph Cache Tiering据生产环境落地还有一定距离。 3、近来很多公司都在搞云计算、搞研发，有些是自主研发而有些是基于开源定制开发。如果是基于开源，根据之前的经验，代码直接提交到社区根据社区的发布流程会省去很多额外的工作。 自主研发版本控制是痛苦的，需要考虑社区base更新，自主研发的代码移植到社区最新base（查分代码到吐。。。），如果社区框架变动大，还需要调试改代码。如果是聪明的开发会找到一条扩展的路，尽量更改原有逻辑，如果更改原有逻辑，对测试来说是个很大的挑战。 4、对于产品化一个开源项目，如ceph，还是寻找一个稳定的方案，然后再搞研发，至少研发失败还给自己留条后路，因为你背后有一个强大的社区团队。 附录 中文文档，如看不懂请看英语原文 https://lnsyyj.github.io/2018/05/12/CEPH-ERASURE-CODE/ https://lnsyyj.github.io/2018/05/12/CACHE-TIERING/ https://lnsyyj.github.io/2018/05/15/ERASURE-CODE-PROFILES/ https://lnsyyj.github.io/2018/05/16/JERASURE-ERASURE-CODE-PLUGIN/ https://lnsyyj.github.io/2018/05/16/ISA-ERASURE-CODE-PLUGIN/ https://lnsyyj.github.io/2018/05/17/LOCALLY-REPAIRABLE-ERASURE-CODE-PLUGIN/ https://lnsyyj.github.io/2018/05/18/SHEC-ERASURE-CODE-PLUGIN/]]></content>
  </entry>
  <entry>
    <title><![CDATA[Avocado Test Framework]]></title>
    <url>%2F2018%2F05%2F24%2FAvocado-Test-Framework%2F</url>
    <content type="text"><![CDATA[Avocado是一组工具和库, 用于自动化测试。 以下是基于tag61版本的源码部署方式： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980编译安装方式：sudo yum install -y python2 git gcc python-devel python-pip libvirt-devel libffi-devel openssl-devel libyaml-devel redhat-rpm-config xz-devel libvirt-pythonpip install --upgrade setuptoolsgit clone https://github.com/avocado-framework/avocado.gitcd avocado/git checkout -b my61 61.0make developsudo make requirementssudo python setup.py install遇到问题：[root@cephL avocado]# avocado -vAvocado 61.0[root@cephL avocado]# avocado -husage: avocado [-h] [-v] [--config [CONFIG_FILE]] [--show [STREAM[:LVL]]] [-s] &#123;config,diff,distro,exec-path,list,multiplex,plugins,run,sysinfo,variants&#125; ...Avocado Test Runneroptional arguments: -h, --help show this help message and exit -v, --version show program&apos;s version number and exit --config [CONFIG_FILE] Use custom configuration from a file --show [STREAM[:LVL]] List of comma separated builtin logs, or logging streams optionally followed by LEVEL (DEBUG,INFO,...). Builtin streams are: &quot;test&quot;: test output; &quot;debug&quot;: tracebacks and other debugging info; &quot;app&quot;: application output; &quot;early&quot;: early logging of other streams, including test (very verbose); &quot;remote&quot;: fabric/paramiko debug; &quot;all&quot;: all builtin streams; &quot;none&quot;: disables regular output (leaving only errors enabled). By default: &apos;app&apos; -s, --silent disables regular output (leaving only errors enabled)subcommands: valid subcommands &#123;config,diff,distro,exec-path,list,multiplex,plugins,run,sysinfo,variants&#125; subcommand help config Shows avocado config keys diff Shows the difference between 2 jobs. distro Shows detected Linux distribution exec-path Returns path to avocado bash libraries and exits. list List available tests multiplex Tool to analyze and visualize test variants and params plugins Displays plugin information run Runs one or more tests (native test, test alias, binary or script) sysinfo Collect system information variants Tool to analyze and visualize test variants and paramsFailed to load plugin from module &quot;avocado_runner_remote&quot;: ImportError(&apos;No module named api&apos;,)Failed to load plugin from module &quot;avocado_runner_docker&quot;: ImportError(&apos;No module named api&apos;,)Failed to load plugin from module &quot;avocado_runner_vm&quot;: ImportError(&apos;No module named api&apos;,)fabric库升级导致No module named api，以下为解决办法：[root@cephL avocado]# pip list | grep fabricfabric (2.0.1)[root@cephL avocado]# pip uninstall fabricUninstalling fabric-2.0.1: /root/.local/lib/python2.7/site-packages/fabric-2.0.1-py2.7.eggProceed (y/n)? y Successfully uninstalled fabric-2.0.1[root@cephL avocado]# pip install fabric==1.14.0测试avocado运行状态[root@cephL avocado]# avocado run /bin/true JOB ID : 7c46b08f9cc78635ef6d4c0b1109da7c5429812aJOB LOG : /root/avocado/job-results/job-2018-05-24T21.58-7c46b08/job.log (1/1) /bin/true: PASS (0.02 s)RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0 | CANCEL 0JOB TIME : 0.12 sJOB HTML : /root/avocado/job-results/job-2018-05-24T21.58-7c46b08/results.html python依赖，以下版本运行正常 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112[root@cephL scripts]# pip list aexpect (1.4.0)asn1crypto (0.24.0)avocado-framework (61.0, /usr/lib/python2.7/site-packages/avocado_framework-61.0-py2.7.egg)avocado-framework-plugin-glib (61.0, /root/github/avocado/optional_plugins/glib)avocado-framework-plugin-golang (61.0, /root/github/avocado/optional_plugins/golang)avocado-framework-plugin-loader-yaml (61.0, /root/github/avocado/optional_plugins/loader_yaml)avocado-framework-plugin-result-html (61.0, /root/github/avocado/optional_plugins/html)avocado-framework-plugin-result-upload (61.0, /root/github/avocado/optional_plugins/result_upload)avocado-framework-plugin-resultsdb (61.0, /root/github/avocado/optional_plugins/resultsdb)avocado-framework-plugin-robot (61.0, /root/github/avocado/optional_plugins/robot)avocado-framework-plugin-runner-docker (61.0, /root/github/avocado/optional_plugins/runner_docker)avocado-framework-plugin-runner-remote (61.0, /root/github/avocado/optional_plugins/runner_remote)avocado-framework-plugin-runner-vm (61.0, /root/github/avocado/optional_plugins/runner_vm)avocado-framework-plugin-varianter-pict (61.0, /root/github/avocado/optional_plugins/varianter_pict)avocado-framework-plugin-varianter-yaml-to-mux (61.0, /root/github/avocado/optional_plugins/varianter_yaml_to_mux)Babel (0.9.6)backports.lzma (0.0.11)backports.ssl-match-hostname (3.5.0.1)bcrypt (3.1.4)Beaker (1.5.4)ceph-deploy (2.0.0)ceph-detect-init (1.0.1)ceph-disk (1.0.0)ceph-volume (1.0.0)cephfs (2.0.0)cffi (1.11.5)chardet (2.2.1)CherryPy (3.2.2)configobj (4.7.2)cryptography (2.2.2)decorator (3.4.0)Django (1.6.11.7)django-filter (0.9.2)djangorestframework (2.4.3)djangorestframework-bulk (0.2)enum34 (1.1.6)Fabric (1.14.0)Flask (0.10.1, /usr/lib/python2.7/site-packages)idna (2.6)iniparse (0.4)invoke (1.0.0)ipaddress (1.0.16)IPy (0.75)itsdangerous (0.23)Jinja2 (2.7.2)kmod (0.1)libvirt-python (3.9.0)M2Crypto (0.21.1)m2ext (0.1)Mako (0.8.1)MarkupSafe (0.11)netaddr (0.7.5)netifaces (0.10.4)nose (1.3.7)numpy (1.7.1)pam (0.1.4)paramiko (2.4.1)Paste (1.7.5.1)pbr (4.0.3)pecan (0.4.5)perf (0.1)Pillow (2.0.0)pip (8.1.2)policycoreutils-default-encoding (0.1)prettytable (0.7.2)psycopg2 (2.5.1)pudb (2017.1.4)pyasn1 (0.4.3)pycparser (2.18)pycurl (7.19.0)Pygments (2.2.0)pygobject (3.22.0)pygpgme (0.3)pyliblzma (0.5.3)PyNaCl (1.2.1)pyOpenSSL (0.13.1)pyparsing (1.5.6)pystache (0.5.4)python-linux-procfs (0.4.9)python-memcached (1.48)pyudev (0.15)pyxattr (0.5.1)PyYAML (3.12)rados (2.0.0)rbd (2.0.0)requests (2.6.0)resultsdb-api (2.0.0)rgw (2.0.0)robotframework (3.0.4)rtslib-fb (2.1.63)schedutils (0.4)seobject (0.1)sepolicy (1.1)setuptools (0.9.8)simplegeneric (0.8)simplejson (3.15.0)singledispatch (3.4.0.2)six (1.11.0)slip (0.4.0)slip.dbus (0.4.0)stevedore (1.28.0)Tempita (0.5.1)urlgrabber (3.10)urllib3 (1.10.2)urwid (2.0.1)virtualenv (16.0.0)WebOb (1.2.3)WebTest (1.3.4)Werkzeug (0.9.1)wheel (0.31.0)yum-metadata-parser (1.1.4) debug方式 1avocado --show test run examples/tests/sleeptest.py 添加测试case方法 123456789101112131415161718[root@cephL ~]# mkdir testcase &amp;&amp; cd testcase[root@cephL ~]# vi sleeptest.py (添加如下代码，http://avocado-framework.readthedocs.io/en/61.0/WritingTests.html)import timefrom avocado import Testclass SleepTest(Test): def test(self): sleep_length = self.params.get(&apos;sleep_length&apos;, default=1) self.log.debug(&quot;Sleeping for %.2f seconds&quot;, sleep_length) time.sleep(sleep_length)运行case[root@cephL ~]# avocado run /root/mac/testcaseJOB ID : b5fcd62a2baf4c3b94adb6cfd14fe6e8e767dc4aJOB LOG : /root/avocado/job-results/job-2018-06-06T00.59-b5fcd62/job.log (1/1) /root/mac/testcase/sleeptest.py:SleepTest.test: PASS (1.01 s)RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0 | CANCEL 0JOB TIME : 1.13 sJOB HTML : /root/avocado/job-results/job-2018-06-06T00.59-b5fcd62/results.html Accessing test parameters每个test都有一组参数，可以通过self.params.get($name, $path=None, $default=None)进行访问，其中： 123name - name of the parameter (key) --- 参数的名称(键)path - where to look for this parameter (when not specified uses mux-path) --- 在何处查找此参数(未指定时使用mux-path)default - what to return when param not found --- 未找到参数时返回的内容 path有点棘手。Avocado使用tree来表示参数。在简单的场景中，您不必担心，您将在默认路径中找到所有值（values），但最终您可能需要check-out Test parameters以了解详细信息。 假设您的test收到以下参数 (在下一节中您将学习如何执行这些参数): 1234567$ avocado variants -m examples/tests/sleeptenmin.py.data/sleeptenmin.yaml --variants 2...Variant 1: /run/sleeptenmin/builtin, /run/variants/one_cycle /run/sleeptenmin/builtin:sleep_method =&gt; builtin /run/variants/one_cycle:sleep_cycles =&gt; 1 /run/variants/one_cycle:sleep_length =&gt; 600... 在test中, 您可以通过以下方法访问这些参数: 123self.params.get(&quot;sleep_method&quot;) # returns &quot;builtin&quot;self.params.get(&quot;sleep_cycles&quot;, &apos;*&apos;, 10) # returns 1self.params.get(&quot;sleep_length&quot;, &quot;/*/variants/*&quot; # returns 600 注意：在可能发生冲突的复杂情况下，path非常重要，因为当有多个具有相同key的values匹配查询时，avocado会引发异常。如前所述，您可以通过使用特定path或定义custom mux-path来避免这些情况，它允许指定解析层次结构。更多细节可以在Test parameters中找到。 Running multiple variants of tests在上一节中, 我们描述了如何处理参数。现在, 让我们来看看如何生成它们, 并使用不同的参数执行测试。 variants subsystem允许创建参数的多个variants（变体）, 以及使用这些参数variants（变体）执行测试。此subsystem是可插拔的, 因此您可以使用自定义插件来生成variants（变体）。为了简单起见，我们使用Avocado的主要实现，名为”yaml_to_mux”。 该”yaml_to_mux”插件接受YAML文件。这将创建一个树状结构，将变量存储为参数，并使用自定义标记（tags）将locations标记为”multiplex” domains。 1The “yaml_to_mux” plugin accepts YAML files. Those will create a tree-like structure, store the variables as parameters and use custom tags to mark locations as “multiplex” domains. 让我们使用examples/tests/sleeptenmin.py.data/sleeptenmin.yaml文件作为示例: 123456789101112131415161718sleeptenmin: !mux builtin: sleep_method: builtin shell: sleep_method: shellvariants: !mux one_cycle: sleep_cycles: 1 sleep_length: 600 six_cycles: sleep_cycles: 6 sleep_length: 100 one_hundred_cycles: sleep_cycles: 100 sleep_length: 6 six_hundred_cycles: sleep_cycles: 600 sleep_length: 1 其产生以下结构和参数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263$ avocado variants -m examples/tests/sleeptenmin.py.data/sleeptenmin.yaml --summary 2 --variants 2Multiplex tree representation: ┗━━ run ┣━━ sleeptenmin ┃ ╠══ builtin ┃ ║ → sleep_method: builtin ┃ ╚══ shell ┃ → sleep_method: shell ┗━━ variants ╠══ one_cycle ║ → sleep_length: 600 ║ → sleep_cycles: 1 ╠══ six_cycles ║ → sleep_length: 100 ║ → sleep_cycles: 6 ╠══ one_hundred_cycles ║ → sleep_length: 6 ║ → sleep_cycles: 100 ╚══ six_hundred_cycles → sleep_length: 1 → sleep_cycles: 600Multiplex variants (8):Variant builtin-one_cycle-f659: /run/sleeptenmin/builtin, /run/variants/one_cycle /run/sleeptenmin/builtin:sleep_method =&gt; builtin /run/variants/one_cycle:sleep_cycles =&gt; 1 /run/variants/one_cycle:sleep_length =&gt; 600Variant builtin-six_cycles-723b: /run/sleeptenmin/builtin, /run/variants/six_cycles /run/sleeptenmin/builtin:sleep_method =&gt; builtin /run/variants/six_cycles:sleep_cycles =&gt; 6 /run/variants/six_cycles:sleep_length =&gt; 100Variant builtin-one_hundred_cycles-633a: /run/sleeptenmin/builtin, /run/variants/one_hundred_cycles /run/sleeptenmin/builtin:sleep_method =&gt; builtin /run/variants/one_hundred_cycles:sleep_cycles =&gt; 100 /run/variants/one_hundred_cycles:sleep_length =&gt; 6Variant builtin-six_hundred_cycles-a570: /run/sleeptenmin/builtin, /run/variants/six_hundred_cycles /run/sleeptenmin/builtin:sleep_method =&gt; builtin /run/variants/six_hundred_cycles:sleep_cycles =&gt; 600 /run/variants/six_hundred_cycles:sleep_length =&gt; 1Variant shell-one_cycle-55f5: /run/sleeptenmin/shell, /run/variants/one_cycle /run/sleeptenmin/shell:sleep_method =&gt; shell /run/variants/one_cycle:sleep_cycles =&gt; 1 /run/variants/one_cycle:sleep_length =&gt; 600Variant shell-six_cycles-9e23: /run/sleeptenmin/shell, /run/variants/six_cycles /run/sleeptenmin/shell:sleep_method =&gt; shell /run/variants/six_cycles:sleep_cycles =&gt; 6 /run/variants/six_cycles:sleep_length =&gt; 100Variant shell-one_hundred_cycles-586f: /run/sleeptenmin/shell, /run/variants/one_hundred_cycles /run/sleeptenmin/shell:sleep_method =&gt; shell /run/variants/one_hundred_cycles:sleep_cycles =&gt; 100 /run/variants/one_hundred_cycles:sleep_length =&gt; 6Variant shell-six_hundred_cycles-1e84: /run/sleeptenmin/shell, /run/variants/six_hundred_cycles /run/sleeptenmin/shell:sleep_method =&gt; shell /run/variants/six_hundred_cycles:sleep_cycles =&gt; 600 /run/variants/six_hundred_cycles:sleep_length =&gt; 1 您可以看到它为每个multiplex domain创建了所有可能的variants（变体）。它们由YAML文件中的!mux 标签（tag）定义，并在树视图中以单行显示（与具有values的单个nodes的双线进行比较）。总共它会为每个test产生8个variants（变体）： 12345678910111213$ avocado run --mux-yaml examples/tests/sleeptenmin.py.data/sleeptenmin.yaml -- passtest.pyJOB ID : cc7ef22654c683b73174af6f97bc385da5a0f02fJOB LOG : /home/medic/avocado/job-results/job-2017-01-22T11.26-cc7ef22/job.log (1/8) passtest.py:PassTest.test;builtin-one_cycle-f659: PASS (0.01 s) (2/8) passtest.py:PassTest.test;builtin-six_cycles-723b: PASS (0.01 s) (3/8) passtest.py:PassTest.test;builtin-one_hundred_cycles-633a: PASS (0.01 s) (4/8) passtest.py:PassTest.test;builtin-six_hundred_cycles-a570: PASS (0.01 s) (5/8) passtest.py:PassTest.test;shell-one_cycle-55f5: PASS (0.01 s) (6/8) passtest.py:PassTest.test;shell-six_cycles-9e23: PASS (0.01 s) (7/8) passtest.py:PassTest.test;shell-one_hundred_cycles-586f: PASS (0.01 s) (8/8) passtest.py:PassTest.test;shell-six_hundred_cycles-1e84: PASS (0.01 s)RESULTS : PASS 8 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 0.16 s 还有其他选项可以影响参数（params），请使用命令avocado run -h查看详细信息，或查看使用文档Test parameters。 执行case123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@cephL]# vi examples/tests/sleeptenmin.py.data/sleeptenmin.yamlsleeptenmin: !mux builtin: sleep_method: builtin shell: sleep_method: shellvariants: !mux one_cycle: sleep_cycles: 1 sleep_length: 600 six_cycles: sleep_cycles: 6 sleep_length: 100 one_hundred_cycles: sleep_cycles: 100 sleep_length: 6 six_hundred_cycles: sleep_cycles: 600 sleep_length: 1[root@cephL]# vi /root/mac/examples/tests/passtest.py#!/usr/bin/env pythonfrom avocado import mainfrom avocado import Testclass PassTest(Test): &quot;&quot;&quot; Example test that passes. :avocado: tags=fast &quot;&quot;&quot; def test(self): &quot;&quot;&quot; A test simply doesn&apos;t have to fail in order to pass &quot;&quot;&quot; passif __name__ == &quot;__main__&quot;: main()[root@cephL]# avocado run --mux-yaml /root/mac/examples/tests/sleeptenmin.py.data/sleeptenmin.yaml -- /root/mac/examples/tests/passtest.pyJOB ID : 2ae9467667984d8c686087859e953a20bfd294a2JOB LOG : /root/avocado/job-results/job-2018-06-06T02.37-2ae9467/job.log (1/8) /root/mac/examples/tests/passtest.py:PassTest.test;builtin-one_cycle-f659: PASS (0.01 s) (2/8) /root/mac/examples/tests/passtest.py:PassTest.test;builtin-six_cycles-723b: PASS (0.01 s) (3/8) /root/mac/examples/tests/passtest.py:PassTest.test;builtin-one_hundred_cycles-633a: PASS (0.02 s) (4/8) /root/mac/examples/tests/passtest.py:PassTest.test;builtin-six_hundred_cycles-a570: PASS (0.01 s) (5/8) /root/mac/examples/tests/passtest.py:PassTest.test;shell-one_cycle-55f5: PASS (0.01 s) (6/8) /root/mac/examples/tests/passtest.py:PassTest.test;shell-six_cycles-9e23: PASS (0.01 s) (7/8) /root/mac/examples/tests/passtest.py:PassTest.test;shell-one_hundred_cycles-586f: PASS (0.01 s) (8/8) /root/mac/examples/tests/passtest.py:PassTest.test;shell-six_hundred_cycles-1e84: PASS (0.02 s)RESULTS : PASS 8 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0 | CANCEL 0JOB TIME : 0.50 sJOB HTML : /root/avocado/job-results/job-2018-06-06T02.37-2ae9467/results.html]]></content>
      <tags>
        <tag>automated testing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实例妙解Sed和Awk的秘密]]></title>
    <url>%2F2018%2F05%2F21%2F%E5%AE%9E%E4%BE%8B%E5%A6%99%E8%A7%A3Sed%E5%92%8CAwk%E7%9A%84%E7%A7%98%E5%AF%86%2F</url>
    <content type="text"><![CDATA[实例妙解Sed和Awk的秘密笔记 课程地址：http://www.imooc.com/learn/819 课程教师：annieose 正则课程介绍Linux工程师面对的问题 面对一堆文本要分析？ 面对乱乱的数据要处理？ 面对长长的服务器日志找报错？ sed和awk应用场景 例如：在服务器日志fresh.log中，找到所有报错的日期 1sed -n &apos;/Error/p&apos; fresh.log | awk &apos;&#123;print $1&#125;&apos; Linux三大利器 grep：查找 | sed：行编辑器 | 一行指令，轻松搞定 awk：文本处理工具 | 学习目标 掌握sed/awk常用处理方法 得心应手使用工具 提高效率，事半功倍 正则表达式主要内容 正则表达式的应用与学习方法 正则中的字符/字符串 正则中的表达式 正则表达式目标 查找 | 取出 | 符合条件的某个字符或字符串 匹配 | 正则单字符的表示方式字符 ==》 特定字符、范围内字符、任意字符 ==》 组合 12345678910111213141516171819202122232425262728特定字符：某个具体的字符。例如：&apos;a&apos;，&apos;1&apos; grep &apos;a&apos; passwd grep &apos;1&apos; passwd范围内字符：单个字符[] 数字字符：[0-9]，[259] grep [0-9] passwd grep [259] passwd 小写字符：[a-z] grep &apos;[a-z]&apos; passwd 大写字符：[A-Z] grep &apos;[A-Z]&apos; passwd 扩大范围举例： grep &apos;[a-zA-Z]&apos; passwd grep &apos;[,:/_]&apos; passwd范围内字符：反向字符^ 取反：[^0-9]，[^0]任意字符： 代表任何一个字符：&apos;.&apos; grep &apos;.&apos; passwd 注意与[.]和\.的区别 grep &apos;[.]&apos; passwd 代表小数点，而不是任意字符 grep &apos;\.&apos; passwd 代表小数点，而不是任意字符 正则其他符号的表示方式123456789101112131415161718边界字符：头尾字符 ^：^root 注意与[^]的区别 grep &apos;^root&apos; passwd $:bash$ grep &apos;bash$&apos; passwd 空行的表示：^$ grep &apos;^$&apos; passwd 元字符(代表普通字符或特殊字符) \w:匹配任何字类字符，包括下划线([A-Za-z0-9]) \W:匹配任何非字类字符，([^A-Za-z0-9]) grep &apos;\w&apos; passwd grep &apos;\W&apos; passwd \b:代表单词的分隔 grep &apos;\bx\b&apos; passwd 正则组合-重复字符表示1234字符串 &apos;root&apos; &apos;1000&apos; &apos;m..c&apos;grep &apos;m..c&apos; passwd (以m开头c结尾，长度为4的字符串)grep &apos;[A-Z][a-z]&apos; passwd (大写字符与小写字符放在一起的字符串组合)grep &apos;\b[0-9][0-9]\b&apos; passwd (两位数字的组合) 字符串 ==》 重复 + 逻辑 ==》 组合表达式 123456789101112131415161718192021222324252627282930313233343536373839重复： *：零次或多次匹配前面的字符或子表达式 +：一次或多次匹配前面的字符或子表达式 ?：零次或一次匹配前面的字符或子表达式[root@cephL ~]# cat test.txt seseseseseseeeeeeeeeeeeeeesoooooooo+se+se+[root@cephL ~]# grep &apos;se*&apos; test.txt seseseseseseeeeeeeeesoooooooo+se+se+[root@cephL ~]# grep &apos;se\+&apos; test.txt seseseseseseeeeeeeee+se+se+[root@cephL ~]# grep &apos;se\?&apos; test.txt seseseseseseeeeeeeeesoooooooo+se+se+[root@cephL ~]# grep &apos;\(se\)\+&apos; test.txt seseseseseseeeeeeeee+se+se+重复特定次数：&#123;n,m&#125; *：&#123;0,&#125; +：&#123;1,&#125; ?：&#123;0,1&#125;[root@cephL ~]# grep &apos;\b[0-9]\&#123;2,3\&#125;\b&apos; passwd (截取2位或3位数字) 正则组合-逻辑符合表示123456789101112131415任意字符串的表示：.* 例如：^r.* m.*c 注意与m..c区别grep &apos;^r.*&apos; passwdgrep &apos;m.*c&apos; passwdgrep &apos;m..c&apos; passwdgrep &apos;\bm.*c\b&apos; passwdgrep &apos;\bm[a-z]*c\b&apos; passwd逻辑的表示： |：&apos;/bin\(false\|true\)&apos;grep &apos;bin/\(bash\|nologin\)&apos; passwd (或者包含bash，或者包含nologin)正则表达式字符组合小结： 重复的表示：* + ? &#123;,&#125; 逻辑的表示：| 正则-案例演123456789案例一 匹配4-10位qq号 grep &apos;^[0-9]\&#123;4,10\&#125;$&apos; qq.txt案例二 匹配15位或18位身份证号(支持带X的) grep &apos;^[1-9]\([0-9]\&#123;13\&#125;\|[0-9]\&#123;16\&#125;\)[0-9xX]$&apos; qq.txt案例三 匹配密码(由数字、26个字母和下划线组成) grep &apos;^\w\+$&apos; qq.txt 正则总结 巧妙破解sed主要内容和原理介绍sed工具用途 123自动处理文件分析日志文件修改配置文件 sed是如何进行文本处理的？我们称sed是一个流处理编辑器 实际的处理流程是行处理 sed一次处理一行内容 sed不改变文件内容（除非重定向） sed应用的基本格式介绍正则选定文本 ==》 sed进行处理 1234567使用sed格式： 命令行格式 `sed [options] &apos;command&apos; file(s)` `options: -e ; -n` `command: 行定位（正则）+ sed命令（操作）` 脚本格式 `sed -f scriptfile file(s)` 使用sed举例 12sed -n &apos;/root/p&apos;sed -e &apos;10,20d&apos; -e &apos;s/false/true/g&apos; sed基本操作命令-p命令基本操作命令（1） 12345678910111213141516171819-p（打印相关的行）与-n配合使用 sed -n &apos;p&apos; passwd定位一行：x;/pattern/ （x代表行号） sed -n &apos;10p&apos; passwd (打印第10行) nl passwd | sed -n &apos;10p&apos; (nl打印行号) sed -n &apos;/root/p&apos; passwd (打印)定位几行：x,y;/pattern/,x; （x，y代表行号） /pattern1/,/pattern2/; x,y! (取反)nl passwd | sed -n &apos;10,20p&apos; (打印10行到20行)nl passwd | sed -n &apos;/operator/,/games/p&apos;nl passwd | sed -n &apos;10!p&apos; (不选择第10行)nl passwd | sed -n &apos;10,20!p&apos; (不选择10行到20行)定位间隔几行：first~step nl passwd | sed -n &apos;1~2p&apos; (从第1行开始，间隔2-1行，再打印) 基本操作命令（2） 1234567891011-a （新增行）/ i（插入行）-c （替代行）-d（删除行） nl passwd | sed &apos;5a ===========&apos; (第5行和第6行之间增加分隔符===========) nl passwd | sed &apos;1,5a===========&apos; (第1行和第5行之间，每一行都增加分隔符===========） nl passwd | sed &apos;5i===========&apos; (第5行之前插入分隔符===========) nl passwd | sed &apos;1,5i===========&apos; (第1行和第5行之前，每一行都插入分隔符===========） nl passwd | sed &apos;40c 123123&apos; (将40行替换成123123) nl passwd | sed &apos;35,40c 123123&apos; (将35行到40行替换成123123，35行以后的内容就没有了，只有123123这一行，并不是每一行进行替换，而是对这些行做一个整体的替换) nl passwd | sed &apos;/mooc/d&apos; (删除mooc所在行) sed不会改变源文件的内容，如果要保存结果需要重定向 案例： 12345678910111213141516案例一：优化服务器的配置 在ssh的配置文件加入相应文本： &quot; Port 52113 PermitRootLogin no PermitEmptyPasswords no &quot; sed &apos;$a \ port\ 52113 \n permitroologin no&apos; sshd_config (空格需要\转义)案例二：文本处理 删除文本中空行 sed &apos;/^$/d&apos; sshd_config (空行用正则/^$/表示)案例三：服务器日志处理 服务器log中找出error报错 sed -n &apos;/Error/p&apos; 基本操作命令（3） 1234567891011-s（替换）：分隔符/,#等 sed &apos;s/nologin/l/&apos; passwd (nologin替换为l)-g（全局）：替换标志 sed &apos;s/:/%/&apos; passwd (只是把每一行的第一个&apos;:&apos;替换为&apos;%&apos;) sed &apos;s/:/%/g&apos; passwd (所有的&apos;:&apos;都被替换为&apos;%&apos;)案例四：数据筛选 获取enp0s8的ip ifconfig enp0s8 | sed -n &apos;/inet /p&apos; | sed &apos;s/inet//&apos; (centos7) ifconfig enp0s8 | sed -n &apos;/inet /p&apos; | sed &apos;s/inet.*r://&apos; (Ubuntu上有:) ifconfig enp0s8 | sed -n &apos;/inet /p&apos; | sed &apos;s/inet//&apos; | sed &apos;s/n.*$//&apos; 高级操作命令（1） 12-&#123;&#125;：多个sed命令，用;分开 nl passwd | sed &apos;&#123;20,30d;s/nologin/login/&#125;&apos; (先删除20行到30行，然后剩下的行nologin替换成login) 高级操作命令（2） 12345-n：读取下一个输入行（用下一个命令处理） nl passwd | sed -n &apos;&#123;n;p&#125;&apos; (你会发现跳行，输出2、4、6、8行。执行过程是sed读入一行后，接着n又读入下一行。输出偶数行) nl passwd | sed -n &apos;2~2p&apos; (输出偶数行) nl passwd | sed -n &apos;&#123;p;n&#125;&apos; (输出奇数行) nl passwd | sed -n &apos;1~2p&apos; (输出奇数行，与上一条命令相等) 高级操作命令（3） 12345-&amp;：替换固定字符串 sed &apos;s/^[a-z_-]\+/&amp; /&apos; passwd (用户名后面加上空格) root:x:0:0:root:/root:/bin/bash (源字符串) root :x:0:0:root:/root:/bin/bash (替换后字符串) 如果替换的字符串与源字符串有重复串，可以用&amp;。类似于在源字符串后追加 案例： 123456789101112案例一：大小写转换 将用户名的首字母转换为大写/小写 （元字符\u \l \U \L：转换为大写/小写字符，小写的\u和\l是对首字母的转换，大写的\U和\L是对一串字符的转换） sed &apos;s/^[a-z_-]\+/\u&amp;/&apos; passwd案例二：大小写转换 将文件夹下的.txt文件名转换为大写 ls *.txt | sed &apos;s/^\w\+/\U&amp;/&apos;案例三：数据筛选 获取passwd中USER、UID和GID sed &apos;s/\(^[a-z_-]\+\):x:\([0-9]\+\):\([0-9]\+\):.*$/USER:\1 UID:\2 GID:\3/&apos; passwd 高级操作命令（4） 1234-\( \)：替换某种(部分)字符串(\1,\2) 案例二：数据筛选 &quot;获取eth0的ip&quot; ifconfig enp0s9 | sed -n &apos;/inet /p&apos; | sed &apos;s/inet \([0-9.]\+\) .*$/\1/&apos; (centos 7) 高级操作命令（5） 12345-r：复制指定文件插入到匹配行(不改变文件内容只改变输出，如果想改变文件内容需要重定向)-w：复制匹配行拷贝指定文件里(改变目标文件)用处：源文件和目标文件互相操作。sed &apos;1r 123.txt&apos; abc.txt (从123.txt中读取内容，插入到abc.txt的第1行后 )sed &apos;1w abc.txt&apos; 123.txt (从123.txt中读入，写入到abc.txt。1w是指定123.txt中的哪一行写入到abc.txt) 高级操作命令（6） 123q：退出sednl passwd | sed &apos;10q&apos; (执行到第10行就结束)nl passwd | sed &apos;/false/q&apos; (找到第一个false就结束) awk主要用途：统计、制表 awk处理方式：awk一次处理一行内容、awk对每行可以切片处理 1awk &apos;&#123;print $1&#125;&apos; //输出首个单词 使用awk的格式： 123456789101、命令行格式：awk [options] &apos;command&apos; file(s) command: pattern &#123;awk 操作命令&#125; pattern: 正则表达式; 逻辑判断式 command1: pattern &#123;awk 操作命令&#125; 操作命令: 内置函数: print() printf() getline..; 控制指令: if()&#123;...&#125;else&#123;...&#125;;while()&#123;...&#125;;2、脚本格式：awk -f awk-script-file file(s) awk内置参数应用： 123456789101112131415161718awk 内置变量1 $0：表示整个当前行 $1：每行第一个字段 $2：每行第二个字段这些字段是怎样划分的呢？我们用分隔符awk内置参数：分隔符 options：-F field-separator(默认为空格) 例如：awk -F &apos;:&apos; &apos;&#123;print $3&#125;&apos; /etc/passwd [root@cephL]# awk -F &apos;:&apos; &apos;&#123;print $1,$3&#125;&apos; passwd awk -F &apos;:&apos; &apos;&#123;print $1&quot; &quot;$3&#125;&apos; passwd awk -F &apos;:&apos; &apos;&#123;print $1&quot;\t&quot;$3&#125;&apos; passwd awk -F &apos;:&apos; &apos;&#123;print &quot;User:&quot;$1&quot;\t&quot;&quot;UID:&quot;$3&#125;&apos; passwdawk内置变量： NR：每行的记录号(行) NF：字段数量变量(列) FILENAME：正在处理的文件名 例如：awk -F &apos;:&apos; &apos;print FILENAME&apos; passwd awk内置参数应用案例： 123456789101112案例一： 显示/etc/passwd每行的行号，每行的列数，对应行的用户名(print，printf) awk -F &apos;:&apos; &apos;&#123;print &quot;Line: &quot;NR,&quot;Col: &quot;NF,&quot;User: &quot;$1&#125;&apos; passwd awk -F &apos;:&apos; &apos;&#123;printf(&quot;Line: %3s Col: %s User: %s\n&quot;,NR,NF,$1)&#125;&apos; passwd 案例二： 显示/etc/passwd中用户ID大于100的行号和用户名(if...else...) awk -F &apos;:&apos; &apos;&#123;if ($3&gt;100) print &quot;Line: &quot;NR,&quot;User: &quot;$1&#125;&apos; passwd案例三： 在服务器log中找出&apos;Error&apos;的发生日期 sed &apos;/Error/p&apos; fresh.log | awk &apos;&#123;print $1&#125;&apos; awk &apos;/Error/&#123;print $1&#125;&apos; fresh.log 使用awk——逻辑判断式 12345678910111213command: pattern &#123;awk 操作命令&#125; pattern: 正则表达式;逻辑判断式awk逻辑 ~,!~: 匹配正则表达式 ==,!=,&lt;,&gt;: 判断逻辑表达式1、第一个字段是&apos;m&apos;开头的awk -F &apos;:&apos; &apos;$1~/^m.*/&#123;print $1&#125;&apos; passwd2、第一个字段不是&apos;m&apos;开头的awk -F &apos;:&apos; &apos;$1!~/^m.*/&#123;print $1&#125;&apos; passwd3、第三个字段大于100awk -F &apos;:&apos; &apos;$3&gt;100&#123;print $1,$3&#125;&apos; passwd 使用awk——扩展格式 12345678910111213141516171819202122231、命令行格式：awk [options] &apos;command&apos; file(s) command2扩展 BEGIN&#123;print &quot;start&quot;&#125; pattern&#123;commands&#125; END&#123;print &quot;end&quot;&#125; awk是以行为操作单位的，BEGIN是在没有读入第一行之间执行的，END是处理完最后一行执行的案例一： 制表显示/etc/passwd每行的行号，每行的列数，对应行的用户名 awk -F &apos;:&apos; &apos;BEGIN&#123;print &quot;Line\tCol\tUser&quot;&#125;&#123;print NR&quot;\t&quot;NF&quot;\t&quot;$1&#125;END&#123;print &quot;---&quot;FILENAME&quot;---&quot;&#125;&apos; passwd案例二： 统计当前文件夹下的文件/文件夹占用的大小 ls -l | awk &apos;BEGIN&#123;size=0&#125;&#123;size+=$5&#125;END&#123;print &quot;size is &quot;size/1024/1024&quot;M&quot;&#125;&apos;案例三： 统计显示/etc/passwd的账户总人数 awk -F &apos;:&apos; &apos;BEGIN&#123;count=0&#125;$1!~/^$/&#123;count++&#125;END&#123;print &quot;count = &quot;count&#125;&apos; passwd 统计显示UID大于100的用户名 awk -F &apos;:&apos; &apos;BEGIN&#123;count=0&#125;&#123;if ($3 &gt; 100) name[count++]=$1&#125;END&#123;for(i=0;i&lt;count;i++) print i,name[i]&#125;&apos; passwd案例四： 统计netstat -anp 状态下为LISTEN和CONNECTED的连接数量 netstat -anp | awk &apos;$6~/CONNECTED|LISTEN/&#123;sum[$6]++&#125;END&#123;for(i in sum)print i,sum[i]&#125;&apos; awk VS sed 12345awk和sed都可以处理文本awk侧重于复杂逻辑处理sed侧重于正则处理awk和sed可以共同使用]]></content>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CEPH-VOLUME]]></title>
    <url>%2F2018%2F05%2F19%2FCEPH-VOLUME%2F</url>
    <content type="text"><![CDATA[CEPH-VOLUME使用pluggable（可插拔）工具将 OSDs 与 lvm 或physical disks等不同的device技术部署在一起（lvm本身被视为plugin）, 并尝试采用predictable、robust的方法来preparing、activating和starting OSDs。 Overview|Plugin Guide Command Line Subcommands目前支持lvm，以及已经使用ceph-disk部署的普通磁盘（带有GPT分区）。 lvm simple Node inventoryinventory子命令提供有关节点物理磁盘inventory（清单）的信息和元数据。 MIGRATING从Ceph版本13.0.0开始，不推荐使用ceph-disk。弃用警告中的link会链接到此页面。强烈建议用户开始使用ceph-volume。有两种迁移途径： 1、保留使用ceph-disk部署的OSD：simple 命令在禁用ceph-disk触发的同时，提供了接管管理的方法。 2、用ceph-volume重新部署现存的OSD：详细信息请参照 Replacing an OSD 有关为何删除ceph-disk的详细信息，请参阅Why was ceph-disk replaced? 章节。 NEW DEPLOYMENTS对于新的部署，推荐使用lvm，它可以使用任何logical volume作为data OSD 的输入, 也可以从device中设置minimal/naive logical volume。 EXISTING OSDS如果群集已经具有ceph-disk提供的OSDs，则ceph-volume可以用simple的方式接管。在data device或OSD directory上完成扫描，并且ceph-disk被完全禁用。完全支持加密。 LVM实现此功能需要通过lvm子命令来部署OSDs：ceph-volume lvm Command Line Subcommandsprepareprepare 该子命令允许filestore或bluestore设置。 建议在使用ceph-volume lvm之前预先配置逻辑卷。除添加额外元数据外，逻辑卷不会改变。 为了帮助识别volumes，preparing的过程中该工具使用LVM tags分配一些元数据信息。 LVM tags使volume易于发现，帮助识别它们作为Ceph系统的一部分，扮演着什么角色（journal, filestore, bluestore, etc…） 虽然最初支持filestore（默认情况下支持），但可以使用以下命令指定： –filestore –bluestore FILESTORE 这是OSD backend，允许为filestore objectstore OSD 准备逻辑卷。 它可以使用逻辑卷作为OSD data和带分区的physical device或逻辑卷作为journal。除了遵循data和journal的最小大小要求外,这些卷不需要特殊准备。 API调用如下所示： 1ceph-volume lvm prepare --filestore --data volume_group/lv_name --journal journal 启用encryption（加密），需要使用–dmcrypt标志： 1ceph-volume lvm prepare --filestore --dmcrypt --data volume_group/lv_name --journal journal activate create list Internal functionalitylvm子命令的其他部分是internal的，不向用户公开，这些部分解释了如何协同工作，阐明了工具的工作流程。 Systemd Units | lvm SIMPLE实现此功能需要通过simple子命令来管理OSD：ceph-volume simple Command Line Subcommands scan activate systemd 通过接管管理，它会disable所有用于在startup时触发device的ceph-disk systemd units，依赖基本的（可定制的）JSON配置和systemd来启动OSD。 此过程包括两个步骤： 1、Scan（扫描）正在运行的OSD或data device 2、Activate（激活）扫描的OSD 扫描将推断出ceph-volume启动OSD所需的所有内容，因此当需要激活时，OSD可以正常启动而不会受到来自ceph-disk的干扰。 作为激活过程的一部分，负责对udev事件作出反应的ceph-disk的systemd units链接到/dev/null，以便它们完全处于非活动状态。 备注123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960ceph-volume lvm（使用LVM和LVM-based技术，像dmcache去部署OSD）1、activate Discover and mount the LVM device associated with an OSD ID and start the Ceph OSD（Discover并mount与OSD ID关联的LVM设备，然后启动Ceph OSD）2、prepare Format an LVM device and associate it with an OSD（Format LVM device并将其与OSD关联）3、create Create a new OSD from an LVM device（从LVM device创建新的OSD）4、list list logical volumes and devices associated with Ceph（列出与Ceph相关的logical volumes和devices）5、batch Automatically size devices for multi-OSD provisioning with minimal interaction（自动调整devices大小，以最少的交互进行多OSD配置）6、trigger systemd helper to activate an OSD（systemd助手来激活OSD）7、zap Removes all data and filesystems from a logical volume or partition.（从logical volume或分区中删除所有数据和文件系统。）ceph-volume simple（使用ceph-volume管理已部署的OSD）1、trigger systemd helper to activate an OSD（systemd助手来激活OSD）2、activate Enable systemd units to mount configured devices and start a Ceph OSD（使systemd units可以mount已配置的devices，并启动Ceph OSD）3、scan Capture metadata from all running ceph-disk OSDs, OSD data partition or directory（从所有正在运行的ceph-disk OSDs，OSD数据分区或目录中捕获元数据）例如：# ceph-volume simple scan执行命令后，会生成类似/etc/ceph/osd/0-ab0a204a-42e3-4a47-ab4c-0888edf429cb.json文件，文件内容为：&#123; &quot;active&quot;: &quot;ok&quot;, &quot;block&quot;: &#123; &quot;path&quot;: &quot;/dev/disk/by-partuuid/0818811f-d70e-4ff0-91c9-58cd701c9a19&quot;, &quot;uuid&quot;: &quot;0818811f-d70e-4ff0-91c9-58cd701c9a19&quot; &#125;, &quot;block_uuid&quot;: &quot;0818811f-d70e-4ff0-91c9-58cd701c9a19&quot;, &quot;bluefs&quot;: 1, &quot;ceph_fsid&quot;: &quot;c4051efa-1997-43ef-8497-fb02bdf08233&quot;, &quot;cluster_name&quot;: &quot;ceph&quot;, &quot;data&quot;: &#123; &quot;path&quot;: &quot;/dev/vdc1&quot;, &quot;uuid&quot;: &quot;ab0a204a-42e3-4a47-ab4c-0888edf429cb&quot; &#125;, &quot;fsid&quot;: &quot;ab0a204a-42e3-4a47-ab4c-0888edf429cb&quot;, &quot;keyring&quot;: &quot;AQB1FLFdXVHVARAARTKkxT1xgrDNU/QECUqdxA==&quot;, &quot;kv_backend&quot;: &quot;rocksdb&quot;, &quot;magic&quot;: &quot;ceph osd volume v026&quot;, &quot;mkfs_done&quot;: &quot;yes&quot;, &quot;ready&quot;: &quot;ready&quot;, &quot;systemd&quot;: &quot;&quot;, &quot;type&quot;: &quot;bluestore&quot;, &quot;whoami&quot;: 0&#125;# ceph-volume simple activate --all--&gt; activating OSD specified in /etc/ceph/osd/1-fe327306-54a4-4362-870d-92d28cf65e42.jsonRunning command: ln -snf /dev/vdc2 /var/lib/ceph/osd/ceph-1/blockRunning command: chown -R ceph:ceph /dev/vdc2Running command: systemctl enable ceph-volume@simple-1-fe327306-54a4-4362-870d-92d28cf65e42Running command: ln -sf /dev/null /etc/systemd/system/ceph-disk@.service--&gt; All ceph-disk systemd units have been disabled to prevent OSDs getting triggered by UDEV eventsRunning command: systemctl enable --runtime ceph-osd@1Running command: systemctl start ceph-osd@1--&gt; Successfully activated OSD 1 with FSID fe327306-54a4-4362-870d-92d28cf65e42ceph-volume inventory（获取此节点可用的disk清单）例如：ceph-volume inventoryDevice Path Size rotates available Model name/dev/vdb 64.00 MB True True/dev/vdc 50.00 GB True True/dev/vda 100.00 GB True False]]></content>
      <tags>
        <tag>CEPH-Luminous</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SHEC ERASURE CODE PLUGIN]]></title>
    <url>%2F2018%2F05%2F18%2FSHEC-ERASURE-CODE-PLUGIN%2F</url>
    <content type="text"><![CDATA[shec plugin封装了multiple SHEC)库。它允许ceph比Reed Solomon codes更有效地恢复数据。 CREATE AN SHEC PROFILE创建一个新的shec erasure code profile： 12345678910ceph osd erasure-code-profile set &#123;name&#125; \ plugin=shec \ [k=&#123;data-chunks&#125;] \ [m=&#123;coding-chunks&#125;] \ [c=&#123;durability-estimator&#125;] \ [crush-root=&#123;root&#125;] \ [crush-failure-domain=&#123;bucket-type&#125;] \ [crush-device-class=&#123;device-class&#125;] \ [directory=&#123;directory&#125;] \ [--force] 其中： k={data-chunks} 1234Description: 每个object都分为多个data-chunks parts，每个part存储在不同的OSD上。Type: IntegerRequired: No.Default: 4 m={coding-chunks} 1234Description: 计算每个object的coding chunks并将它们存储在不同的OSD上。coding chunks的数量也是在不丢失数据的情况下，允许损失OSD的数量。Type: IntegerRequired: No.Default: 3 c={durability-estimator} 12345Description: 校验chunk的数量，每个data chunk都包含在计算范围内。该数字被用作耐久性估算。例如，如果c = 2，则可以在不丢失数据的情况下down掉2个OSD。The number of parity chunks each of which includes each data chunk in its calculation range. The number is used as a durability estimator. For instance, if c=2, 2 OSDs can be down without losing data.Type: IntegerRequired: No.Default: 2 crush-root={root} 1234Description: crush bucket名字用于CRUSH rule的first step。&quot;take&quot;为step的默认值。Type: StringRequired: No.Default: default crush-failure-domain={bucket-type} 1234Description: 确保不要有两个chunks位于相同故障域的bucket中。例如，如果故障域是host，则不会在同一主机上存储两个chunks。它用于创建CRUSH rule step，比如step chooseleaf host。Type: StringRequired: No.Default: host crush-device-class={device-class} 1234Description: 使用CRUSH map中的crush device class名称，限定存储到特定class devices（例如SSD或HDD）。Type: StringRequired: No.Default: directory={directory} 1234Description: 设置加载erasure code plugin的目录名。Type: StringRequired: No.Default: /usr/lib/ceph/erasure-code –force 123Description: 用相同的名称覆盖已有的profile。Type: StringRequired: No. BRIEF DESCRIPTION OF SHEC’S LAYOUTSSPACE EFFICIENCYSpace efficiency是data chunks与object中所有chunks的比率，表示为k /（k + m）。为了提高space efficiency，您应该增加k或减少m。 12space efficiency of SHEC(4,3,2) = 4/(4+3) = 0.57SHEC(5,3,2) or SHEC(4,2,2) improves SHEC(4,3,2)&apos;s space efficiency DURABILITYSHEC（= c）的第三个参数是一个耐久性估算，它近似于在不丢失数据的情况下可以丢失OSD的数量。SHEC（4,3,2）的耐久性估算= 2 RECOVERY EFFICIENCY描述recovery efficiency的计算超出了本文件的范围，但至少在不增加c的情况下增加m可以提高恢复效率。（但是，在这种情况下，我们必须牺牲space efficiency。） SHEC（4,2,2）→SHEC（4,3,2）：提高了recovery efficiency ERASURE CODE PROFILE EXAMPLES12345$ ceph osd erasure-code-profile set SHECprofile \ plugin=shec \ k=8 m=4 c=3 \ crush-failure-domain=host$ ceph osd pool create shecpool 256 256 erasure SHECprofile]]></content>
      <tags>
        <tag>CEPH-Luminous</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LOCALLY REPAIRABLE ERASURE CODE PLUGIN]]></title>
    <url>%2F2018%2F05%2F17%2FLOCALLY-REPAIRABLE-ERASURE-CODE-PLUGIN%2F</url>
    <content type="text"><![CDATA[通过jerasure plugin，当erasure coded的object存储在多个OSD中时，恢复丢失的1个OSD数据需要从所有其他OSD读取数据并修复。例如，如果jerasure配置为k = 8和m = 4，则丢失1个OSD需要从其他11个OSD读取并修复。 lrc erasure code plugin创建本地奇偶校验chunks，以便能够使用较少的OSD恢复。例如，如果lrc配置为k = 8，m = 4和l = 4，则会为每4个OSD创建一个附加奇偶校验chunk。当1个OSD丢失时，只需要使用4个OSD而不是11个来恢复。 ERASURE CODE PROFILE EXAMPLESREDUCE RECOVERY BANDWIDTH BETWEEN HOSTS当所有主机连接到相同的交换机时，实际上可以观察到减少的带宽。 12345$ ceph osd erasure-code-profile set LRCprofile \ plugin=lrc \ k=4 m=2 l=3 \ crush-failure-domain=host$ ceph osd pool create lrcpool 12 12 erasure LRCprofile REDUCE RECOVERY BANDWIDTH BETWEEN RACKS在Firefly中，只有当primary OSD与丢失chunk在同一个rack中时，才会观察到减少的带宽。 123456$ ceph osd erasure-code-profile set LRCprofile \ plugin=lrc \ k=4 m=2 l=3 \ crush-locality=rack \ crush-failure-domain=host$ ceph osd pool create lrcpool 12 12 erasure LRCprofile CREATE AN LRC PROFILE创建新的lrc erasure code profile： 1234567891011ceph osd erasure-code-profile set &#123;name&#125; \ plugin=lrc \ k=&#123;data-chunks&#125; \ m=&#123;coding-chunks&#125; \ l=&#123;locality&#125; \ [crush-root=&#123;root&#125;] \ [crush-locality=&#123;bucket-type&#125;] \ [crush-failure-domain=&#123;bucket-type&#125;] \ [crush-device-class=&#123;device-class&#125;] \ [directory=&#123;directory&#125;] \ [--force] 其中： k={data chunks} 1234Description: 每个object都分为多个data-chunks parts，每个part存储在不同的OSD上。Type: IntegerRequired: Yes.Example: 4 m={coding-chunks} 1234Description: 计算每个object的coding chunks并将它们存储在不同的OSD上。coding chunks的数量也是在不丢失数据的情况下，允许损失OSD的数量。Type: IntegerRequired: Yes.Example: 2 l={locality} 123456Description: 将coding和data chunks分组为指定size locality的sets。例如，对于k = 4和m = 2，当locality = 3时，创建2个groups，每个group中3个。每个set都可以在不读取另一set中chunks的情况下恢复。Group the coding and data chunks into sets of size locality. For instance, for k=4 and m=2, when locality=3 two groups of three are created. Each set can be recovered without reading chunks from another set.Type: IntegerRequired: Yes.Example: 3 crush-root={root} 1234Description: crush bucket名字用于CRUSH rule的first step。&quot;take&quot;为step的默认值。Type: StringRequired: No.Default: default crush-locality={bucket-type} 1234Description: crush bucket的type，其中由l定义的每个chunks集合将被存储。例如，如果它被设置为rack，则每个l chunks将被放置在不同的rack中。它用于创建CRUSH rule step，如step选择rack。如果没有设置，则不会进行这样的分组。The type of the crush bucket in which each set of chunks defined by l will be stored. For instance, if it is set to rack, each group of l chunks will be placed in a different rack. It is used to create a CRUSH rule step such as step choose rack. If it is not set, no such grouping is done.Type: StringRequired: No. crush-failure-domain={bucket-type} 123456Description: 确保不要有两个chunks位于相同故障域的bucket中。例如，如果故障域是host，则不会在同一主机上存储两个chunks。它用于创建CRUSH rule step，比如step chooseleaf host。Ensure that no two chunks are in a bucket with the same failure domain. For instance, if the failure domain is host no two chunks will be stored on the same host. It is used to create a CRUSH rule step such as step chooseleaf host.Type: StringRequired: No.Default: host crush-device-class={device-class} 1234Description: 使用CRUSH map中的crush device class名称，限定存储到特定class devices（例如SSD或HDD）。Type: StringRequired: No.Default: directory={directory} 1234Description: 设置加载erasure code plugin的目录名。Type: StringRequired: No.Default: /usr/lib/ceph/erasure-code –force 123Description: 用相同的名称覆盖已有的profile。Type: StringRequired: No. LOW LEVEL PLUGIN CONFIGURATIONk和m的和必须是l parameter的倍数。low level configuration parameters不会施加这样的限制，并且为了特定目的而使用它可能更加方便。例如可以定义两个groups，一个group有4个chunks，另一个group有3个chunks。还可以递归地定义locality sets，例如datacenters和racks到datacenters。通过生成low level configuration来实现k/m/l。 lrc erasure code plugin递归地应用erasure code技术，以便在大多数情况下从某些chunks的丢失中仅恢复需要的可用chunks的子集。例如，three coding steps被描述为： 1234chunk nr 01234567step 1 _cDD_cDDstep 2 cDDD____step 3 ____cDDD 其中c是从data chunks D计算出的coding chunks，丢失的chunk 7，可以用后4个chunks恢复。丢失的chunk 2的chunk可以用前4个chunks恢复。 ERASURE CODE PROFILE EXAMPLES USING LOW LEVEL CONFIGURATIONMINIMAL TESTING它严格等同于使用默认的erasure code profile。DD意味着K = 2，c意味着M = 1，并且使用默认的jerasure plugin。 12345$ ceph osd erasure-code-profile set LRCprofile \ plugin=lrc \ mapping=DD_ \ layers=&apos;[ [ &quot;DDc&quot;, &quot;&quot; ] ]&apos;$ ceph osd pool create lrcpool 12 12 erasure LRCprofile REDUCE RECOVERY BANDWIDTH BETWEEN HOSTS当所有主机连接到同一个交换机时，可以观察到减少的带宽使用情况。它相当于k = 4，m = 2和l = 3，尽管chunks的layout不同： 123456789$ ceph osd erasure-code-profile set LRCprofile \ plugin=lrc \ mapping=__DD__DD \ layers=&apos;[ [ &quot;_cDD_cDD&quot;, &quot;&quot; ], [ &quot;cDDD____&quot;, &quot;&quot; ], [ &quot;____cDDD&quot;, &quot;&quot; ], ]&apos;$ ceph osd pool create lrcpool 12 12 erasure LRCprofile REDUCE RECOVERY BANDWIDTH BETWEEN RACKS在Firefly中，只有当primary OSD与丢失chunk在同一个rack中时，才会观察到减少的带宽。 12345678910111213$ ceph osd erasure-code-profile set LRCprofile \ plugin=lrc \ mapping=__DD__DD \ layers=&apos;[ [ &quot;_cDD_cDD&quot;, &quot;&quot; ], [ &quot;cDDD____&quot;, &quot;&quot; ], [ &quot;____cDDD&quot;, &quot;&quot; ], ]&apos; \ crush-steps=&apos;[ [ &quot;choose&quot;, &quot;rack&quot;, 2 ], [ &quot;chooseleaf&quot;, &quot;host&quot;, 4 ], ]&apos;$ ceph osd pool create lrcpool 12 12 erasure LRCprofile TESTING WITH DIFFERENT ERASURE CODE BACKENDSLRC现在使用jerasure作为默认的EC backend。可以使用low level configuration在每个layer基础上指定EC backend/algorithm。layers=’[ [ “DDc”, “” ] ]’ 的第二个参数实际上是用于此level的erasure code profile。下面的例子指定了在lrcpool中使用cauchy technique的ISA backend。 12345$ ceph osd erasure-code-profile set LRCprofile \ plugin=lrc \ mapping=DD_ \ layers=&apos;[ [ &quot;DDc&quot;, &quot;plugin=isa technique=cauchy&quot; ] ]&apos;$ ceph osd pool create lrcpool 12 12 erasure LRCprofile 您还可以为每个layer使用不同的erasure code profile。 123456789$ ceph osd erasure-code-profile set LRCprofile \ plugin=lrc \ mapping=__DD__DD \ layers=&apos;[ [ &quot;_cDD_cDD&quot;, &quot;plugin=isa technique=cauchy&quot; ], [ &quot;cDDD____&quot;, &quot;plugin=isa&quot; ], [ &quot;____cDDD&quot;, &quot;plugin=jerasure&quot; ], ]&apos;$ ceph osd pool create lrcpool 12 12 erasure LRCprofile ERASURE CODING AND DECODING ALGORITHM在layers中找到的steps描述： 12345chunk nr 01234567step 1 _cDD_cDDstep 2 cDDD____step 3 ____cDDD 按顺序应用。例如，如果一个4K的object被encoded，它将首先通过step 1并被分成4个1K的chunk（4个大写D）。它们按顺序存储在chunk 2,3,6和7中。由此计算出两个coding chunks（两个小写字母c）。coding chunks分别存储在chunk 1和5中。 step 2以类似的方式重新使用由step 1创建的内容，并在位置0处存储单个coding chunk c。为了便于阅读，最后四个标记有下划线（_）的chunks将被忽略。 step 3在位置4存储单个coding chunk c。由step 1创建的3个chunks被用于计算该coding chunk，即来自step 1的coding chunk在step 3中变为data chunk。 如果chunk 2丢失： 12345chunk nr 01234567step 1 _c D_cDDstep 2 cD D____step 3 __ _cDDD decoding将尝试通过以相反的顺序来执行steps恢复它：step 3然后是step 2，最后是step 1。 step 3对chunk 2（即它是下划线）一无所知并被跳过。 存储在chunk 0中来自step 2的coding chunk允许其恢复chunk 2的内容。没有更多的chunks要恢复，过程停止，不考虑step 1。 恢复chunk 2需要读取chunks 0,1,3，并写回chunk 2。 如果chunk 2,3,6丢失： 12345chunk nr 01234567step 1 _c _c Dstep 2 cD __ _step 3 __ cD D step 3可以恢复chunk 6的内容： 12345chunk nr 01234567step 1 _c _cDDstep 2 cD ____step 3 __ cDDD step 2无法恢复并被跳过，因为缺少两个chunks（2,3），并且它只能从一个缺失的chunk中恢复。 存储在chunk 1,5中的来自step 1的coding chunk允许它恢复chunk 2,3的内容： 12345chunk nr 01234567step 1 _cDD_cDDstep 2 cDDD____step 3 ____cDDD CONTROLLING CRUSH PLACEMENT默认的CRUSH rule提供位于不同hosts上隔离OSD。 例如： 12345chunk nr 01234567step 1 _cDD_cDDstep 2 cDDD____step 3 ____cDDD 需要8个OSD，每个chunk需要一个OSD。如果hosts位于两个相邻的racks中，则前4个chunks可以放在第一个rack中，最后4个放在第二个rack中。因此，从丢失的单个OSD中恢复并不需要在两个racks之间使用带宽。 例如： 1crush-steps=&apos;[ [ &quot;choose&quot;, &quot;rack&quot;, 2 ], [ &quot;chooseleaf&quot;, &quot;host&quot;, 4 ] ]&apos; 将创建一个rule，将选择两个类型rack的crush buckets，为它们每个选择4个OSD，它们中的每一个都位于类型host的不同buckets中。 CRUSH rule也可以手动制作以实现更精细的控制。]]></content>
      <tags>
        <tag>CEPH-Luminous</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ISA ERASURE CODE PLUGIN]]></title>
    <url>%2F2018%2F05%2F16%2FISA-ERASURE-CODE-PLUGIN%2F</url>
    <content type="text"><![CDATA[ISA plugin封装了ISA库。 它只能在Intel处理器上运行。 CREATE AN ISA PROFILE创建新的isa erasure code profile： 12345678910ceph osd erasure-code-profile set &#123;name&#125; \ plugin=isa \ technique=&#123;reed_sol_van|cauchy&#125; \ [k=&#123;data-chunks&#125;] \ [m=&#123;coding-chunks&#125;] \ [crush-root=&#123;root&#125;] \ [crush-failure-domain=&#123;bucket-type&#125;] \ [crush-device-class=&#123;device-class&#125;] \ [directory=&#123;directory&#125;] \ [--force] 其中： k={data chunks} 1234Description: 每个object都分为多个data-chunks parts，每个part存储在不同的OSD上。Type: IntegerRequired: No.Default: 7 m={coding-chunks} 1234Description: 计算每个object的coding chunks并将它们存储在不同的OSD上。coding chunks的数量也是在不丢失数据的情况下，允许损失OSD的数量。Type: IntegerRequired: No.Default: 3 technique={reed_sol_van|cauchy} 123456Description: ISA插件有两种Reed Solomon形式（https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction）。如果设置了reed_sol_van，则为Vandermonde，如果设置了cauchy（https://en.wikipedia.org/wiki/Vandermonde_matrix），则为Cauchy（https://en.wikipedia.org/wiki/Cauchy_matrix）。Type: StringRequired: No.Default: reed_sol_van crush-root={root} 1234Description: crush bucket名字用于CRUSH rule的first step。&quot;take&quot;为step的默认值。Type: StringRequired: No.Default: default crush-failure-domain={bucket-type} 1234Description: 确保不要有两个chunks位于相同故障域的bucket中。例如，如果故障域是host，则不会在同一主机上存储两个chunks。它用于创建CRUSH rule step，比如step chooseleaf host。Type: StringRequired: No.Default: host crush-device-class={device-class} 1234Description: 使用CRUSH map中的crush device class名称，限定存储到特定class devices（例如SSD或HDD）。Type: StringRequired: No.Default: directory={directory} 1234Description: 设置加载erasure code plugin的目录名。Type: StringRequired: No.Default: /usr/lib/ceph/erasure-code –force 123Description: 用相同的名称覆盖已有的profile。Type: StringRequired: No.]]></content>
      <tags>
        <tag>CEPH-Luminous</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JERASURE ERASURE CODE PLUGIN]]></title>
    <url>%2F2018%2F05%2F16%2FJERASURE-ERASURE-CODE-PLUGIN%2F</url>
    <content type="text"><![CDATA[jerasure plugin是最通用和灵活的插件，它也是Ceph erasure coded pools的默认设置。 jerasure plugin封装了Jerasure库。建议阅读Jelasure文档以更好地理解参数。 CREATE A JERASURE PROFILE创建新的jerasure erasure code profile： 12345678910ceph osd erasure-code-profile set &#123;name&#125; \ plugin=jerasure \ k=&#123;data-chunks&#125; \ m=&#123;coding-chunks&#125; \ technique=&#123;reed_sol_van|reed_sol_r6_op|cauchy_orig|cauchy_good|liberation|blaum_roth|liber8tion&#125; \ [crush-root=&#123;root&#125;] \ [crush-failure-domain=&#123;bucket-type&#125;] \ [crush-device-class=&#123;device-class&#125;] \ [directory=&#123;directory&#125;] \ [--force] 其中： k={data chunks} 1234Description: 每个object都分为多个data-chunks parts，每个part存储在不同的OSD上。Type: IntegerRequired: Yes.Example: 4 m={coding-chunks} 1234Description: 计算每个object的coding chunks并将它们存储在不同的OSD上。coding chunks的数量也是在不丢失数据的情况下，允许损失OSD的数量。Type: IntegerRequired: Yes.Example: 2 technique={reed_sol_van|reed_sol_r6_op|cauchy_orig|cauchy_good|liberation|blaum_roth|liber8tion} 1234Description: 更灵活的technique是reed_sol_van：只需要设置k和m。cauchy_good technique可以更快但您需要调优packetsize。reed_sol_r6_op，liberation，blaum_roth，liber8tion都与RAID6等价，因为它们只能配置m=2。Type: StringRequired: No.Default: reed_sol_van packetsize={bytes} 1234Description: encoding将在一次bytes大小的packets上完成。选择合理的packet大小是困难的。Jeasure文档包含了关于这个topic的大量信息。Type: IntegerRequired: No.Default: 2048 crush-root={root} 12345Description: crush bucket名字用于CRUSH rule的first step。&quot;take&quot;为step的默认值。The name of the crush bucket used for the first step of the CRUSH rule. For intance step take default.Type: StringRequired: No.Default: default crush-failure-domain={bucket-type} 1234Description: 确保不要有两个chunks位于相同故障域的bucket中。例如，如果故障域是host，则不会在同一主机上存储两个chunks。它用于创建CRUSH rule step，比如step chooseleaf host。Type: StringRequired: No.Default: host crush-device-class={device-class} 123Description: 使用CRUSH map中的crush device class名称，限定存储到特定class devices（例如SSD或HDD）。Type: StringRequired: No. directory={directory} 1234Description: 设置加载erasure code plugin的目录名。Type: StringRequired: No.Default: /usr/lib/ceph/erasure-code –force 123Description: 以相同的名称重写已经存在的profile。Type: StringRequired: No.]]></content>
      <tags>
        <tag>CEPH-Luminous</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ERASURE CODE PROFILES]]></title>
    <url>%2F2018%2F05%2F15%2FERASURE-CODE-PROFILES%2F</url>
    <content type="text"><![CDATA[Erasure code由profile定义，并在erasure coded pool关联CRUSH rule时使用。 默认的erasure code profile（在Ceph集群初始化时被创建）提供了与两个副本相同的冗余级别，但相比两副本需要的磁盘空间减少了25％。它被描述为k = 2和m = 1的profile，这意味着信息分布在三个OSD（k + m == 3）上，其中一份可以被丢失。 为了在不增加raw space的情况下提高冗余度，可以创建新的profile。例如，k = 10和m = 4的profile可以通过在14（k + m = 14）个OSD上分散一个object来支撑4个（m = 4）OSD的丢失。首先将object分成10个chunks（如果object为10MB，每个chunk为1MB）并且计算4个coding chunks用于recovery（每个coding chunk具有与data chunk相同的大小，即1MB）。raw space开销只有40％，即使四个OSD同时中断，object也不会丢失。 Jerasure erasure code plugin ISA erasure code plugin Locally repairable erasure code plugin SHEC erasure code plugin OSD ERASURE-CODE-PROFILE SET创建新的erasure code profile： 123456ceph osd erasure-code-profile set &#123;name&#125; \ [&#123;directory=directory&#125;] \ [&#123;plugin=plugin&#125;] \ [&#123;stripe_unit=stripe_unit&#125;] \ [&#123;key=value&#125; ...] \ [--force] 其中： {directory=directory} 1234Description: 设置加载erasure code plugin的目录名。Type: StringRequired: No.Default: /usr/lib/ceph/erasure-code {plugin=plugin} 1234Description: 使用erasure code plugin计算coding chunks和recover missing chunks。Type: StringRequired: No.Default: jerasure {stripe_unit=stripe_unit} 123Description: data chunk中每个stripe的数据量。例如，具有2个data chunks并且StruPeyUng=4K的profile将在chunk 0中放置0-4K，在chunk 1中放置4K-8K，然后再在chunk 0中放置8K-12K。4K的倍数可以获得最佳的性能。当创建pool时，默认值取自monitor config中的osd_pool_erasure_code_stripe_unit。使用此profile的pool的stripe_width将是data chunks数乘以此stripe_unit的数量。Type: StringRequired: No. {key=value} 123Description: 其余的key/value对semantic由erasure code plugin定义。Type: StringRequired: No. –force 123Description: 以相同的名称覆盖现有的profile，并允许设置非4K对齐的stripe_unit。Type: StringRequired: No. OSD ERASURE-CODE-PROFILE RM删除一个erasure code profile： 1ceph osd erasure-code-profile rm &#123;name&#125; 如果profile被某个pool引用，删除将失败。 OSD ERASURE-CODE-PROFILE GET显示一个erasure code profile： 1ceph osd erasure-code-profile get &#123;name&#125; OSD ERASURE-CODE-PROFILE LS列出所有erasure code profiles的名称： 1ceph osd erasure-code-profile ls]]></content>
      <tags>
        <tag>CEPH-Luminous</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CACHE TIERING]]></title>
    <url>%2F2018%2F05%2F12%2FCACHE-TIERING%2F</url>
    <content type="text"><![CDATA[cache tier为Ceph Clients读取存储在后端的数据子集提供更好的I/O性能。Cache tiering意味着创建pool时，可以为pool配置一个相对更快/更昂贵的存储设备作为缓存层，并且配置一个经济的后端数据存储pool（由erasure-coded或相对较慢/较便宜的设备组成）。Ceph objecter handles 负责处理放置objects位置，并且tiering agent 决定何时将缓存中的objects刷新到后端存储层。所以cache tier和后端存储层对Ceph clients来说是完全透明的。 cache tiering agent handles自动处理缓存层和后端存储层之间的数据迁移。但是，管理员可以配置此迁移的发生方式。 主要有两种情况： Writeback Mode: 当管理员以writeback mode配置tiers时，Ceph clients将数据写入缓存层并从缓存层接收ACK。经过一段时间后，写入缓存层的数据将迁移到存储层，并从缓存层中清除。从概念上讲，缓存层被覆盖在后端存储层的“前面”。当Ceph client需要驻留在存储层中的数据时，cache tiering agent会在读取数据时将数据迁移到缓存层，然后将其发送到Ceph client。此后，Ceph client可以使用缓存层执行I/O，直到数据变为非活动状态。这对于易变数据（例如照片/视频编辑，交易数据等）是理想的。 Read-proxy Mode: 此模式将使用已存在于缓存层中的任何objects，但如果缓存中没有objects，则将请求代理到存储层。这对于从writeback mode转换为禁用缓存非常有用，因为它允许负载在缓存耗尽时正常运行，而无需向缓存中添加任何新objects。 A WORD OF CAUTIONCache tiering会降低大多数负载的性能。用户在使用此功能之前应特别小心 Workload dependent: 缓存是否会提高性能，高度依赖于负载。由于将objects移入或移出缓存会产生额外成本，因此只有在数据集中访问存在较大偏差时才会生效，这样大多数请求才会命中少量objects。缓存池应该大到足以捕获你的负载，以避免抖动。 Difficult to benchmark: 使用cache tiering，用户常规衡量性能的基准测试将显得很糟糕，部分原因是由于很少有人将请求集中在一小部分objects上，所以缓存“预热”可能需要很长时间，同时预热可能带来更高的成本。 Usually slower: 对于缓存分层不友好的负载，性能通常比没有设置cache tiering enabled的普通RADOS pool慢。 librados object enumeration: 在这种情况下，librados级别的object enumeration API并不一致。如果您的应用程序直接使用librados并依赖于object enumeration，则缓存分层可能无法按预期工作。 （这对于RGW，RBD或CephFS来说不是问题。） Complexity: 启用缓存分层会带来额外的复杂性。这会增加其他用户尚未遇到的错误的可能性，并且会使您的部署处于更高级别的风险中。 KNOWN GOOD WORKLOADS RGW time-skewed: 如果RGW负载几乎所有读取操作都针对最近写入的objects，可配置为一段时间后，将最近写入的对象从缓存层迁移到存储层，这种场景可以很好地工作。 KNOWN BAD WORKLOADS已知下列配置对cache tiering效果不佳。 RBD with replicated cache and erasure-coded base: 这是一个常见的要求，但通常表现不佳。即使合理偏差的负载仍然会向cold objects发送一些small writes操作，由于erasure-coded pool尚不支持small writes操作，因此必须将整个（通常为4 MB）objects迁移到缓存中以满足small write（通常为4 KB）。只有少数用户成功部署了此配置，并且仅适用于他们，因为他们的数据extremely cold（备份）并且对性能没有任何要求。 RBD with replicated cache and base: 具有replicated base tier（存储层）的RBD，效果要好于erasure coded base tier（存储层）时的RBD，但它仍高度依赖于负载中的偏差量，而且很难验证。用户需要很好地理解他们的负载，并需要调整缓存分层参数。 SETTING UP POOLS要设置cache tiering，您必须有两个pool。 一个pool充当backing storage，另一个pool充当cache。 SETTING UP A BACKING STORAGE POOL设置backing storage pool通常涉及以下两种方案之一： Standard Storage: 在这种情况下，pool在Ceph Storage Cluster中存储object的多个副本。 Erasure Coding: 在这种情况下，pool使用erasure coding来更有效地存储数据，并且性能折衷很小。 在standard storage方案中，您可以设置CRUSH rule来建立故障域（例如，osd, host, chassis, rack, row, 等）。当rule中的所有存储驱动器都具有相同的大小，速度（RPM-转速和吞吐量）和类型时，Ceph OSD Daemons会达到最佳性能。有关创建rule的详细信息，请参阅CRUSH Maps。一旦你创建了rule，就可以创建backing storage pool了。 在erasure coding方案中，pool创建参数将自动生成适当的rule。 有关详细信息，请参阅Create a Pool。 在随后的例子中，我们将backing storage pool称为cold-storage。 SETTING UP A CACHE POOL设置cache pool遵循与standard storage方案相同的过程，但有这样的区别：作为cache tier（缓存层）的驱动器通常是服务器中的高性能驱动器，并具有自己的CRUSH rule。在设置这样的rule时，应该考虑具有高性能驱动器的主机，忽略不具有高性能驱动器的主机。有关详细信息，请参阅Placing Different Pools on Different OSDs。 在后面的例子中，我们将把cache pool称为hot-storage，将backing pool称为cold-storage。 有关cache tier配置和默认值，请参阅Pools - Set Pool Values。 CREATING A CACHE TIER设置cache tier涉及到关联backing storage pool和cache pool。 1ceph osd tier add &#123;storagepool&#125; &#123;cachepool&#125; 例如： 1ceph osd tier add cold-storage hot-storage 要设置cache模式，请执行以下操作： 1ceph osd tier cache-mode &#123;cachepool&#125; &#123;cache-mode&#125; 例如： 1ceph osd tier cache-mode hot-storage writeback cache tiers覆盖backing storage tier，他们需要一个额外的步骤：您必须将所有客户端流量从storage pool引导到cache pool。要将客户端流量直接指向cache pool，请执行以下操作： 1ceph osd tier set-overlay &#123;storagepool&#125; &#123;cachepool&#125; 例如： 1ceph osd tier set-overlay cold-storage hot-storage CONFIGURING A CACHE TIERCache tiers有几个配置选项。您可以使用以下方法来设置cache tier配置选项： 1ceph osd pool set &#123;cachepool&#125; &#123;key&#125; &#123;value&#125; 有关详细信息，请参阅Pools - Set Pool Values。 TARGET SIZE AND TYPECeph的cache tiers对hit_set_type使用Bloom Filter： 1ceph osd pool set &#123;cachepool&#125; hit_set_type bloom 例如： 1ceph osd pool set hot-storage hit_set_type bloom hit_set_count和hit_set_period限定要存储多少个这样的HitSet，以及每个HitSet应该cover多长时间。 123ceph osd pool set &#123;cachepool&#125; hit_set_count 12ceph osd pool set &#123;cachepool&#125; hit_set_period 14400ceph osd pool set &#123;cachepool&#125; target_max_bytes 1000000000000 1注意：更大的hit_set_count会导致ceph-osd进程消耗更多的内存。 分级访问，随着时间的推移，可以让Ceph确定是否一个Ceph client访问某个object至少一次，或者在一段时间内（“age” vs “temperature”）超过一次。 min_read_recency_for_promote决定在处理读取操作时，有多少HitSet检查object的存在。检查结果用于决定是否异步提升object。它的值应该在0到hit_set_count之间。如果它设置为0，则该object总是被提升。如果它设置为1，则检查当前的HitSet。如果这个object在当前的HitSet中，它会被提升。否则不是。对于其他值，将检查存档HitSet的确切数量。如果在最近的min_read_recency_for_promote HitSets中找到该object，则会提升该object。 可以为写入操作设置一个类似的参数，即min_write_recency_for_promote。 12ceph osd pool set &#123;cachepool&#125; min_read_recency_for_promote 2ceph osd pool set &#123;cachepool&#125; min_write_recency_for_promote 2 1注意：时间越长，min_read_recency_for_promote和min_write_recency_for_promote`value值越高，ceph-osd daemon消耗的内存就越多。特别是，当agent处于active状态刷新或逐出缓存objects时，所有hit_set_count HitSets都将加载到内存中。 CACHE SIZINGcache tiering agent执行两个主要功能： Flushing: agent识别修改的（或脏的）objects，并将它们转发到storage pool以进行长期存储。 Evicting: agent标识未被修改（或干净的）的objects，并从缓存中逐出其中不常用的objects。 ABSOLUTE SIZINGcache tiering agent可以基于字节总数或objects总数来flush或者evict objects。要指定最大字节数，请执行以下操作： 1ceph osd pool set &#123;cachepool&#125; target_max_bytes &#123;#bytes&#125; 例如，以1TB flush或evict，请执行以下操作： 1ceph osd pool set hot-storage target_max_bytes 1099511627776 要指定objects的最大数量，请执行以下操作： 1ceph osd pool set &#123;cachepool&#125; target_max_objects &#123;#objects&#125; 例如，要flush或evict 1M个对象，请执行以下操作： 1ceph osd pool set hot-storage target_max_objects 1000000 1注意：Ceph无法自动确定cache pool的大小，因此在此需要配置一个绝对值大小，否则flush/evict将不起作用。 如果您指定了这两个限制，则在触发阈值时，cache tiering agent将开始flushing或evicting。 1注意：只有在达到target_max_bytes或target_max_objects时，所有client请求才会被blocked RELATIVE SIZINGcache tiering agent可以根据cache pool大小flush或evict objects（由target_max_bytes / target_max_objects指定绝对大小）。当cache pool由（有）一定比例修改过的（或脏）objects组成时，cache tiering agent将其flush到storage pool。要设置cache_target_dirty_ratio，请执行以下操作： 1ceph osd pool set &#123;cachepool&#125; cache_target_dirty_ratio &#123;0.0..1.0&#125; 例如，将值设置为0.4将在修改过的（脏的）objects达到cache pool的容量40％时开始flushing： 1ceph osd pool set hot-storage cache_target_dirty_ratio 0.4 当脏objects达到其容量的一定比例时，以较高的速度flush脏objects。要设置cache_target_dirty_high_ratio： 1ceph osd pool set &#123;cachepool&#125; cache_target_dirty_high_ratio &#123;0.0..1.0&#125; 例如，将值设置为0.6将在脏objects达到cache pool容量的60％时开始大量flush脏objects。显然，我们最好设置dirty_ratio和full_ratio之间的值： 1ceph osd pool set hot-storage cache_target_dirty_high_ratio 0.6 当cache pool达到其容量的一定比例时，cache tiering agent将驱逐objects以保持可用容量。要设置cache_target_full_ratio，请执行以下操作： 1ceph osd pool set &#123;cachepool&#125; cache_target_full_ratio &#123;0.0..1.0&#125; 例如，将值设置为0.8，当它们达到cache pool容量的80％时，开始flushing未修改的（干净）objects： 1ceph osd pool set hot-storage cache_target_full_ratio 0.8 CACHE AGE您可以在cache tiering agent将最近修改的（或脏的）object刷新到backing storage pool之前指定object的最小age： 1ceph osd pool set &#123;cachepool&#125; cache_min_flush_age &#123;#seconds&#125; 例如，要在10分钟后flush修改的（或脏的）objects，请执行以下操作： 1ceph osd pool set hot-storage cache_min_flush_age 600 您可以指定object在缓存层被evict之前的最小age： 1ceph osd pool &#123;cache-tier&#125; cache_min_evict_age &#123;#seconds&#125; 例如，要在30分钟后evict object，请执行以下操作： 1ceph osd pool set hot-storage cache_min_evict_age 1800 REMOVING A CACHE TIER删除cache tier取决于它是writeback cache还是read-only cache。 REMOVING A READ-ONLY CACHE由于read-only cache中没有修改过的（脏的）数据，因此可以直接禁用并删除它，在缓存中objects的最新更改不会丢失。 1、将cache-mode更改为none以禁用它。 1ceph osd tier cache-mode &#123;cachepool&#125; none 例如： 1ceph osd tier cache-mode hot-storage none 2、从backing pool上移除cache pool。 1ceph osd tier remove &#123;storagepool&#125; &#123;cachepool&#125; 例如： 1ceph osd tier remove cold-storage hot-storage REMOVING A WRITEBACK CACHE由于writeback cache中可能修改了数据，因此必须采取措施，确保在禁用和删除缓存中的objects之前不会丢失任何最新更改。 1、将cache mode更改为forward，以便最新的和修改的objects将flush到backing storage pool。 1ceph osd tier cache-mode &#123;cachepool&#125; forward 例如： 1ceph osd tier cache-mode hot-storage forward 2、确保cache pool已被flush。 这可能需要几分钟的时间： 1rados -p &#123;cachepool&#125; ls 如果cache pool仍有objects，则可以手动flush它们。 例如： 1rados -p &#123;cachepool&#125; cache-flush-evict-all 3、删除overlay，以便client不会将流量导向缓存。 1ceph osd tier remove-overlay &#123;storagetier&#125; 例如： 1ceph osd tier remove-overlay cold-storage 4、最后，从backing storage pool上移除cache tier pool。 1ceph osd tier remove &#123;storagepool&#125; &#123;cachepool&#125; 例如： 1ceph osd tier remove cold-storage hot-storage]]></content>
      <tags>
        <tag>CEPH-Luminous</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CEPH ERASURE CODE]]></title>
    <url>%2F2018%2F05%2F12%2FCEPH-ERASURE-CODE%2F</url>
    <content type="text"><![CDATA[为了承受OSD丢失，一个Ceph pool与一个类型相关联（大多数情况下每个磁盘有一个OSD）。当创建pool时，默认选择replicate，这意味着每个object都被复制到多个磁盘上。可以使用Erasure Code pool类型替代replicate，以节省空间。 CREATING A SAMPLE ERASURE CODED POOL最简单的erasure coded pool相当于RAID5，并且至少需要三台主机： 12345$ ceph osd pool create ecpool 12 12 erasurepool 'ecpool' created$ echo ABCDEFGHI | rados --pool ecpool put NYAN -$ rados --pool ecpool get NYAN -ABCDEFGHI 请注意，pool 中的12表示placement groups的数量。 ERASURE CODE PROFILES默认的erasure code profile会承受丢失1个OSD。它相当于一个大小为2的replicated pool，但需要1.5TB而不是2TB来存储1TB的数据。默认profile显示： 123456$ ceph osd erasure-code-profile get defaultk=2m=1plugin=jerasurecrush-failure-domain=hosttechnique=reed_sol_van 选择正确的profile非常重要，因为在创建pool后无法对其进行修改：需要创建具有不同profile的新pool，并将先前pool中的所有objects都移动到新的pool中。 profile最重要的参数是K，M和crush-failure-domain，因为它们定义了存储开销和数据持久性。例如，期望在系统架构中必须承受两个racks间的损失，并且控制额外增加40％存储开销，则可以定义以下profile： 12345678$ ceph osd erasure-code-profile set myprofile \ k=3 \ m=2 \ crush-failure-domain=rack$ ceph osd pool create ecpool 12 12 erasure myprofile$ echo ABCDEFGHI | rados --pool ecpool put NYAN -$ rados --pool ecpool get NYAN -ABCDEFGHI NYAN object将被分成三部分（K = 3），并创建两个附加chunks（M = 2）。M的值定义了可以同时丢失多少OSD而不损失任何数据。crush-failure-domain = rack将创建一个CRUSH rule，以确保没有两个chunks存储在同一个rack中。 更多信息可以在erasure code profiles documentation中找到。 ERASURE CODING WITH OVERWRITES默认情况下，erasure coded pools仅适用于像RGW那样，完整object writes和appends的场景。Luminous版本，可以为每个pool设置启用erasure coded pool的partial writes。 这让RBD和CephFS可以将他们的数据存储在erasure coded pool中： 1ceph osd pool set ec_pool allow_ec_overwrites true 这只能对bluestore OSD上的pool启用，因为bluestore的checksumming用于在deep-scrub过程中检查bitrot或其他corruption。除了不安全之外，使用filestore进行ec overwrites，相对于bluestore性能低。 Erasure coded pools不支持omap（ObjectMap），因此要将它们与RBD和CephFS一起使用，必须明确指示它们将data存储在ec pool中，将metadata存储在replicated pool中。对于RBD，这意味着在创建image时，需要使用–data-pool指定erasure coded pool： 1rbd create --size 1G --data-pool ec_pool replicated_pool/image_name 对于CephFS，在创建file system时或者通过file layouts设定一个erasure coded pool作为默认的data pool。 ERASURE CODED POOL AND CACHE TIERINGErasure coded pools需要比replicated pools更多的资源，并且缺少一些功能，如omap。为了克服这些限制，可以在erasure coded pool之前设置一个cache tier。 例如，hot-storage pool由fast storage设备组成： 123$ ceph osd tier add ecpool hot-storage$ ceph osd tier cache-mode hot-storage writeback$ ceph osd tier set-overlay ecpool hot-storage hot-storage pool作为ecpool的tier，将数据以writeback模式回写至ecpool，所以每次write和read这个ecpool的时候，实际使用的是hot-storage。 更多信息可以在cache tiering documentation中找到。 GLOSSARYchunk 当encoding function被调用时，它返回相同大小的chunks。Data chunks可以连接起来重建original object和coding chunks，这些chunks可以用来重建丢失的chunk。 K data chunks的数量，即original object被划分成chunks的数量。例如，如果K = 2，10KB objects分为每个5KB的K个objects。 M coding chunks的数量，即由encoding functions计算的附加chunks的数量。如果有2个coding chunks，这意味着2个OSD可以在不丢失数据的情况下out。 TABLE OF CONTENTErasure code profilesJerasure erasure code pluginISA erasure code pluginLocally repairable erasure code pluginSHEC erasure code plugin 附录 ObjectMap参考链接： 【1】http://bean-li.github.io/ceph-omap 【2】https://my.oschina.net/u/2460844/blog/604530 【3】http://www.wzxue.com/ceph-filestore ceph中所有块设备、对象存储、文件存储最后都要转化为对象（object），这个object包含3个元素：data、xattr、omap。 data是保存对象的数据。 xattr是保存对象的扩展属性，每个对象文件都可以设置文件的属性，这个属性是一个key/value值对，但是受到文件系统的限制，key/value对的个数和每个value的大小都进行了限制。 如果要设置的对象的key/value不能存储在文件的扩展属性中，还存在另外一种方式保存omap，omap实际上是保存到了key/vaule 值对的数据库levelDB（L版是RocksDB）中，在这里value的值限制要比xattr中好的多。]]></content>
      <tags>
        <tag>CEPH-Luminous</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iozone使用]]></title>
    <url>%2F2018%2F05%2F09%2Fiozone%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[yum安装方法1、安装 1yum install -y iozone 2、参数 1iozone -l 1 -u 1 -r 16k -s 64g -F -l是最小进程数量lower -u是最大进程数量upper -r是读写的基本单位，16k作为读写的基本单位，根据模拟应用程序进行合理设置（目的是模拟真实应用） -s指定默认读写的大小，建议不要指定的太小，一般指定的是内存的2倍 -F指定测试文件位置，可以是多个 编译安装方法123456789101112yum install gccwget http://www.iozone.org/src/current/iozone3_471.tartar xvf iozone3_471.tarcd iozone3_471/src/current/ARM平台：make linux-armX86平台：make linux-AMD64cp iozone /usr/bin/./iozone -+u -+d -+p -+t -z -l 1 -u 1 -r 4k -s 1G -F /home/node-3/1-FILE]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go并发编程案例解析]]></title>
    <url>%2F2018%2F05%2F01%2FGo%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E6%A1%88%E4%BE%8B%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Go并发编程案例解析学习笔记 课程地址：https://www.imooc.com/video/17021 课程教师：麦可同学 课程介绍 并发编程基础知识介绍 日志监控系统实战 课程总结 准备知识 有一定的编程基础 了解Golang基本语法 有并发编程经验就更好了 常见并发模型讲解 Golang并发实现 程序并发执行（goroutine） 多个goroutine间的数据同步与通信（channels） 多个channels选择数据读取或者写入（select） Goroutines（程序并发执行）1234foo() //执行函数foo，程序等待函数foo返回go foo() //执行函数foobar() //不用等待foo返回 Channels（多个goroutines间的数据同步与通信）123456c := make(chan string) //创建一个channelgo func()&#123; time.Sleep(1 * time.Second) c &lt;- &quot;message from closure&quot; //发送数据到channel中&#125;() //这个()表示调用该函数msg := &lt;-c //阻塞直到接收到数据 Select（从多个channel中读取或写入数据）12345678select &#123; case v := &lt;-c1: fmt.Println(&quot;channel 1 sends&quot;, v) case v := &lt;-c2: fmt.Println(&quot;channel 2 sends&quot;, v&quot;) default: //可选 fmt.Println(&quot;neither channel was ready&quot;)&#125; 并发拓展：并发与并行定义： 并发：指同一时刻，系统通过调度，来回切换交替的运行多个任务，“看起来”是同时进行 并行：指同一时刻，两个任务“真正的”同时进行 可以把《实时读取》《解析》《写入》拆成多个模块，使用多个goroutine。那么这么多个goroutine是并行执行还是并发执行呢？换句话说，多个goroutine执行，是一个CPU核心通过不断的切换时间片，并发的执行？还是将goroutine分散到多核的CPU并行的执行？这个问题Golang为我们屏蔽掉了，编程人员不需要考虑这个问题。 总结 将复杂的任务拆分，通过goroutine去并发执行 通过channel做数据的同步与通信 Golang中的面向对象Golang中没有类和对象的概念，但是支持 struct interface 传统的面向对象中，继承、封装、多态都可以基于这两个特性来实现。 封装123456789101112131415// 结构体，baz相当于成员变量，可以看做是类type Foo struct &#123; baz string&#125;// 接收者Receiver，接收之后，就可以使用结构体中的字段了，相当于成员函数func (f *Foo) echo()&#123; fmt.Println(f.baz)&#125;// 在main中初始化结构体，相当于实例化一个类，然后调用成员方法func main()&#123; f := Foo&#123;baz: "hello, struct"&#125; f.echo()&#125; 继承（个人认为这是设计模式中的组合）1234567891011121314151617181920// Foo结构体type Foo struct &#123; baz string&#125;// 接收者Receiver，Foo成员函数func (f *Foo) echo() &#123; fmt.Println(f.baz)&#125;// 在Bar结构体中直接把Foo结构体写进来，这个叫匿名字段。这样写之后，Bar结构体就拥有了Foo结构体的所有特性type Bar struct &#123; Foo&#125;// 在main函数中初始化Bar结构体，然后直接调用echo()方法，echo()方法其实是Foo中的成员函数func main() &#123; b := Bar&#123;Foo&#123;baz: "hello, struct"&#125;&#125; b.echo()&#125; 多态1234567891011121314151617181920// 定义一个interface，可以看成是一组方法的集合，通过interface定义一些对象的行为type Foo interface &#123; qux()&#125;// 定义Bar和Baz两个结构体type Bar struct &#123;&#125;type Baz struct &#123;&#125;// Bar和Baz两个结构体分别实现了qux()函数，这样就可以说Bar和Baz都是Foo这种类型了。这里并没有显示的说我实现了Foo这个接口，只要结构体中实现了qux()这个函数，就认为它实现了这个接口，这就是所谓的非侵入式接口func (b Bar) qux() &#123;&#125;func (b Baz) qux() &#123;&#125;// 在main中，定义了f的变量，它的类型是Foo。无论是Bar还是Baz都可以赋值给f这个变量，这也证明了Bar&#123;&#125;和Baz&#123;&#125;是同一类型。可以说类型相同，实现不同func main() &#123; var f Foo f = Bar&#123;&#125; f = Baz&#123;&#125; fmt.Println(f)&#125; 日志监控程序的实现日志分析系统实战123456789101112131415161718192021222324252627282930313233343536373839404142434445package mainimport ( "strings" "fmt" "time")type LogProcess struct &#123; rc chan string wc chan string path string // 读取文件的路径 influxDBDsn string // influx data source&#125;func (l *LogProcess) ReadFromFile() &#123; // 需要代码优化，只能从文件中读取数据 // 读取模块 line := "message" l.rc &lt;- line&#125;func (l *LogProcess) Process() &#123; // 解析模块 data := &lt;- l.rc l.wc &lt;- strings.ToUpper(data)&#125;func (l *LogProcess) WriteToInfluxDB() &#123; // 需要代码优化，只能向influxDB中写入数据 // 写入模块 fmt.Println(&lt;-l.wc)&#125;func main() &#123; lp := LogProcess&#123; rc : make(chan string), wc : make(chan string), path: "/tmp/access.log", influxDBDsn: "", &#125; go lp.ReadFromFile() go lp.Process() go lp.WriteToInfluxDB() time.Sleep(time.Second)&#125; 代码优化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package mainimport ( "strings" "fmt" "time")// 定义接口，抽象读取模块type Reader interface &#123; Read(rc chan string)&#125;type ReadFromFile struct &#123; path string // 读取文件的路径&#125;func (l *ReadFromFile) Read(rc chan string) &#123; // 读取模块 line := "message" rc &lt;- line&#125;// 定义接口，抽象写入模块type Writer interface &#123; Write(wc chan string)&#125;type WriteToInfluxDB struct &#123; influxDBDsn string // influx data source&#125;func (l *WriteToInfluxDB) Write(wc chan string) &#123; // 写入模块 fmt.Println(&lt;-wc)&#125;type LogProcess struct &#123; rc chan string wc chan string read Reader write Writer&#125;func (l *LogProcess) Process() &#123; // 解析模块 data := &lt;- l.rc l.wc &lt;- strings.ToUpper(data)&#125;func main() &#123; r := &amp;ReadFromFile&#123; path: "/tmp/access.log", &#125; w := &amp;WriteToInfluxDB&#123; influxDBDsn: "", &#125; lp := LogProcess&#123; rc : make(chan string), wc : make(chan string), read: r, write: w, &#125; go lp.read.Read(lp.rc) go lp.Process() go lp.write.Write(lp.wc) time.Sleep(time.Second)&#125; 读取模块实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798package mainimport ( "strings" "fmt" "time" "os" "bufio" "io")// 定义两个接口，抽象读取模块和写入模块type Reader interface &#123; Read(rc chan []byte)&#125;type ReadFromFile struct &#123; path string // 读取文件的路径&#125;func (l *ReadFromFile) Read(rc chan []byte) &#123; // 读取模块 // 1. 打开文件 f,err := os.Open(l.path) if err != nil &#123; panic(fmt.Sprintf("open file error : %s \n", err)) &#125; // 2. 从文件末尾开始逐行读取文件内容 f.Seek(0,2) rd := bufio.NewReader(f) // 3. 写入到Read Channel中 for &#123; line, err := rd.ReadBytes('\n') if err == io.EOF &#123; time.Sleep(500 * time.Millisecond) continue &#125; else if err != nil &#123; panic(fmt.Sprintf("ReadBytes error : %s", err.Error())) &#125; rc &lt;- line[:len(line) - 1] &#125;&#125;type Writer interface &#123; Write(wc chan string)&#125;type WriteToInfluxDB struct &#123; influxDBDsn string // influx data source&#125;func (l *WriteToInfluxDB) Write(wc chan string) &#123; // 写入模块 for v := range wc&#123; fmt.Println(v) &#125;&#125;type LogProcess struct &#123; rc chan []byte wc chan string read Reader write Writer&#125;func (l *LogProcess) Process() &#123; // 解析模块 for v := range l.rc &#123; l.wc &lt;- strings.ToUpper(string(v)) &#125;&#125;func main() &#123; r := &amp;ReadFromFile&#123; path: "/tmp/access.log", &#125; w := &amp;WriteToInfluxDB&#123; influxDBDsn: "", &#125; lp := LogProcess&#123; rc : make(chan []byte), wc : make(chan string), read: r, write: w, &#125; go lp.read.Read(lp.rc) go lp.Process() go lp.write.Write(lp.wc) time.Sleep(60 * time.Second)&#125; 解析模块的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156package mainimport ( "strings" "fmt" "time" "os" "bufio" "io" "regexp" "log" "strconv" "net/url")// 定义两个接口，抽象读取模块和写入模块type Reader interface &#123; Read(rc chan []byte)&#125;type ReadFromFile struct &#123; path string // 读取文件的路径&#125;func (l *ReadFromFile) Read(rc chan []byte) &#123; // 读取模块 // 1. 打开文件 f,err := os.Open(l.path) if err != nil &#123; panic(fmt.Sprintf("open file error : %s \n", err)) &#125; // 2. 从文件末尾开始逐行读取文件内容 f.Seek(0,2) rd := bufio.NewReader(f) // 3. 写入到Read Channel中 for &#123; line, err := rd.ReadBytes('\n') if err == io.EOF &#123; time.Sleep(500 * time.Millisecond) continue &#125; else if err != nil &#123; panic(fmt.Sprintf("ReadBytes error : %s", err.Error())) &#125; rc &lt;- line[:len(line) - 1] &#125;&#125;type Writer interface &#123; Write(wc chan *Message)&#125;type WriteToInfluxDB struct &#123; influxDBDsn string // influx data source&#125;func (l *WriteToInfluxDB) Write(wc chan *Message) &#123; // 写入模块 for v := range wc&#123; fmt.Println(v) &#125;&#125;type LogProcess struct &#123; rc chan []byte wc chan *Message read Reader write Writer&#125;type Message struct &#123; TimeLocal time.Time ByteSent int Path, Method, Scheme, Status string UpstreamTime, RequestTime float64&#125;func (l *LogProcess) Process() &#123; // 解析模块 // 172.0.0.12 - - [04/Mar/2018:13:49:52 +0000] http "GET /foo?query=t HTTP:1.0" 200 2133 "-" "KeepAliveClient" "-" 1.005 1.854 r := regexp.MustCompile(`([\d\.]+)\s+([^ \[]+)\s+([^ \[]+)\s+\[([^\]]+)\]\s+([a-z]+)\s+\"([^"]+)\"\s+(\d&#123;3&#125;)\s+(\d+)\s+\"([^"]+)\"\s+\"(.*?)\"\s+\"([\d\.-]+)\"\s+([\d\.-]+)\s+([\d\.-]+)`) loc , _ := time.LoadLocation("Asia/Shanghai") // 1. 从Read Channel中读取每行日志数据 for v := range l.rc &#123; // 2. 正则提取所需的监控数据（path，status，method等） ret := r.FindStringSubmatch(string(v)) if len(ret) != 14 &#123; log.Println("FindStringSubmatch failed : ", string(v)) continue &#125; message := &amp;Message&#123;&#125; t, err := time.ParseInLocation("02/Jan/2006:15:04:05 +0000", ret[4], loc) if err != nil &#123; log.Println("ParseInLocation failed : ", err.Error(), ret[4]) &#125; message.TimeLocal = t byteSent, _ := strconv.Atoi(ret[8]) message.ByteSent = byteSent reqSli := strings.Split(ret[6]," ") if len(reqSli) != 3 &#123; log.Println("string.Split failed : ", ret[6]) continue &#125; message.Method = reqSli[0] u, err := url.Parse(reqSli[1]) if err != nil &#123; log.Println("url parse failed : ", err) continue &#125; message.Path = u.Path message.Scheme = ret[5] message.Status = ret[7] upstreamTime, err := strconv.ParseFloat(ret[12],64) message.UpstreamTime = upstreamTime requestTime, err := strconv.ParseFloat(ret[13], 64) message.RequestTime = requestTime // 3. 写入Write Channel l.wc &lt;- message &#125;&#125;func main() &#123; r := &amp;ReadFromFile&#123; path: "/tmp/access.log", &#125; w := &amp;WriteToInfluxDB&#123; influxDBDsn: "", &#125; lp := LogProcess&#123; rc : make(chan []byte), wc : make(chan *Message), read: r, write: w, &#125; go lp.read.Read(lp.rc) go lp.Process() go lp.write.Write(lp.wc) time.Sleep(60 * time.Second)&#125; 写入模块流程讲解 在URL中，db=mydb指定database。使用逗号做分隔’,’，cpu_usage表示measurement。host=server01,region=us-west value=0.64 1434055562000000000表示points。host=server01,region=us-west表示points中的tags，value=0.64表示points中的fields，1434055562000000000表示points中的time。 influxDB提供了Golang的客户端，可以使用这个客户端很方便的写入数据https://github.com/influxdata/influxdb/tree/master/client，首先先引入包&quot;github.com/influxdata/influxdb/client/v2&quot; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216package mainimport ( "strings" "fmt" "time" "os" "bufio" "io" "regexp" "log" "strconv" "net/url" "github.com/influxdata/influxdb/client/v2" "flag")// 定义两个接口，抽象读取模块和写入模块type Reader interface &#123; Read(rc chan []byte)&#125;type ReadFromFile struct &#123; path string // 读取文件的路径&#125;func (l *ReadFromFile) Read(rc chan []byte) &#123; // 读取模块 // 1. 打开文件 f,err := os.Open(l.path) if err != nil &#123; panic(fmt.Sprintf("open file error : %s \n", err)) &#125; // 2. 从文件末尾开始逐行读取文件内容 f.Seek(0,2) rd := bufio.NewReader(f) // 3. 写入到Read Channel中 for &#123; line, err := rd.ReadBytes('\n') if err == io.EOF &#123; time.Sleep(500 * time.Millisecond) continue &#125; else if err != nil &#123; panic(fmt.Sprintf("ReadBytes error : %s", err.Error())) &#125; rc &lt;- line[:len(line) - 1] &#125;&#125;type Writer interface &#123; Write(wc chan *Message)&#125;type WriteToInfluxDB struct &#123; influxDBDsn string // influx data source&#125;func (l *WriteToInfluxDB) Write(wc chan *Message) &#123; // 写入模块 // 1.初始化influxDB client // 2.从Write Channel中读取监控数据 // 3.构造数据并写入influxDB infSli := strings.Split(l.influxDBDsn, "@") // Create a new HTTPClient c, err := client.NewHTTPClient(client.HTTPConfig&#123; Addr: infSli[0], Username: infSli[1], Password: infSli[2], &#125;) if err != nil &#123; log.Fatal(err) &#125; for v := range wc&#123; fmt.Println(v) defer c.Close() // Create a new point batch bp, err := client.NewBatchPoints(client.BatchPointsConfig&#123; Database: infSli[3], Precision: infSli[4], &#125;) if err != nil &#123; log.Fatal(err) &#125; // Create a point and add to batch tags := map[string]string&#123;"Path": v.Path, "Method": v.Method, "Scheme": v.Scheme, "Status": v.Status&#125; fields := map[string]interface&#123;&#125;&#123; "UpstreamTime": v.UpstreamTime, "RequestTime": v.RequestTime, "BytesSent": v.ByteSent, &#125; pt, err := client.NewPoint("nginx_log", tags, fields, v.TimeLocal) if err != nil &#123; log.Fatal(err) &#125; bp.AddPoint(pt) // Write the batch if err := c.Write(bp); err != nil &#123; log.Fatal(err) &#125; // Close client resources if err := c.Close(); err != nil &#123; log.Fatal(err) &#125; fmt.Println("write success") &#125;&#125;type LogProcess struct &#123; rc chan []byte wc chan *Message read Reader write Writer&#125;type Message struct &#123; TimeLocal time.Time ByteSent int Path, Method, Scheme, Status string UpstreamTime, RequestTime float64&#125;func (l *LogProcess) Process() &#123; // 解析模块 // 172.0.0.12 - - [04/Mar/2018:13:49:52 +0000] http "GET /foo?query=t HTTP:1.0" 200 2133 "-" "KeepAliveClient" "-" 1.005 1.854 r := regexp.MustCompile(`([\d\.]+)\s+([^ \[]+)\s+([^ \[]+)\s+\[([^\]]+)\]\s+([a-z]+)\s+\"([^"]+)\"\s+(\d&#123;3&#125;)\s+(\d+)\s+\"([^"]+)\"\s+\"(.*?)\"\s+\"([\d\.-]+)\"\s+([\d\.-]+)\s+([\d\.-]+)`) loc , _ := time.LoadLocation("Asia/Shanghai") // 1. 从Read Channel中读取每行日志数据 for v := range l.rc &#123; // 2. 正则提取所需的监控数据（path，status，method等） ret := r.FindStringSubmatch(string(v)) if len(ret) != 14 &#123; log.Println("FindStringSubmatch failed : ", string(v)) continue &#125; message := &amp;Message&#123;&#125; t, err := time.ParseInLocation("02/Jan/2006:15:04:05 +0000", ret[4], loc) if err != nil &#123; log.Println("ParseInLocation failed : ", err.Error(), ret[4]) &#125; message.TimeLocal = t byteSent, _ := strconv.Atoi(ret[8]) message.ByteSent = byteSent reqSli := strings.Split(ret[6]," ") if len(reqSli) != 3 &#123; log.Println("string.Split failed : ", ret[6]) continue &#125; message.Method = reqSli[0] u, err := url.Parse(reqSli[1]) if err != nil &#123; log.Println("url parse failed : ", err) continue &#125; message.Path = u.Path message.Scheme = ret[5] message.Status = ret[7] upstreamTime, err := strconv.ParseFloat(ret[12],64) message.UpstreamTime = upstreamTime requestTime, err := strconv.ParseFloat(ret[13], 64) message.RequestTime = requestTime // 3. 写入Write Channel l.wc &lt;- message &#125;&#125;func main() &#123; var path, influxDsn string flag.StringVar(&amp;path, "path", "/tmp/access.log", "read file path") flag.StringVar(&amp;influxDsn, "influxDsn", "http://127.0.0.1:8086@imooc@imoocpass@nginx@s", "influx data source") flag.Parse() r := &amp;ReadFromFile&#123; path: path, &#125; w := &amp;WriteToInfluxDB&#123; influxDBDsn: influxDsn, &#125; lp := LogProcess&#123; rc : make(chan []byte), wc : make(chan *Message), read: r, write: w, &#125; go lp.read.Read(lp.rc) go lp.Process() go lp.write.Write(lp.wc) time.Sleep(6000 * time.Second)&#125; 监控模块的实现1、总处理日志行数 2、系统吞吐量 3、read channel长度 4、write channel长度 5、运行总时间 6、错误数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291package mainimport ( "strings" "fmt" "time" "os" "bufio" "io" "regexp" "log" "strconv" "net/url" "github.com/influxdata/influxdb/client/v2" "flag" "net/http" "encoding/json")// 定义两个接口，抽象读取模块和写入模块type Reader interface &#123; Read(rc chan []byte)&#125;type ReadFromFile struct &#123; path string // 读取文件的路径&#125;func (l *ReadFromFile) Read(rc chan []byte) &#123; // 读取模块 // 1. 打开文件 f,err := os.Open(l.path) if err != nil &#123; panic(fmt.Sprintf("open file error : %s \n", err)) &#125; // 2. 从文件末尾开始逐行读取文件内容 f.Seek(0,2) rd := bufio.NewReader(f) // 3. 写入到Read Channel中 for &#123; line, err := rd.ReadBytes('\n') if err == io.EOF &#123; time.Sleep(500 * time.Millisecond) continue &#125; else if err != nil &#123; panic(fmt.Sprintf("ReadBytes error : %s", err.Error())) &#125; TypeMonitorChan &lt;- TypeHandleLine rc &lt;- line[:len(line) - 1] &#125;&#125;type Writer interface &#123; Write(wc chan *Message)&#125;type WriteToInfluxDB struct &#123; influxDBDsn string // influx data source&#125;func (l *WriteToInfluxDB) Write(wc chan *Message) &#123; // 写入模块 // 1.初始化influxDB client // 2.从Write Channel中读取监控数据 // 3.构造数据并写入influxDB infSli := strings.Split(l.influxDBDsn, "@") // Create a new HTTPClient c, err := client.NewHTTPClient(client.HTTPConfig&#123; Addr: infSli[0], Username: infSli[1], Password: infSli[2], &#125;) if err != nil &#123; log.Fatal(err) &#125; for v := range wc&#123; fmt.Println(v) defer c.Close() // Create a new point batch bp, err := client.NewBatchPoints(client.BatchPointsConfig&#123; Database: infSli[3], Precision: infSli[4], &#125;) if err != nil &#123; log.Fatal(err) &#125; // Create a point and add to batch tags := map[string]string&#123;"Path": v.Path, "Method": v.Method, "Scheme": v.Scheme, "Status": v.Status&#125; fields := map[string]interface&#123;&#125;&#123; "UpstreamTime": v.UpstreamTime, "RequestTime": v.RequestTime, "BytesSent": v.ByteSent, &#125; pt, err := client.NewPoint("nginx_log", tags, fields, v.TimeLocal) if err != nil &#123; log.Fatal(err) &#125; bp.AddPoint(pt) // Write the batch if err := c.Write(bp); err != nil &#123; log.Fatal(err) &#125; // Close client resources if err := c.Close(); err != nil &#123; log.Fatal(err) &#125; fmt.Println("write success") &#125;&#125;type LogProcess struct &#123; rc chan []byte wc chan *Message read Reader write Writer&#125;type Message struct &#123; TimeLocal time.Time ByteSent int Path, Method, Scheme, Status string UpstreamTime, RequestTime float64&#125;// 系统状态监控type SystemInfo struct &#123; HandleLine int `json:"HandleLine"` // 总处理日志行数 Tps float64 `json:"tps"` // 系统吞吐量 ReadChanLen int `json:"ReadChanLen"` // read channel 长度 WriteChanLen int `json:"WriteChanLen"` // write channel 长度 RunTime string `json:"RunTime"` // 运行总时间 ErrNum int `json:"ErrNum"` // 错误数&#125;const ( TypeHandleLine = 0 TypeErrNum = 1)var TypeMonitorChan = make(chan int, 200)type Monitor struct &#123; startTime time.Time data SystemInfo tpsSli []int&#125;func (m *Monitor) start(lp *LogProcess) &#123; // 消费数据 go func() &#123; for n := range TypeMonitorChan &#123; switch n &#123; case TypeErrNum: m.data.ErrNum += 1 case TypeHandleLine: m.data.HandleLine += 1 &#125; &#125; &#125;() ticker := time.NewTicker(time.Second * 5) go func() &#123; for &#123; &lt;-ticker.C m.tpsSli = append(m.tpsSli, m.data.HandleLine) if len(m.tpsSli) &gt; 2 &#123; m.tpsSli = m.tpsSli[1:] &#125; &#125; &#125;() http.HandleFunc("/monitor", func(writer http.ResponseWriter, request *http.Request) &#123; m.data.RunTime = time.Now().Sub(m.startTime).String() m.data.WriteChanLen = len(lp.wc) m.data.ReadChanLen = len(lp.rc) if len(m.tpsSli) &gt;= 2 &#123; m.data.Tps = float64(m.tpsSli[1] - m.tpsSli[0]) / 5 &#125; ret, _ := json.MarshalIndent(m.data,"","\t") io.WriteString(writer, string(ret)) &#125;) http.ListenAndServe(":9193", nil)&#125;func (l *LogProcess) Process() &#123; // 解析模块 // 172.0.0.12 - - [04/Mar/2018:13:49:52 +0000] http "GET /foo?query=t HTTP:1.0" 200 2133 "-" "KeepAliveClient" "-" 1.005 1.854 r := regexp.MustCompile(`([\d\.]+)\s+([^ \[]+)\s+([^ \[]+)\s+\[([^\]]+)\]\s+([a-z]+)\s+\"([^"]+)\"\s+(\d&#123;3&#125;)\s+(\d+)\s+\"([^"]+)\"\s+\"(.*?)\"\s+\"([\d\.-]+)\"\s+([\d\.-]+)\s+([\d\.-]+)`) loc , _ := time.LoadLocation("Asia/Shanghai") // 1. 从Read Channel中读取每行日志数据 for v := range l.rc &#123; // 2. 正则提取所需的监控数据（path，status，method等） ret := r.FindStringSubmatch(string(v)) if len(ret) != 14 &#123; TypeMonitorChan &lt;- TypeErrNum log.Println("FindStringSubmatch failed : ", string(v)) continue &#125; message := &amp;Message&#123;&#125; t, err := time.ParseInLocation("02/Jan/2006:15:04:05 +0000", ret[4], loc) if err != nil &#123; TypeMonitorChan &lt;- TypeErrNum log.Println("ParseInLocation failed : ", err.Error(), ret[4]) continue &#125; message.TimeLocal = t byteSent, _ := strconv.Atoi(ret[8]) message.ByteSent = byteSent reqSli := strings.Split(ret[6]," ") if len(reqSli) != 3 &#123; TypeMonitorChan &lt;- TypeErrNum log.Println("string.Split failed : ", ret[6]) continue &#125; message.Method = reqSli[0] u, err := url.Parse(reqSli[1]) if err != nil &#123; TypeMonitorChan &lt;- TypeErrNum log.Println("url parse failed : ", err) continue &#125; message.Path = u.Path message.Scheme = ret[5] message.Status = ret[7] upstreamTime, err := strconv.ParseFloat(ret[12],64) message.UpstreamTime = upstreamTime requestTime, err := strconv.ParseFloat(ret[13], 64) message.RequestTime = requestTime // 3. 写入Write Channel l.wc &lt;- message &#125;&#125;func main() &#123; var path, influxDsn string flag.StringVar(&amp;path, "path", "/tmp/access.log", "read file path") flag.StringVar(&amp;influxDsn, "influxDsn", "http://127.0.0.1:8086@imooc@imoocpass@nginx@s", "influx data source") flag.Parse() r := &amp;ReadFromFile&#123; path: path, &#125; w := &amp;WriteToInfluxDB&#123; influxDBDsn: influxDsn, &#125; lp := LogProcess&#123; rc : make(chan []byte), wc : make(chan *Message), read: r, write: w, &#125; go lp.read.Read(lp.rc) go lp.Process() go lp.write.Write(lp.wc) m := &amp;Monitor&#123; startTime: time.Now(), data: SystemInfo&#123;&#125;, &#125; m.start(&amp;lp) //time.Sleep(6000 * time.Second)&#125;]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Package flag]]></title>
    <url>%2F2018%2F04%2F27%2FPackage-flag%2F</url>
    <content type="text"><![CDATA[Package flagimport “flag” Package flag实现了command-line flag解析 Usage:定义flags使用flag.String(), Bool(), Int()等。 以下声明了一个整型flag，名字是-flagname，存储在指针ip中，类型为*int。 12import &quot;flag&quot;var ip = flag.Int(&quot;flagname&quot;, 1234, &quot;help message for flagname&quot;) 如果你喜欢，你可以使用Var()函数将flag绑定到一个变量。 1234var flagvar intfunc init() &#123; flag.IntVar(&amp;flagvar, &quot;flagname&quot;, 1234, &quot;help message for flagname&quot;)&#125; 或者，您可以创建满足Value接口（指针接收器）的自定义flags 1flag.Var(&amp;flagVal, &quot;name&quot;, &quot;help message for flagname&quot;) 对于这样的flags，默认值是变量的初始值。 所有flags定义后，调用： 1flag.Parse() 将命令行解析为所定义的flags。 flags可以直接使用。如果你使用flags本身，它们都是指针；如果你绑定到变量，它们就是值。 12fmt.Println(&quot;ip has value &quot;, *ip)fmt.Println(&quot;flagvar has value &quot;, flagvar) 解析后，flags后面的参数可作为slice，flag.Args()。或者独立的一个值，flag.Arg(i)。参数索引，从0到flag.NArg() - 1。 Command line flag语法： 123-flag-flag=x-flag x // non-boolean flags only 可以使用一个或两个减号；它们是等价的。最后一种形式不允许用于boolean flags 1cmd -x * 其中*是一个Unix shell通配符，如果有一个名为0或false等文件时它将会更改。您必须使用-flag=false形式来关闭boolean flag。 flag解析在第一个non-flag参数之前（”-“是一个non-flag参数）或终止符“–”之后停止。 整数flags接受1234，0664，0x1234并且可能是负数。Boolean flags可能是： 11, 0, t, f, T, F, true, false, TRUE, FALSE, True, False Duration flags接受任何有效的time.ParseDuration。 默认的一组command-line flags由顶层函数控制。FlagSet类型允许您定义独立的flags集，例如在command-line interface中实现子命令。FlagSet的方法类似于command-line flag集的顶层函数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// These examples demonstrate more intricate uses of the flag package.package mainimport ( &quot;errors&quot; &quot;flag&quot; &quot;fmt&quot; &quot;strings&quot; &quot;time&quot;)// Example 1: A single string flag called &quot;species&quot; with default value &quot;gopher&quot;.// 示例1：一个名为“species”的字符串flag，默认值为“gopher”。var species = flag.String(&quot;species&quot;, &quot;gopher&quot;, &quot;the species we are studying&quot;)// Example 2: Two flags sharing a variable, so we can have a shorthand.// The order of initialization is undefined, so make sure both use the// same default value. They must be set up with an init function.// 示例2：两个flags共享一个变量，所以我们可以用简写。// 初始化的顺序是未定义的，所以确保两者都使用相同的默认值。 它们必须用init函数来设置。var gopherType stringfunc init() &#123; const ( defaultGopher = &quot;pocket&quot; usage = &quot;the variety of gopher&quot; ) flag.StringVar(&amp;gopherType, &quot;gopher_type&quot;, defaultGopher, usage) flag.StringVar(&amp;gopherType, &quot;g&quot;, defaultGopher, usage+&quot; (shorthand)&quot;)&#125;// Example 3: A user-defined flag type, a slice of durations.type interval []time.Duration// String is the method to format the flag&apos;s value, part of the flag.Value interface.// The String method&apos;s output will be used in diagnostics.func (i *interval) String() string &#123; return fmt.Sprint(*i)&#125;// Set is the method to set the flag value, part of the flag.Value interface.// Set&apos;s argument is a string to be parsed to set the flag.// It&apos;s a comma-separated list, so we split it.func (i *interval) Set(value string) error &#123; // If we wanted to allow the flag to be set multiple times, // accumulating values, we would delete this if statement. // That would permit usages such as // -deltaT 10s -deltaT 15s // and other combinations. if len(*i) &gt; 0 &#123; return errors.New(&quot;interval flag already set&quot;) &#125; for _, dt := range strings.Split(value, &quot;,&quot;) &#123; duration, err := time.ParseDuration(dt) if err != nil &#123; return err &#125; *i = append(*i, duration) &#125; return nil&#125;// Define a flag to accumulate durations. Because it has a special type,// we need to use the Var function and therefore create the flag during// init.var intervalFlag intervalfunc init() &#123; // Tie the command-line flag to the intervalFlag variable and // set a usage message. flag.Var(&amp;intervalFlag, &quot;deltaT&quot;, &quot;comma-separated list of intervals to use between events&quot;)&#125;func main() &#123; // All the interesting pieces are with the variables declared above, but // to enable the flag package to see the flags defined there, one must // execute, typically at the start of main (not init!): // flag.Parse() // We don&apos;t run it here because this is not a main function and // the testing suite has already parsed the flags.&#125; 原文：https://golang.org/pkg/flag/]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用gopkg.in管理import的第三方package]]></title>
    <url>%2F2018%2F04%2F18%2F%E4%BD%BF%E7%94%A8gopkg-in%E7%AE%A1%E7%90%86import%E7%9A%84%E7%AC%AC%E4%B8%89%E6%96%B9package%2F</url>
    <content type="text"><![CDATA[在近期项目中，想针对Prometheus的ceph_exporter做定制化修改，但是遇到了一些麻烦。 使用go get github.com/digitalocean/ceph_exporter 命令down下来的代码和GitHub上的master分支或tag中的代码都不一样，这是为什么呢？如果go get指向目标Repository的master分支，那么master分支怎样保证任何时刻都是可用的呢？于是带着疑问开始寻找答案。 package管理工具当你在代码中引用package时，通常使用看起来像URL的导入路径，例如github.com/jpoehls/gophermail。当使用go build构建代码时，Go工具使用此路径在GOPATH中查找这些软件package，如果找不到，则失败。 如何拉取这些package？ 121、手动下载。你可以使用git clone这些package到你的GOPATH中。2、使用go get命令( download and install packages and dependencies )。go get只是简单地将导入路径作为URL来对待，并尝试通过HTTP或HTTPS来下载它。它足够聪明，可以处理Git，Mercurial，Bazaar和Subversion。Go对GitHub和Bitbucket等常用的代码管理站点有特殊的支持，同时也支持自定义URL。 如何管理GitHub中单个Repository中的多个Versions？ 有一种解决方法，可以让你在同一个Repository中，保存软件package的多个版本，并使用branches/tags来区分它们。go get支持自定义URL，你可以将版本号插入到package的导入路径中。这意味着需要编写一个代理服务，用于解析URL，并将请求代理到存储Repository的branch/tag上。 幸运的是，有人已经为我们做了这些工作。GoPkg.in完全符合上述内容。 例如： 使用此方法管理gophermail package。这意味着，不是使用github.com/jpoehls/gophermail导入软件package，而是使用gopkg.in/jpoehls/gophermail.v0。.v0是因为gophermail还没有达到1.0。当我发布1.0并声明稳定的API时，导入路径将更改为gopkg.in/jpoehls/gophermail.v1。 弄到这，好像和下面Prometheus的ceph_exporter输出没太大关系啊。。。。。。。。等我再研究研究。。。。 123456789101112131415161718[root@centos7 ~]# go versiongo version go1.10 linux/amd64[root@centos7 ~]# go get -v github.com/digitalocean/ceph_exportergithub.com/digitalocean/ceph_exporter (download)github.com/digitalocean/ceph_exporter/vendor/github.com/ceph/go-ceph/radosgithub.com/digitalocean/ceph_exporter/vendor/github.com/beorn7/perks/quantilegithub.com/digitalocean/ceph_exporter/vendor/github.com/golang/protobuf/protogithub.com/digitalocean/ceph_exporter/vendor/github.com/prometheus/client_model/gogithub.com/digitalocean/ceph_exporter/vendor/github.com/matttproud/golang_protobuf_extensions/pbutilgithub.com/digitalocean/ceph_exporter/vendor/github.com/prometheus/common/internal/bitbucket.org/ww/goautoneggithub.com/digitalocean/ceph_exporter/vendor/github.com/prometheus/common/modelgithub.com/digitalocean/ceph_exporter/vendor/github.com/prometheus/common/expfmtgithub.com/digitalocean/ceph_exporter/vendor/github.com/prometheus/procfs/xfsgithub.com/digitalocean/ceph_exporter/vendor/github.com/prometheus/procfsgithub.com/digitalocean/ceph_exporter/vendor/github.com/prometheus/client_golang/prometheusgithub.com/digitalocean/ceph_exporter/collectorsgithub.com/digitalocean/ceph_exporter/vendor/github.com/prometheus/client_golang/prometheus/promhttpgithub.com/digitalocean/ceph_exporter/vendor/gopkg.in/yaml.v2 参考链接： 【1】http://zduck.com/2014/go-and-package-versioning【2】https://stackoverflow.com/questions/24855081/how-do-i-import-a-specific-version-of-a-package-using-go-get 【3】https://ieevee.com/tech/2017/07/10/go-import.html]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Vundle管理Vim插件]]></title>
    <url>%2F2018%2F04%2F17%2F%E4%BD%BF%E7%94%A8Vundle%E7%AE%A1%E7%90%86Vim%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[工欲善其事，必先利其器。这篇文章记录怎样在Linux上使用Vundle管理Vim插件，提高效率。 步骤很简单，大体如下： 系统环境1234567[root@centos7 ~]# yum install -y redhat-lsb[root@centos7 ~]# lsb_release -aLSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.4.1708 (Core) Release: 7.4.1708Codename: Core Install vim and git1[root@centos7 ~]# yum install -y git vim Set up Vundle1[root@centos7 ~]# git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim Configure Plugins123456789101112131415161718192021222324252627[root@centos7 ~]# vim ~/.vimrcset nocompatible &quot; be iMproved, requiredfiletype off &quot; required&quot; set the runtime path to include Vundle and initializeset rtp+=~/.vim/bundle/Vundle.vimcall vundle#begin()&quot; alternatively, pass a path where Vundle should install plugins&quot;call vundle#begin(&apos;~/some/path/here&apos;)&quot; let Vundle manage Vundle, requiredPlugin &apos;VundleVim/Vundle.vim&apos;&quot; All of your Plugins must be added before the following linecall vundle#end() &quot; requiredfiletype plugin indent on &quot; required&quot; To ignore plugin indent changes, instead use:&quot;filetype plugin on&quot;&quot; Brief help&quot; :PluginList - lists configured plugins&quot; :PluginInstall - installs plugins; append `!` to update or just :PluginUpdate&quot; :PluginSearch foo - searches for foo; append `!` to refresh local cache&quot; :PluginClean - confirms removal of unused plugins; append `!` to auto-approve removal&quot;&quot; see :h vundle for more details or wiki for FAQ&quot; Put your non-Plugin stuff after this line 打开~/.vimrc，并添加你要安装的Plugins 12345[root@centos7 ~]# vim ~/.vimrccall vundle#begin()&quot; 在其之间添加你要安装的Plugins，例如我这里安装一个NERDTreePlugin &apos;scrooloose/nerdtree&apos;call vundle#end() Install Plugins启动vim并运行:PluginInstall 12[root@centos7 ~]# vim ~/.vimrc:PluginInstall 配置已安装NERDTree插件 123[root@centos7 ~]# vim ~/.vimrc添加以下，注意&lt;F2&gt;后面有空格，配置好以后，使用vim打开任意文件就可以使用F2来调出NERDTree了map &lt;F2&gt; :NERDTreeToggle&lt;CR&gt; 当然vim不止这一个插件，这里只是介绍一下怎样配置，其他好玩的插件请自己发掘。 vim常用选项http://gohom.win/2015/06/08/Vim-Setup/]]></content>
      <tags>
        <tag>VIM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vdbench 文件系统测试]]></title>
    <url>%2F2018%2F04%2F12%2Fvdbench-%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[Centos 7 安装vdbench12345671、首先安装Java JDK和一些工具包，这里使用的是java-1.7.0[root@cephL ~]# sudo yum install -y java-1.7.0-openjdk java-1.7.0-openjdk-devel unzip[root@cephL ~]# mkdir vdbench &amp;&amp; cd vdbench2、把下载后的vdbench拷贝到该目录并解压，这里使用的是vdbench50406[root@cephL vdbench]# unzip vdbench50406.zip3、测试是否可以运行[root@cephL vdbench]# ./vdbench -t 运行测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152531、准备测试目录[root@cephL node-1]# pwd/root/node-12、到vdbench目录准备配置文件[root@cephL ~]# cd /root/vdbench/[root@cephL vdbench]# vi filesystem.conffsd=fsd1,anchor=/root/node-1,depth=2,width=2,files=2,size=128kfwd=fwd1,fsd=fsd1,operation=read,xfersize=4k,fileio=sequential,fileselect=random,threads=2rd=rd1,fwd=fwd1,fwdrate=max,format=yes,elapsed=10,interval=1该配置文件表示：第一行：fsd、fwd、rd：是唯一标识depth：从/root/node-1目录开始，在其中创建2层深度的目录（深度）width：从/root/node-1目录开始，每层目录创建2个平级目录（广度）files：在使用depth和width创建的目录中，最深层每个目录创建2个文件size：每个文件大小为128kopenflags：第二行：operation：值为read，表示每个线程，根据fileselect的值（这里是随机）选择一个文件后，打开该文件进行读取xfersize：连续读取4k blocks(xfersize=4k)直到文件结束(size=128k)，关闭文件并随机选择另一个文件fileio：表示文件IO的方式，random或者sequentialfileselect：值为random，表示每个线程随机选择一个文件threads：值为2，表示启动2个线程（线程默认值为1）第三行：fwdrate：每秒有多少file system operations，max为无限制format：值为yes，表示创建完整的目录结构，包括所有文件初始化到所需的128k大小elapsed：持续运行时间，默认设置为30（以秒为单位）。注意：至少是interval的2倍，interval：该参数指定每个报告间隔时间（以秒为单位）3、开始一个简单的测试[root@cephL vdbench]# ./vdbench -f filesystem.conf4、查看被测目录中生成的测试文件[root@cephL ~]# tree /root/node-1/ -h/root/node-1/ --- depth 0├── [ 68] no_dismount.txt├── [ 44] vdb.1_1.dir --- depth 1 width 1│ ├── [ 50] vdb.2_1.dir --- depth 2 width 1│ │ ├── [128K] vdb_f0001.file --- depth 2 width 1 files 1│ │ └── [128K] vdb_f0002.file --- depth 2 width 1 files 2│ └── [ 50] vdb.2_2.dir --- depth 2 width 2│ ├── [128K] vdb_f0001.file --- depth 2 width 2 files 1│ └── [128K] vdb_f0002.file --- depth 2 width 2 files 2├── [ 44] vdb.1_2.dir --- depth 1 width 2│ ├── [ 50] vdb.2_1.dir --- depth 2 width 1│ │ ├── [128K] vdb_f0001.file --- depth 2 width 1 files 1│ │ └── [128K] vdb_f0002.file --- depth 2 width 1 files 2│ └── [ 50] vdb.2_2.dir --- depth 2 width 2│ ├── [128K] vdb_f0001.file --- depth 2 width 2 files 1│ └── [128K] vdb_f0002.file --- depth 2 width 2 files 2└── [ 159] vdb_control.file6 directories, 10 files 其他常用参数openflags12这个参数可以指定在SD, WD, FSD, FWD, RD中可用 sizes（注意这里有s）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970这个参数可以指定在FSD中该参数指定文件的大小（size），可以指定一个或一组文件的大小（size）。例如：sizes=(32k,50,64k,50)，其中第一个数字表示文件大小（size），第二个数字表示必须具有此大小（size）的文件的百分比。如果指定一组文件，百分比加起来必须为100。当您指定sizes=(nnn,0)时，vdbench将创建平均大小（size）为&apos;nnn&apos;字节的文件以上标出（size）可能与下面的规则有关，所以保留原文。以下规则个人理解为，如果大于某个size规则（如果有小数点），那么必须是某个规则的倍数。有一些规则与最终使用的文件大小（size）有关：如果size &gt; 10m，size将为1m的倍数。（个人理解，size为10.1m是不行的，必须是1m的倍数）如果size &gt; 1m，size将为100k的倍数。如果size &gt; 100k，size将为10k的倍数。如果size &lt; 100k，size将为1k的倍数。关于百分比的实验：实验1在8个文件中，4个1k的文件，4个2k的文件[root@cephL vdbench]# vi filesystem.conffsd=fsd1,anchor=/root/node-1,depth=2,width=2,files=2,sizes=(1k,50,2k,50)fwd=fwd1,fsd=fsd1,operation=read,xfersize=4k,fileio=sequential,fileselect=random,threads=2rd=rd1,fwd=fwd1,fwdrate=max,format=yes,elapsed=10,interval=1[root@cephL vdbench]# ./vdbench -f filesystem.conf[root@cephL ~]# tree node-1/ -hnode-1/├── [ 68] no_dismount.txt├── [ 44] vdb.1_1.dir│ ├── [ 50] vdb.2_1.dir│ │ ├── [2.0K] vdb_f0001.file│ │ └── [1.0K] vdb_f0002.file│ └── [ 50] vdb.2_2.dir│ ├── [1.0K] vdb_f0001.file│ └── [2.0K] vdb_f0002.file├── [ 44] vdb.1_2.dir│ ├── [ 50] vdb.2_1.dir│ │ ├── [1.0K] vdb_f0001.file│ │ └── [1.0K] vdb_f0002.file│ └── [ 50] vdb.2_2.dir│ ├── [2.0K] vdb_f0001.file│ └── [2.0K] vdb_f0002.file└── [ 194] vdb_control.file6 directories, 10 files实验2在8个文件中，1个1k的文件，3个2k的文件，3个3k的文件，1个4k的文件。不知道为什么。。。。。。。。[root@cephL vdbench]# vi filesystem.conffsd=fsd1,anchor=/root/node-1,depth=2,width=2,files=2,sizes=(1k,25,2k,25,3k,25,4k,25)fwd=fwd1,fsd=fsd1,operation=read,xfersize=4k,fileio=sequential,fileselect=random,threads=2rd=rd1,fwd=fwd1,fwdrate=max,format=yes,elapsed=10,interval=1[root@cephL vdbench]# ./vdbench -f filesystem.conf[root@cephL ~]# tree node-1/ -hnode-1/├── [ 68] no_dismount.txt├── [ 44] vdb.1_1.dir│ ├── [ 50] vdb.2_1.dir│ │ ├── [3.0K] vdb_f0001.file│ │ └── [2.0K] vdb_f0002.file│ └── [ 50] vdb.2_2.dir│ ├── [1.0K] vdb_f0001.file│ └── [3.0K] vdb_f0002.file├── [ 44] vdb.1_2.dir│ ├── [ 50] vdb.2_1.dir│ │ ├── [2.0K] vdb_f0001.file│ │ └── [2.0K] vdb_f0002.file│ └── [ 50] vdb.2_2.dir│ ├── [4.0K] vdb_f0001.file│ └── [3.0K] vdb_f0002.file└── [ 234] vdb_control.file6 directories, 10 files 多机测试应用场景为多个NFS Client挂在相同NFS Server的读写。 12345678910111213hd=default,vdbench=/home/vdbench,user=root,shell=sshhd=hd1,system=node1hd=hd2,system=node2hd=hd3,system=node3fsd=fsd1,anchor=/mnt/test863,depth=1,width=10,files=10000,size=20m,shared=yesfwd=format,threads=6,xfersize=1mfwd=default,xfersize=1m,fileio=random,fileselect=random,rdpct=100,threads=6fwd=fwd1,fsd=fsd1,host=hd1fwd=fwd2,fsd=fsd1,host=hd2fwd=fwd3,fsd=fsd1,host=hd3rd=rd1,fwd=fwd*,fwdrate=max,format=(restart,only),elapsed=600,interval=1 参数解析： 有3台测试节点node1、node2、node3。每台测试节点的/home/vdbench/目录都存在可执行vdbench二进制文件（位置必须相同），使用root用户通过ssh方式连接（节点间需要做ssh免密），每台测试节点的测试目录为/mnt/test863，目录深度为1，最深层目录中的目录宽度为10，最深层每个目录中有10000个文件，每个文件大小20mb 关于shared=yes 随着Vdbench运行多个slaves和可选的多个hosts，slaves和hosts之间关于文件状态的通信变得困难。使所有这些slaves设备相互通信所涉及的开销变得过于昂贵。您不希望一个slave删除另一个slave当前正在读取或写入的文件。因此，Vdbench不允许您跨slaves和hosts共享FSD。 当然，在你开始使用庞大的文件系统之前，这一切听起来都很棒。 您刚刚填满了500 TB的磁盘文件，然后您决定要与一个或多个远程主机共享该数据。 从头开始重新创建整个文件结构需要很长时间。 该怎么办？ 指定’shared = yes’时，Vdbench将允许您共享文件系统定义（FSD）。 它通过允许每个slave设备仅使用FSD文件结构中定义的每个“第n”文件来实现这一点，其中“n”等于slave数量。（It does this by allowing each slave to use only every ‘nth’ file as is defined in the FSD file structure, where ‘n’ equals the amount of slaves.） 这意味着不同的host不会互相踩到脚趾，但有一个例外：当您指定’format = yes’时，Vdbench首先删除已存在的文件结构。由于这可能是一个旧的文件结构，Vdbench无法传播文件删除周围，让每个slave删除他的’第n’文件。因此，每个slave设备都会尝试删除所有文件，但如果删除失败则不会生成错误消息（因为不同的slave设备只是删除了它）。这些失败的删除将被计算并报告在“Miscellaneous statistics”中的“FILE_DELETE_SHARED”计数器下。但是，“FILE_DELETES”计数器可以包含高于存在的文件数量的计数。我已经看到多个slaves设备能够同时删除同一个文件而没有操作系统将任何错误传递给Java的情况。 关于rdpct（Read Percentage） 此参数允许您混合读取和写入。 使用operation=read只允许你做read，operation=write只允许你做write。 但是，指定rdpct=，您将能够在同一个选定文件中混合读取和写入。请注意，对于sequential（顺序），这没有多大意义。您可以以读取块1，写入块2，读取块3等。对于随机I/O，这非常有意义。 关于format= no 不需要任何格式,但现有文件结构必须与 FSD 定义的结构相匹配。 yes Vdbench 将首先删除当前文件结构,然后再次创建文件结构。然后,它将执行您在 RD 中的请求。 restart Vdbench将仅创建尚未创建的文件，并且还将扩展未达到其正确大小的文件。 （这是totalsize和workingsetsize可以发挥作用的地方）。 only 与’yes’相同，但Vdbench不会执行当前的RD。 dir(ectories) 与‘yes’相同，但它只会创建目录。 clean Vdbench只会删除当前的文件结构，而不会执行当前的RD。 once 这将覆盖每个forxxx参数循环完成format的默认行为。 limited format将在elapsed=seconds之后终止，而不是所有文件或为totalsize=选择的文件已格式化之后终止。 complete 只能与’format=no’一起使用，并且会告诉Vdbench format已经完成，但是Vdbench不应该尝试通过目录搜索来验证每个目录和文件的状态。 当然，如果一个或多个目录或文件丢失或文件未达到其预期大小，结果不可预测。在测试期间删除或创建目录或文件时非常危险。]]></content>
  </entry>
  <entry>
    <title><![CDATA[RocksDB介绍]]></title>
    <url>%2F2018%2F04%2F12%2FRocksDB%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Welcome to RocksDBRocksDB：A Persistent Key-Value Store for Flash and RAM Storage RocksDB是一个嵌入式key-value store C ++库，其中keys和values是任意byte streams。RocksDB由Facebook Database Engineering Team开发和维护，基于LevelDB二次开发，并对LevelDB API提供向后兼容。它支持原子读取和写入。RocksDB借鉴了开源leveldb项目中的重要代码以及Apache HBase的重要思想。最初的代码是从开源的leveldb 1.5中分离出来的。它还建立在Facebook之前在RocksDB开发的代码和想法之上。 RocksDB针对Flash进行了优化，延迟极低。RocksDB使用Log Structured Database Engine进行存储，完全用C ++编写。一个名为RocksJava的Java版本目前正在开发中。 RocksDB具有高度灵活的配置选项，可以调整以运行在各种生产环境上的不同设备之上，包括pure memory，Flash，hard disks或HDFS。它支持各种压缩算法并且为production support和debugging提供良好的工具。 Features 专门希望存储在本地Flash drives或RAM中，高达几TB数据的应用程序服务器而设计 针对fast storage设备，存储small到medium size key-values进行了优化( flash devices 或 in-memory ) 与CPU数量线性扩展，以便在具有多核的处理器上运行良好 Features Not in LevelDB RocksDB引入了几十个新的主要features。不在LevelDB中的feature列表 Assumptions and GoalsPerformanceRocksDB设计主要专注于fast storage和server workloads。它应该利用Flash或RAM子系统提供的高速率读/写全部潜力。它应该支持高效的point lookups以及range scans。它应该可配置为支持high random-read workloads，high update workloads或两者的组合（最优解）。其架构应易于调整Read Amplification, Write Amplification 和 Space Amplification。 Production SupportCompatibility]]></content>
      <tags>
        <tag>RocksDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos 7 部署Ceph L版]]></title>
    <url>%2F2018%2F04%2F10%2Fcentos-7-%E9%83%A8%E7%BD%B2ceph-L%E7%89%88%2F</url>
    <content type="text"><![CDATA[ceph L版已经发布很久了，官方说默认使用BlueStore作为OSD的存储后端，在Cephalocon APAC 2018上也是讨论的焦点之一。 提到BlueStore，不得不说一说Ceph的STORAGE DEVICES。 STORAGE DEVICESCeph守护进程将数据存储在磁盘上：123456Ceph OSDs ( Object Storage Daemons ) Client端的大多数数据写入Ceph后被存储的地方，一般而言，每个OSD都由单一存储设备支持，如传统硬盘（HDD）或固态硬盘（SSD）。 OSD还可以由多种设备组合，如存储数据的HDD和存储某些元数据的SSD（或SSD的分区）。 群集中OSD的数量通常取决于你要存储的数据量，还需要考虑每个存储设备的容量以及冗余级别和存储类型（replication或erasure coding）。Ceph Monitor 管理关键群集状态，如cluster membership和authentication信息。对于较小的集群，需要几千兆字节（几个GB），然而对于较大的集群，monitor的数据库可以达到几十甚至几百千兆（几十个GB甚至几百个GB）。 OSD BACKENDSOSD可以通过两种方式管理存储的数据。从Luminous 12.2.z发行版开始，新的默认（推荐）后端是 BlueStore。在Luminous之前，默认（也是唯一的选择）是 FileStore。 BLUESTOREBlueStore是专门用于Ceph OSD管理磁盘上的数据的专用存储后端。在过去十年间，受到了FileStore管理OSD经验的启发.BlueStore的主要功能包括：123456789101112131415161718直接管理存储设备 ( Direct management of storage devices ) BlueStore使用原始块设备或分区。这避免了任何可能限制性能或增加复杂性的抽象层（如像XFS这样的本地文件系统）。使用RocksDB进行元数据管理 ( Metadata management with RocksDB ) 为了管理内部元数据，我们嵌入了RocksDB的key/value数据库。例如在磁盘上，从object names到block locations的映射。完整的数据和元数据校验 ( Full data and metadata checksumming ) 默认情况下，写入BlueStore的所有数据和元数据都受到一个或多个校验和的保护。没有数据或元数据在未经过验证的情况下，就从磁盘读取或返回给用户。内置压缩 ( Inline compression ) 写入的数据在写入磁盘之前可以选择压缩。多设备元数据分层 ( Multi-device metadata tiering ) BlueStore允许将其内部journal（预写日志，write-ahead log）写入单独的高速设备（如SSD，NVMe或NVDIMM）以提高性能。 如果有大量更快速的存储可用，则内部元数据也可以存储在更快的设备上。高效的写时复制 ( Efficient copy-on-write ) RBD和CephFS快照依赖于copy-on-write clone机制，也在BlueStore中得到了有效的实现。这将为常规快照和erasure coded池提供高效的IO（依靠clone实现高效的two-phase commits） http://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/http://docs.ceph.com/docs/master/rados/operations/bluestore-migration/ FILESTOREFileStore是在Ceph中存储objects的传统方法。它依赖于标准文件系统（通常是XFS）和某个元数据的key/value数据库（传统上是LevelDB，现在是RocksDB）结合使用。FileStore经过良好测试并广泛用于生产，但由于其整体设计和对传统文件系统存储object数据的依赖性，因此存在许多性能缺陷。尽管FileStore通常能够在大多数与POSIX兼容的文件系统（包括btrfs和ext4）上运行，但我们只建议使用XFS。btrfs和ext4都有已知的bug和缺陷，使用它们可能会导致数据丢失。默认情况下，所有的Ceph提供的工具都将使用XFS。 http://docs.ceph.com/docs/master/rados/configuration/filestore-config-ref/ 在ceph L版代码结构改动比较大，增加了CEPH-MGR向外部监测和管理系统提供额外的监测接口，今天就用虚拟机搭建实验环境玩一玩。 环境信息123456789101112131415161718[root@cephL ~]# yum install -y redhat-lsb[root@cephL ~]# lsb_release -aLSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.4.1708 (Core)Release: 7.4.1708Codename: Core[root@cephL ~]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 40G 0 disk├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 39G 0 part ├─centos-root 253:0 0 36G 0 lvm / └─centos-swap 253:1 0 3G 0 lvm [SWAP]sdb 8:16 0 30G 0 disksdc 8:32 0 30G 0 disksr0 11:0 1 1024M 0 rom 安装安装pip和ceph-deploy1234567[root@cephL ~]# curl "https://bootstrap.pypa.io/get-pip.py" -o "get-pip.py"[root@cephL ~]# python get-pip.py[root@cephL ~]# python -m pip install -U pip[root@cephL ~]# pip install --upgrade setuptools[root@cephL ~]# pip install ceph-deploy[root@cephL ~]# ceph-deploy --version2.0.0 安装ceph软件包12[root@cephL ~]# mkdir ceph-deploy &amp;&amp; cd ceph-deploy[root@cephL ceph-deploy]# ceph-deploy install cephL --release luminous 开始部署一个新的集群，然后为它写一个CLUSTER.conf和keyring1[root@cephL ceph-deploy]# ceph-deploy new --public-network 192.168.56.101/24 --cluster-network 192.168.56.101/24 cephL 部署MON123[root@cephL ceph-deploy]# ceph-deploy mon create-initial[root@cephL ceph-deploy]# ceph-deploy mon create cephLceph 1110 1 0 12:57 ? 00:00:01 /usr/bin/ceph-mon -f --cluster ceph --id cephL --setuser ceph --setgroup ceph 部署OSDbluestore方法 12345# 在创建osd时，L版默认是bluestore[root@cephL ceph-deploy]# ceph-deploy osd create --data /dev/sdb cephLceph 1514 1 0 12:57 ? 00:00:01 /usr/bin/ceph-osd -f --cluster ceph --id 0 --setuser ceph --setgroup ceph[root@cephL ceph-deploy]# ceph-deploy osd create --data /dev/sdc cephLceph 1518 1 0 12:57 ? 00:00:01 /usr/bin/ceph-osd -f --cluster ceph --id 1 --setuser ceph --setgroup ceph 遇到问题 123456789101112131415161718192021222324[root@cephL ceph-deploy]# ceph -s2018-04-10 12:00:19.660298 7fd1fe0ae700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory2018-04-10 12:00:19.660310 7fd1fe0ae700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication2018-04-10 12:00:19.660312 7fd1fe0ae700 0 librados: client.admin initialization error (2) No such file or directory[errno 2] error connecting to the cluster[root@cephL ceph-deploy]# chmod +r *[root@cephL ceph-deploy]# cp ceph.client.admin.keyring /etc/ceph/[root@cephL ceph-deploy]# ceph -s cluster: id: 765752b7-1f77-4d0d-bc18-936b8ad409fd health: HEALTH_WARN no active mgr services: mon: 1 daemons, quorum cephL mgr: no daemons active osd: 2 osds: 2 up, 2 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 bytes usage: 0 kB used, 0 kB / 0 kB avail pgs: filestore方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134# 如果是filestore则需要对data device和journal device先做GPT partition--data DATA The OSD data logical volume (vg/lv) or absolute path to device--journal JOURNAL Logical Volume (vg/lv) or path to GPT partition[root@cephL ceph-deploy]# fdisk /dev/sdbWARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion.欢迎使用 fdisk (util-linux 2.23.2)。更改将停留在内存中，直到您决定将更改写入磁盘。使用写入命令前请三思。命令(输入 m 获取帮助)：gBuilding a new GPT disklabel (GUID: 80097CEF-475B-4161-ACC7-7164F6A39DD2)命令(输入 m 获取帮助)：n分区号 (1-128，默认 1)：第一个扇区 (2048-62914526，默认 2048)：Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (2048-62914526，默认 62914526)：已创建分区 1命令(输入 m 获取帮助)：wThe partition table has been altered!Calling ioctl() to re-read partition table.正在同步磁盘。[root@cephL ceph-deploy]# fdisk /dev/sdcWARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion.欢迎使用 fdisk (util-linux 2.23.2)。更改将停留在内存中，直到您决定将更改写入磁盘。使用写入命令前请三思。命令(输入 m 获取帮助)：gBuilding a new GPT disklabel (GUID: 21DFA98C-5BCF-40E7-A120-3DEDEA6600ED)命令(输入 m 获取帮助)：n分区号 (1-128，默认 1)：第一个扇区 (2048-62914526，默认 2048)：Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (2048-62914526，默认 62914526)：已创建分区 1命令(输入 m 获取帮助)：wThe partition table has been altered!Calling ioctl() to re-read partition table.正在同步磁盘。[root@cephL ceph-deploy]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 40G 0 disk ├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 39G 0 part ├─centos-root 253:0 0 36G 0 lvm / └─centos-swap 253:1 0 3G 0 lvm [SWAP]sdb 8:16 0 30G 0 disk └─sdb1 8:17 0 30G 0 part sdc 8:32 0 30G 0 disk └─sdc1 8:33 0 30G 0 part sr0 11:0 1 1024M 0 rom [root@cephL ceph-deploy]# ceph-deploy osd create --filestore --fs-type xfs --data /dev/sdb1 --journal /dev/sdc1 cephL[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (2.0.0): /usr/bin/ceph-deploy osd create --filestore --fs-type xfs --data /dev/sdb1 --journal /dev/sdc1 cephL[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] bluestore : None[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x22c7320&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] fs_type : xfs[ceph_deploy.cli][INFO ] block_wal : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] journal : /dev/sdc1[ceph_deploy.cli][INFO ] subcommand : create[ceph_deploy.cli][INFO ] host : cephL[ceph_deploy.cli][INFO ] filestore : True[ceph_deploy.cli][INFO ] func : &lt;function osd at 0x225ae60&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] zap_disk : False[ceph_deploy.cli][INFO ] data : /dev/sdb1[ceph_deploy.cli][INFO ] block_db : None[ceph_deploy.cli][INFO ] dmcrypt : False[ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] dmcrypt_key_dir : /etc/ceph/dmcrypt-keys[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] debug : False[ceph_deploy.osd][DEBUG ] Creating OSD on cluster ceph with data device /dev/sdb1[cephL][DEBUG ] connected to host: cephL [cephL][DEBUG ] detect platform information from remote host[cephL][DEBUG ] detect machine type[cephL][DEBUG ] find the location of an executable[ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.4.1708 Core[ceph_deploy.osd][DEBUG ] Deploying osd to cephL[cephL][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[cephL][DEBUG ] find the location of an executable[cephL][INFO ] Running command: /usr/sbin/ceph-volume --cluster ceph lvm create --filestore --data /dev/sdb1 --journal /dev/sdc1[cephL][DEBUG ] Running command: /bin/ceph-authtool --gen-print-key[cephL][DEBUG ] Running command: /bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new 8b7be4a6-b563-434e-b030-132880a10d31[cephL][DEBUG ] Running command: vgcreate --force --yes ceph-8e2515c1-6170-4299-b82c-a5a47681f946 /dev/sdb1[cephL][DEBUG ] stdout: Physical volume "/dev/sdb1" successfully created.[cephL][DEBUG ] stdout: Volume group "ceph-8e2515c1-6170-4299-b82c-a5a47681f946" successfully created[cephL][DEBUG ] Running command: lvcreate --yes -l 100%FREE -n osd-data-8b7be4a6-b563-434e-b030-132880a10d31 ceph-8e2515c1-6170-4299-b82c-a5a47681f946[cephL][DEBUG ] stdout: Logical volume "osd-data-8b7be4a6-b563-434e-b030-132880a10d31" created.[cephL][DEBUG ] Running command: /bin/ceph-authtool --gen-print-key[cephL][DEBUG ] Running command: mkfs -t xfs -f -i size=2048 /dev/ceph-8e2515c1-6170-4299-b82c-a5a47681f946/osd-data-8b7be4a6-b563-434e-b030-132880a10d31[cephL][DEBUG ] stdout: meta-data=/dev/ceph-8e2515c1-6170-4299-b82c-a5a47681f946/osd-data-8b7be4a6-b563-434e-b030-132880a10d31 isize=2048 agcount=4, agsize=1965824 blks[cephL][DEBUG ] = sectsz=512 attr=2, projid32bit=1[cephL][DEBUG ] = crc=1 finobt=0, sparse=0[cephL][DEBUG ] data = bsize=4096 blocks=7863296, imaxpct=25[cephL][DEBUG ] = sunit=0 swidth=0 blks[cephL][DEBUG ] naming =version 2 bsize=4096 ascii-ci=0 ftype=1[cephL][DEBUG ] log =internal log bsize=4096 blocks=3839, version=2[cephL][DEBUG ] = sectsz=512 sunit=0 blks, lazy-count=1[cephL][DEBUG ] realtime =none extsz=4096 blocks=0, rtextents=0[cephL][DEBUG ] Running command: mount -t xfs -o rw,noatime,inode64 /dev/ceph-8e2515c1-6170-4299-b82c-a5a47681f946/osd-data-8b7be4a6-b563-434e-b030-132880a10d31 /var/lib/ceph/osd/ceph-0[cephL][DEBUG ] Running command: chown -R ceph:ceph /dev/sdc1[cephL][DEBUG ] Running command: ln -s /dev/sdc1 /var/lib/ceph/osd/ceph-0/journal[cephL][DEBUG ] Running command: ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/osd/ceph-0/activate.monmap[cephL][DEBUG ] stderr: got monmap epoch 1[cephL][DEBUG ] Running command: chown -R ceph:ceph /dev/sdc1[cephL][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-0/[cephL][DEBUG ] Running command: ceph-osd --cluster ceph --osd-objectstore filestore --mkfs -i 0 --monmap /var/lib/ceph/osd/ceph-0/activate.monmap --osd-data /var/lib/ceph/osd/ceph-0/ --osd-journal /var/lib/ceph/osd/ceph-0/journal --osd-uuid 8b7be4a6-b563-434e-b030-132880a10d31 --setuser ceph --setgroup ceph[cephL][DEBUG ] stderr: 2018-05-07 23:01:34.834993 7f315e466d00 -1 journal check: ondisk fsid 00000000-0000-0000-0000-000000000000 doesn't match expected 8b7be4a6-b563-434e-b030-132880a10d31, invalid (someone else's?) journal[cephL][DEBUG ] stderr: 2018-05-07 23:01:34.865621 7f315e466d00 -1 journal do_read_entry(4096): bad header magic[cephL][DEBUG ] 2018-05-07 23:01:34.865667 7f315e466d00 -1 journal do_read_entry(4096): bad header magic[cephL][DEBUG ] 2018-05-07 23:01:34.865988 7f315e466d00 -1 read_settings error reading settings: (2) No such file or directory[cephL][DEBUG ] stderr: 2018-05-07 23:01:34.916284 7f315e466d00 -1 created object store /var/lib/ceph/osd/ceph-0/ for osd.0 fsid 39f3b85e-ee3c-4d8d-93c2-7f7c8aa47121[cephL][DEBUG ] Running command: ceph-authtool /var/lib/ceph/osd/ceph-0/keyring --create-keyring --name osd.0 --add-key AQBDavBa0IPpIBAAlQxlaWxNrnTX/uaOMdZEQw==[cephL][DEBUG ] stdout: creating /var/lib/ceph/osd/ceph-0/keyring[cephL][DEBUG ] added entity osd.0 auth auth(auid = 18446744073709551615 key=AQBDavBa0IPpIBAAlQxlaWxNrnTX/uaOMdZEQw== with 0 caps)[cephL][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-0/keyring[cephL][DEBUG ] --&gt; ceph-volume lvm prepare successful for: /dev/sdb1[cephL][DEBUG ] Running command: ln -snf /dev/sdc1 /var/lib/ceph/osd/ceph-0/journal[cephL][DEBUG ] Running command: chown -R ceph:ceph /dev/sdc1[cephL][DEBUG ] Running command: systemctl enable ceph-volume@lvm-0-8b7be4a6-b563-434e-b030-132880a10d31[cephL][DEBUG ] stderr: Created symlink from /etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-0-8b7be4a6-b563-434e-b030-132880a10d31.service to /usr/lib/systemd/system/ceph-volume@.service.[cephL][DEBUG ] Running command: systemctl start ceph-osd@0[cephL][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: 0[cephL][DEBUG ] --&gt; ceph-volume lvm create successful for: /dev/sdb1[cephL][INFO ] checking OSD status...[cephL][DEBUG ] find the location of an executable[cephL][INFO ] Running command: /bin/ceph --cluster=ceph osd stat --format=json[ceph_deploy.osd][DEBUG ] Host cephL is now ready for osd use. 移除OSD123456789101112# 使OSD进入out状态[root@cephL ceph-deploy]# ceph osd out 0marked out osd.0.# 观察数据迁移[root@cephL ceph-deploy]# ceph -w# 停止对应的OSD进程[root@cephL ceph-deploy]# sudo systemctl stop ceph-osd@0# 清除数据[root@cephL ceph-deploy]# ceph osd purge 0 --yes-i-really-mean-itpurged osd.0# 在ceph.conf中移除osd配置[root@cephL ceph-deploy]# vi /etc/ceph/ceph.conf 部署CEPH-MGR12345678910install netstat tool[root@cephL ~]# yum -y install net-tools[root@cephL ceph-deploy]# ceph-deploy mgr create cephL:cephLMGRceph 1111 1 0 12:57 ? 00:00:08 /usr/bin/ceph-mgr -f --cluster ceph --id cephLMGR --setuser ceph --setgroup ceph[root@cephL ceph-deploy]# ceph mgr module enable dashboardopen 7000 port[root@cephL ceph-deploy]# sudo firewall-cmd --zone=public --add-port=7000/tcp --permanent[root@cephL ceph-deploy]# sudo firewall-cmd --reload 相关命令 123[root@cephL ceph-deploy]# ceph mgr module ls[root@cephL ceph-deploy]# ceph mgr services[root@cephL ceph-deploy]# ceph tell mgr help 部署MDS并创建CEPH FS 12[root@cephL ceph-deploy]# ceph-deploy mds create cephLceph 2150 1 0 13:00 ? 00:00:00 /usr/bin/ceph-mds -f --cluster ceph --id cephL --setuser ceph --setgroup ceph Ceph文件系统至少需要两个RADOS pool，一个用于存储数据，一个用于存储元数据。 配置这些pool时，可以考虑： ​ 对元数据pool使用更多的replication数量，因为该pool中的任何数据丢失都可能导致整个文件系统无法访问。 ​ 为元数据pool使用SSD等较低延迟的存储设备，因为这将直接影响客户端上文件系统操作的延迟。 12345ceph osd pool create cephfs_data &lt;pg_num&gt;ceph osd pool create cephfs_metadata &lt;pg_num&gt;例如：[root@cephL ceph-deploy]# ceph osd pool create cephfs_data 32[root@cephL ceph-deploy]# ceph osd pool create cephfs_metadata 32 更改pool的副本数 1234ceph osd pool set &#123;poolname&#125; size &#123;num-replicas&#125;例如：[root@cephL ceph-deploy]# ceph osd pool set cephfs_data size 1[root@cephL ceph-deploy]# ceph osd pool set cephfs_data size 1 一旦创建了pool，就可以使用fs new命令启用文件系统： 123ceph fs new &lt;fs_name&gt; &lt;metadata&gt; &lt;data&gt;例如：ceph fs new cephFS cephfs_metadata cephfs_data 一旦创建了文件系统，您的MDS将能够进入active状态。例如，在single MDS system中： 12[root@cephL ceph-deploy]# ceph mds statcephFS-1/1/1 up &#123;0=cephL=up:active&#125; 一旦创建了文件系统并且MDS处于active状态，你就可以挂载文件系统了。如果您创建了多个文件系统，在挂载文件系统时，选择使用哪一个。 如果创建了多个文件系统，并且client在挂载时没有指定挂载哪个文件系统，你可以使用ceph fs set-default命令来设置client默认看到的文件系统。 挂载CEPH FS ( File System ) 有两种方式： KERNEL DRIVER 要挂载Ceph文件系统，您可以在知道monitor主机IP地址的情况下使用mount命令，或使用mount.ceph utility将monitor主机名解析为IP地址。例如： 1234567sudo mkdir /mnt/mycephfssudo mount -t ceph 192.168.0.1:6789:/ /mnt/mycephfs例如：[root@cephL ceph-deploy]# sudo mount -t ceph 192.168.56.101:6789:/ /mnt/mycephfsmount error 22 = Invalid argumentCeph 10.x (Jewel)版本开始，如果使用kernel方式（无论是krbd还是cephFS）官方推荐至少使用4.x的kernel。如果无法升级linux kernel，那么映射rbd请使用librbd方式，cephFS请使用fuse方式。 如果挂载Ceph文件系统时开启了cephx authentication，您必须指定user和secret。 1sudo mount -t ceph 192.168.0.1:6789:/ /mnt/mycephfs -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ== 上述用法在Bash history中留下了secret。更安全的方法是从文件中读取secret。 例如： 1sudo mount -t ceph 192.168.0.1:6789:/ /mnt/mycephfs -o name=admin,secretfile=/etc/ceph/admin.secret 如果您有多个文件系统，请使用mds_namespace选项指定要挂载的文件系统，例如-o mds_namespace=myfs 要卸载Ceph文件系统，可以使用umount命令。 例如： 12sudo umount /mnt/mycephfs提示：在执行此命令之前，请确保您不在挂载的目录中。 FUSE 在用户空间（FUSE）中挂载Ceph文件系统之前，请确保客户端主机具有Ceph配置文件的副本以及Ceph元数据服务器的CAPS keyring。 在您的客户端主机上，将Ceph配置文件从monitor主机复制到/etc/ceph目录。 12sudo mkdir -p /etc/cephsudo scp &#123;user&#125;@&#123;server-machine&#125;:/etc/ceph/ceph.conf /etc/ceph/ceph.conf 在您的客户端主机上，将monitor主机的Ceph keyring复制到/etc/ceph目录。 1sudo scp &#123;user&#125;@&#123;server-machine&#125;:/etc/ceph/ceph.keyring /etc/ceph/ceph.keyring 确保Ceph配置文件和keyring在您的客户端机器上设置了适当的权限（例如，chmod 644）。 要将Ceph文件系统挂在为FUSE，可以使用ceph-fuse命令。 例如： 12sudo mkdir /home/usernname/cephfssudo ceph-fuse -m 192.168.0.1:6789 /home/username/cephfs 如果您拥有多个文件系统，请使用 –client_mds_namespace 命令行参数指定要挂载哪一个文件系统，或者向ceph.conf中添加client_mds_namespace设置。 要自动挂载ceph-fuse，您可以在system fstab中添加一个条目。此外还可以使用ceph-fuse@.service和ceph-fuse.target systemd units。通常这些unit文件为ceph-fuse描述默认的dependencies和推荐的execution context。例如使用ceph-fuse挂载到/mnt： 1sudo systemctl start ceph-fuse@/mnt.service 持久化挂载点可通过以下方式进行设置： 1sudo systemctl enable ceph-fuse@/mnt.service 部署RGW Ceph Object Gateway原来叫RADOS Gateway，它是构建在librados之上的对象存储接口，为应用程序提供了一个RESTful gateway，用户可以通过HTTP协议访问Ceph存储集群。 Ceph Object Storage支持两个接口： S3-compatible：与Amazon S3 RESTful API中一些子集兼容的接口，提供对象存储功能。 Swift-compatible：与OpenStack Swift API中一些子集兼容的接口，提供对象存储功能。 Ceph Object Storage使用Ceph Object Gateway daemon (radosgw)，它是一个HTTP server，用于与Ceph存储集群进行交互。由于它提供了与OpenStack Swift和Amazon S3兼容的接口，因此Ceph Object Gateway具有自己的用户管理。Ceph Object Gateway可以将数据存储在与Ceph Filesystem和Ceph Block Device相同的Ceph存储集群中。但是我相信在生产环境中不会这么做，如果数据量大的话会影响Ceph Filesystem和Ceph Block Device的性能，个人一般会独立出一个Ceph Object Gateway集群。S3和Swift API共享一个通用的namespace，因此您可以使用一个API编写数据并使用另一个API检索它。 1Note：Ceph Object Storage 不使用 Ceph Metadata Server 12345678910111213141516171819202122# 必须部署MGR，才能部署RGW[root@cephL ceph-deploy]# ceph-deploy rgw create cephL:RGWroot 2799 1 0 13:13 ? 00:00:00 /usr/bin/radosgw -f --cluster ceph --name client.rgw.RGW --setuser ceph --setgroup ceph# 重启RGW[root@cephL ~]# systemctl restart ceph-radosgw@rgw.cephL.service[root@cephL ~]# systemctl restart ceph-radosgw@rgw问题一，这难道是ceph-deploy 2.0.0的坑？[root@cephL ~]# tailf /var/log/ceph/ceph-client.rgw.log2018-05-11 22:30:31.999421 7f537c31fe00 0 ceph version 12.2.4 (52085d5249a80c5f5121a76d6288429f35e4e77b) luminous (stable), process (unknown), pid 34502018-05-11 22:30:32.021546 7f537c31fe00 -1 auth: unable to find a keyring on /var/lib/ceph/radosgw/ceph-rgw/keyring: (2) No such file or directory2018-05-11 22:30:32.021561 7f537c31fe00 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication2018-05-11 22:30:32.021563 7f537c31fe00 0 librados: client.rgw initialization error (2) No such file or directory2018-05-11 22:30:32.022900 7f537c31fe00 -1 Couldn't init storage provider (RADOS)[root@cephL radosgw]# pwd/var/lib/ceph/radosgw[root@cephL radosgw]# lsceph-rgw.RGW[root@cephL radosgw]# mv ceph-rgw.RGW ceph-rgw 配置变动在L版中，删除pool的操作做了强制限制。需要在/etc/ceph/ceph.conf中加入相关参数才允许删除pool。 12# 允许删除pool，需要添加mon allow pool delete = true]]></content>
      <tags>
        <tag>CEPH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go命令]]></title>
    <url>%2F2018%2F04%2F07%2FGo%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Go命令Go发行版包含了一个名为go的命令，实现Go包和命令的自动下载，编译，安装和测试。 动机Go从开始就明确目标：能够仅使用源代码本身的信息来构建Go代码，而不需要编写makefile或Makefile。 起初，在没有Go编译器的时候，为了达到自动化构建的目的，Go单独使用一个程序生成makefile。新的go命令的目的使我们回到这个理想。 Go的惯例该go命令要求代码遵循几个关键的惯例：123451、导入路径源自源码的URL。对于Bitbucket、GitHub、Google Code和Launchpad来说， 其代码仓库的根目录由该仓库的主URL确定，无需 http:// 前缀。 子目录名附加在该路径之后。例如https://github.com/golang/example这个项目，仓库根目录的导入路径为“github.com/golang/example”。 stringutil 包存储在子目录中，因此它的导入路径为“github.com/golang/example/stringutil”。2、本地文件系统中存储源码的位置源自导入路径。特别地，首选路径为 $GOPATH/src/&lt;导入路径&gt;。若 $GOPATH 未设置， go命令会回到标准Go包存储源码的地方，即 $GOROOT/src/pkg/&lt;导入路径&gt;。 若 $GOPATH 设置了多个路径，go命令就会尝试这些目录的每一个 &lt;目录&gt;/src/&lt;导入路径&gt;。3、源码树中的每个目录都对应于单个包。通过将一个目录限定为单个包， 我们无需先指定目录，再从该目录中指定包来创建混合的导入路径了。此外， 大部分文件管理工具和用户界面，都是将目录作为基本工作单元的。 将基本的Go单元—包—同化为文件系统的结构，也就意味着文件系统工具成了Go包的工具。 复制、移动或删除一个包就对应于复制、移动或删除一个目录。4、包只通过源码中的信息构建。这会让它更容易适应构建环境和条件的改变。 例如，若我们允许额外的配置（如编译器标志或命令行选项等），那么每当构建工具被更改后， 相应的配置也需要更新；它天生还会绑定至特定的工具链。 go 命令基础首先配置GOPATH，进入此目录后，我们就可以添加一些源码了。假设我们想要使用codesearch项目中的索引包， 以及一个左倾红黑树包。我们可以用“go get”子命令安装二者：12$ go get code.google.com/p/codesearch/index$ go get github.com/petar/GoLLRB/llrb 这两个包现在已被下载并安装到我们的 $GOPATH 目录中了。该目录树现在包含了 src/code.google.com/p/codesearch/index/ 和 src/github.com/petar/GoLLRB/llrb/ 这两个目录，以及那些库和依赖的已编译包 （在 pkg/ 中) “go list”子命令会列出其对应实参的导入路径，而模式”./…” 意为从当前目录（”./“）开始，查找该目录下的所有包（”…”）： 123yujiangdeMBP-13:go yujiang$ go list ./...github.com/prometheus/prometheus/cmd/prometheusgithub.com/prometheus/prometheus/cmd/promtool 我们也可以测试这些包: 1go test ./... “go install”子命令会将包的最新副本安装到pkg目录中。由于 go 命令会分析依赖关系，因此，除非该包导入的其它包已过期，否则 “go install” 也会递归地根据依赖关系安装它们。 go get可以根据要求和实际情况从互联网上下载或更新指定的代码包及其依赖包，并对它们进行编译和安装。 12[root@centos7 ~]# go help getusage: go get [-d] [-f] [-fix] [-insecure] [-t] [-u] [-v] [build flags] [packages] -d标志指示在下载软件包后停止，不安装软件包。 -f标志仅在设置-u时有效，该标记会让命令程序忽略掉对已下载代码包的导入路径的检查。如果下载并安装的代码包所属的项目是你从别人那里Fork过来的，那么这样做就尤为重要了。 -fix标志指示命令程序在下载代码包后先执行修正动作，而后再进行编译和安装。 -insecure标志允许命令程序使用非安全的scheme（如HTTP）去下载指定的代码包。如果你用的代码仓库（如公司内部的Gitlab）没有HTTPS支持，可以添加此标记。请在确定安全的情况下使用它。 -t标志指示让命令程序同时下载并安装指定的代码包中的测试源码文件中依赖的代码包。 -u标志指示让命令利用网络来更新已有代码包及其依赖包。默认情况下，该命令只会从网络上下载本地不存在的代码包，而不会更新已有的代码包。 -v标志启用详细的进度和调试输出。 -x标志可以看到go get命令执行过程中所使用的所有命令 Get也接受build flags来控制安装。请参阅go help build。 当检出一个新的包时，get创建目标目录GOPATH/src/&lt;import-path&gt;。如果GOPATH包含多个条目，get使用第一个条目。欲了解更多详情，请参阅：go help gopath。 在checkout或update软件包时，请查找与本地安装的Go版本相匹配的branch或tag。最重要的规则是，如果本地安装运行版本“go1”，则搜索名为“go1”的branch或tag。如果不存在这样的版本，它将检索软件包的默认branch。 当去checkout或update git repository时，它还会更新repository引用的任何git子模块。 永远不要checksout或update存储在vendor目录中的代码。 有关指定软件包的更多信息，请参阅go help packages。 有关go get如何查找要下载的源代码的更多信息，请参阅go help importpath。 另请参见：go build，go install，go clean。 参考资料【1】http://wiki.jikexueyuan.com/project/go-command-tutorial/0.3.html]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置 Golang 开发环境]]></title>
    <url>%2F2018%2F04%2F06%2FGolang-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[配置 Golang 开发环境1、首先根据你所使用的系统，下载对应的Golang包。 国内可以在这里下载：https://studygolang.com/dl 2、然后配置Golang的环境变量，配置方法类似于Java环境变量 持久化方法：编辑 ~/.bash_profile 文件，配置自己的目录，例如： 123456789101112GOPATH=&quot;/Users/yujiang/go&quot;GOROOT=&quot;/usr/local/go&quot;# 测试段export GOPATH=&quot;/root/gocodes&quot;export GOROOT=/usr/local/goexport GOBIN=$GOROOT/binexport GOPKG=$GOROOT/pkg/tool/linux_amd64 export GOARCH=amd64export GOOS=linuxexport GOPATH=/Golangexport PATH=$PATH:$GOBIN:$GOPKG:$GOPATH/bin 使配置生效source ~/.bash_profile 主要的环境变量有: GOROOT：你所安装的go可执行文件的目录 GOROOT=&quot;/usr/local/go&quot; GOPATH：是自己的项目所在目录，也就是所谓的工作目录，可以配置多个目录，使用’;’分隔。 GOPATH如果配置多个目录，使用go install/go get时，会默认匹配第一个目录，后面的忽略。 GOPATH=&quot;/Users/yujiang/go&quot; 3、工作目录结构 通常在你的工作目录中，包含3个子目录123bin 存放编译后的二进制文件src 存放源码文件pkg 存放编译后的库文件*.a]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DEPLOY A CLUSTER MANUALLY - MANUAL DEPLOYMENT]]></title>
    <url>%2F2017%2F12%2F03%2FDEPLOY-A-CLUSTER-MANUALLY-MANUAL-DEPLOYMENT%2F</url>
    <content type="text"><![CDATA[所有Ceph集群都至少需要一个monitor，并且至少需要与群集上存储的对象的副本一样多的OSD。 Bootstrapping the initial monitor(s) 是部署Ceph存储集群的第一步。Monitor部署还为整个群集设置了重要的标准，例如池副本的数量，每个OSD的placement groups数量，心跳间隔，是否需要身份验证等。大多数这些值是默认设置的，因此在生产环境设置集群时了解这些值是有用的。 按照与Installation (Quick)http://docs.ceph.com/docs/master/start/相同的配置，我们将建立一个集群，node1为monitor节点，node2和node3为OSD节点。 MONITOR BOOTSTRAPPING Bootstrapping a monitor（Ceph存储集群，理论上）需要一些东西： Unique Identifier: fsid是集群的唯一标识符，当Ceph存储集群主要用于Ceph Filesystem时，代表从日期生成的File System ID Cluster Name: Ceph集群有一个集群名称，这是一个没有空格的简单字符串。默认群集名称是ceph，但是您可以指定不同的群集名称。当您使用多个群集时，您需要清楚地了解您正在使用哪个群集，这时覆盖默认的集群名称特别有用。例如，当您在federated architecture中运行多个群集时，群集名称（例如，us-west，us-east）标识当前CLI会话的群集。注：要在命令行接口上标识集群名称，请指定具有群集名称的Ceph配置文件（例如，ceph.conf，us-west.conf，us-east.conf等）。另请参阅CLI使用（ceph –cluster {cluster-name}）。 Monitor Name: 群集中的每个monitor实例都有唯一的名称。通常情况下，Ceph Monitor名称是主机名（我们推荐每个单独的主机使用一个Ceph Monitor，Ceph OSD Daemons不能混合部署Ceph Monitors）。您可以通过hostname -s检索short hostname。 Monitor Map: Bootstrapping the initial monitor(s)需要您生成一个monitor map。monitor map需要fsid，群集名称（或使用默认值）以及至少一个主机名和它的IP地址。 Monitor Keyring: Monitors通过secret key相互通信。当bootstrapping the initial monitor(s)时，您必须生成一个带有monitor secret的keyring并提供它。 Administrator Keyring: 要使用ceph CLI工具，你必须有一个client.admin user。因此，您必须生成admin user和keyring，并且还必须将client.admin user添加到monitor keyring中。 上述要求并不意味着一个Ceph配置文件的建立。但是，作为最佳做法，我们建议创建一个Ceph配置文件并使用fsid，mon initial members和mon host配置填充它。 您也可以在运行时获取并设置所有的monitor settings。但是，Ceph配置文件可能只包含那些覆盖默认值的settings。将settings添加到Ceph配置文件时，这些settings会覆盖默认settings。在Ceph配置文件中维护这些settings可以更容易地维护集群。 步骤如下： 1、登录到initial monitor node(s)： ssh {hostname} For example: ssh node1 2、确保你有一个Ceph配置文件的目录。默认情况下，Ceph使用/etc/ceph。安装ceph时，安装程序将自动创建/etc/ceph目录。 ls /etc/ceph 注意：清除群集时，部署工具可能会删除此目录（例如，ceph-deploy purgedata {node-name}, ceph-deploy purge {node-name}）。 3、创建一个Ceph配置文件。默认情况下，Ceph使用ceph.conf，其中ceph反映了集群名称。 sudo vim /etc/ceph/ceph.conf 4、为您的群集生成一个唯一的ID（即fsid）。 uuidgen 5、添加唯一的ID到您的Ceph配置文件。 fsid = {UUID} For example: fsid = a7f64266-0894-4f1e-a635-d0aeaca0e993 6、将initial monitor(s)添加到您的Ceph配置文件中。 mon initial members = {hostname}[,{hostname}] For example: mon initial members = node1 7、将initial monitor(s)的IP地址添加到您的Ceph配置文件并保存该文件。 mon host = {ip-address}[,{ip-address}] For example: mon host = 192.168.0.1 注意：您可以使用IPv6地址而不是IPv4地址，但必须将ms bind ipv6设置为true。有关网络配置的详情，请参阅Network Configuration Referencehttp://docs.ceph.com/docs/master/rados/configuration/network-config-ref/。 8、为您的群集创建一个keyring并生成一个monitor secret key。 ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon &apos;allow *&apos; 9、生成一个administrator keyring，生成一个client.admin user并将user添加到该keyring中。 sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon &apos;allow *&apos; --cap osd &apos;allow *&apos; --cap mds &apos;allow *&apos; --cap mgr &apos;allow *&apos; 10、生成一个bootstrap-osd keyring，生成一个client.bootstrap-osd user并将user添加到该keyring。 sudo ceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring --gen-key -n client.bootstrap-osd --cap mon &apos;profile bootstrap-osd&apos; 11、将生成的keys添加到ceph.mon.keyring。 sudo ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring sudo ceph-authtool /tmp/ceph.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring 12、使用hostname(s), host IP address(es)和FSID生成monitor map。 将其另存为/tmp/monmap： monmaptool --create --add {hostname} {ip-address} --fsid {uuid} /tmp/monmap For example: monmaptool --create --add node1 192.168.0.1 --fsid a7f64266-0894-4f1e-a635-d0aeaca0e993 /tmp/monmap 13、在monitor host(s)上创建一个默认数据目录（或多个目录）。 sudo mkdir /var/lib/ceph/mon/{cluster-name}-{hostname} For example: sudo -u ceph mkdir /var/lib/ceph/mon/ceph-node1 有关详细信息，请参阅Monitor Config Reference - Data http://docs.ceph.com/docs/master/rados/configuration/mon-config-ref/#data。 14、向monitor daemon(s)填充monitor map 和 keyring。 sudo -u ceph ceph-mon [--cluster {cluster-name}] --mkfs -i {hostname} --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring For example: sudo -u ceph ceph-mon --mkfs -i node1 --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring 15、考虑配置Ceph configuration file。 常用settings包括以下内容： [global] fsid = {cluster-id} mon initial members = {hostname}[, {hostname}] mon host = {ip-address}[, {ip-address}] public network = {network}[, {network}] cluster network = {network}[, {network}] auth cluster required = cephx auth service required = cephx auth client required = cephx osd journal size = {n} osd pool default size = {n} # Write an object n times. osd pool default min size = {n} # Allow writing n copies in a degraded state. osd pool default pg num = {n} osd pool default pgp num = {n} osd crush chooseleaf type = {n} 在前面的示例中，配置的[global]部分可能如下所示： [global] fsid = a7f64266-0894-4f1e-a635-d0aeaca0e993 mon initial members = node1 mon host = 192.168.0.1 public network = 192.168.0.0/24 auth cluster required = cephx auth service required = cephx auth client required = cephx osd journal size = 1024 osd pool default size = 3 osd pool default min size = 2 osd pool default pg num = 333 osd pool default pgp num = 333 osd crush chooseleaf type = 1 16、touch the done 文件。标记monitor已被创建并准备好启动： sudo touch /var/lib/ceph/mon/ceph-node1/done 17、启动monitor(s).对于大多数发行版，现在通过systemd启动服务： sudo systemctl start ceph-mon@node1 对于Ubuntu Trusty，请使用Upstart： sudo start ceph-mon id=node1 [cluster={cluster-name}] 在这种情况下，要在每次重新启动时启动守护进程，必须创建两个空文件，如下所示： sudo touch /var/lib/ceph/mon/{cluster-name}-{hostname}/upstart For example: sudo touch /var/lib/ceph/mon/ceph-node1/upstart 对于以前的Debian/CentOS/RHEL，使用sysvinit： sudo /etc/init.d/ceph start mon.node1 18、确认monitor正在运行。 ceph -s 您应该看到monitor已启动并正在运行的输出，并且您应该看到一个health error，指出placement groups处于非活动状态（inactive）。它应该看起来像这样： cluster: id: a7f64266-0894-4f1e-a635-d0aeaca0e993 health: HEALTH_OK services: mon: 1 daemons, quorum node1 mgr: node1(active) osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 bytes usage: 0 kB used, 0 kB / 0 kB avail pgs: 注意：一旦添加OSD并启动它们后，placement group health errors应该消失。 有关详细信息，请参阅Adding OSDshttp://docs.ceph.com/docs/master/install/manual-deployment/#adding-osds。 MANAGER DAEMON CONFIGURATION 在运行ceph-mon daemon的每个node上，也应该设置一个ceph-mgr daemon。请参阅ceph-mgr administrator’s guidehttp://docs.ceph.com/docs/master/mgr/administrator/#mgr-administrator-guide ADDING OSDS 一旦你的initial monitor(s)运行，你就应该添加OSD。除非有足够的OSD来处理对象的副本数（例如，osd pool default size = 2至少需要两个OSD），否则您的集群无法达到active + clean状态。在bootstrapping你的monitor之后，您的群集有一个默认的CRUSH map; 然而，CRUSH map没有任何Ceph OSD Daemons映射到Ceph节点。 SHORT FORM Ceph提供了ceph-disk实用程序，它可以用于Ceph的prepare disk,partition或directory。ceph-disk实用程序通过incrementing the index来创建OSD ID。 另外，ceph-disk会将新的OSD添加到主机下的CRUSH map中。执行ceph-disk -h获取CLI详细信息。ceph-disk实用程序可以自动执行下面的Long Formhttp://docs.ceph.com/docs/master/install/manual-deployment/#long-form的步骤。要使用short form的过程创建前两个OSD，请在node2和node3上执行以下操作： 1、Prepare the OSD ssh {node-name} sudo ceph-disk prepare --cluster {cluster-name} --cluster-uuid {uuid} {data-path} [{journal-path}] For example: ssh node1 sudo ceph-disk prepare --cluster ceph --cluster-uuid a7f64266-0894-4f1e-a635-d0aeaca0e993 --fs-type ext4 /dev/hdd1 2、Activate the OSD: sudo ceph-disk activate {data-path} [--activate-key {path}] For example: sudo ceph-disk activate /dev/hdd1 注意：如果Ceph Node上没有/var/lib/ceph/bootstrap-osd/{cluster}.keyring副本，请使用–activate-key参数。 LONG FORM没有任何帮助工具的好处，使用以下过程创建一个OSD并将其添加到群集和CRUSH map。要使用long form过程创建前两个OSD，请为每个OSD执行以下步骤。注意：这个过程没有描述利用dm-crypt ‘lockbox’部署dm-crypt。1、Connect to the OSD host and become root. ssh {node-name} sudo bash 2、生成OSD的UUID。 UUID=$(uuidgen) 3、为OSD生成一个cephx key。 OSD_SECRET=$(ceph-authtool --gen-print-key) 4、创建OSD。请注意，如果您需要重新使用以前销毁的OSD ID，OSD ID可以作为ceph osd的附加参数提供。我们假设client.bootstrap-osd key存在于机器上。您也可以在存在该密钥的其他主机上以client.admin身份执行此命令： ID=$(echo &quot;{\&quot;cephx_secret\&quot;: \&quot;$OSD_SECRET\&quot;}&quot; | \ ceph osd new $UUID -i - \ -n client.bootstrap-osd -k /var/lib/ceph/bootstrap-osd/ceph.keyring) 5、在新的OSD上创建默认目录。 mkdir /var/lib/ceph/osd/ceph-$ID 6、如果OSD用于OS drive以外的drive（驱动器），准备与Ceph一起使用，并将其mount到您刚刚创建的目录中。 mkfs.xfs /dev/{DEV} mount /dev/{DEV} /var/lib/ceph/osd/ceph-$ID 7、将secret（密钥）写入OSD keyring文件。 ceph-authtool --create-keyring /var/lib/ceph/osd/ceph-$ID/keyring \ --name osd.$ID --add-key $OSD_SECRET 8、初始化OSD数据目录。 ceph-osd -i $ID --mkfs --osd-uuid $UUID 9、Fix ownership. chown -R ceph:ceph /var/lib/ceph/osd/ceph-$ID 10、将OSD添加到Ceph后，OSD将处于您的配置中。 但是，它还没有运行。在它接收数据之前您必须先启动新的OSD。 For modern systemd distributions: systemctl enable ceph-osd@$ID systemctl start ceph-osd@$ID For example: systemctl enable ceph-osd@12 systemctl start ceph-osd@12 ADDING MDS 在下面的说明中，{id}是一个任意的名字，比如机器的主机名。1、创建mds数据目录： mkdir -p /var/lib/ceph/mds/{cluster-name}-{id} 2、创建一个keyring ceph-authtool --create-keyring /var/lib/ceph/mds/{cluster-name}-{id}/keyring --gen-key -n mds.{id} 3、导入keyring并设置caps ceph auth add mds.{id} osd &quot;allow rwx&quot; mds &quot;allow&quot; mon &quot;allow profile mds&quot; -i /var/lib/ceph/mds/{cluster}-{id}/keyring 4、添加到ceph.conf [mds.{id}] host = {id} 5、手动启动daemon ceph-mds --cluster {cluster-name} -i {id} -m {mon-hostname}:{mon-port} [-f] 6、以正确的方式启动daemon（使用ceph.conf entry） service ceph start 7、如果启动daemon失败并出现此错误 mds.-1.0 ERROR: failed to authenticate: (22) Invalid argument 确保在ceph.conf中的global部分没有设置keyring; 将其移动到client部分; 或添加keyring setting到特定mds daemon。并验证您看到的key与mds数据目录和ceph auth获取的mds.{id}输出相同。 8、现在你已经准备好创建一个Ceph filesystemhttp://docs.ceph.com/docs/master/cephfs/createfs/了。 SUMMARY 一旦启动并运行了monitor和两个OSD，您可以通过执行以下操作来观看placement groups peer： ceph -w 要查看tree，请执行以下操作： ceph osd tree 你应该看到如下所示的输出： id weight type name up/down reweight -1 2 root default -2 2 host node1 0 1 osd.0 up 1 -3 1 host node2 1 1 osd.1 up 1 要添加（或删除）其他monitors，请参阅Add/Remove Monitorshttp://docs.ceph.com/docs/master/rados/operations/add-or-rm-mons/。要添加（或删除）其他Ceph OSD Daemons，请参阅Add/Remove OSDshttp://docs.ceph.com/docs/master/rados/operations/add-or-rm-osds/。]]></content>
      <tags>
        <tag>CEPH-Luminous</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CEPH MANAGER DAEMON - RESTful plugin]]></title>
    <url>%2F2017%2F11%2F27%2FCEPH-MANAGER-DAEMON-RESTful-plugin%2F</url>
    <content type="text"><![CDATA[CEPH MANAGER DAEMON - RESTful plugin- RESTFUL PLUGIN RESTful module通过SSL-secured连接提供REST API对cluster状态的访问。 ENABLING restful模块启用： ceph mgr module enable restful 在API endpoint 可用之前，您还需要配置SSL certificate（证书）。默认情况下，module将在主机上所有IPv4和IPv6地址的8003端口上接受HTTPS请求。 SECURING 所有的restful连接都通过SSL安全保护。您可以使用以下命令生成自签名证书： ceph restful create-self-signed-cert 请注意，对于self-signed certificate（自签名证书），大多数客户端将需要一个flag（标志）来允许连接and/or禁止警告消息。例如，如果ceph-mgr daemon在同一主机上，则： curl -k https://localhost:8003/ 为了更安全的部署，应使用由组织认证机构签发的证书。例如，可以使用类似于以下的命令来生成密钥对： openssl req -new -nodes -x509 \ -subj &quot;/O=IT/CN=ceph-mgr-restful&quot; \ -days 3650 -keyout restful.key -out restful.crt -extensions v3_ca 然后restful.crt应由您的organization的CA（证书颁发机构）签名。一旦完成，您可以将其设置为： ceph config-key set mgr/restful/$name/crt -i restful.crt ceph config-key set mgr/restful/$name/key -i restful.key 其中$name是ceph-mgr实例的名称（通常是主机名）。如果所有manager实例要共享相同的证书，则可以忽略$name部分： ceph config-key set mgr/restful/crt -i restful.crt ceph config-key set mgr/restful/key -i restful.key CONFIGURING IP AND PORT 像任何其他RESTful API endpoint一样，restful绑定IP和port。默认情况下，当前active的ceph-mgr daemon将绑定主机上任何可用的IPv4或IPv6地址的8003端口。由于每个ceph-mgr都拥有自己的restful实例，因此也可能需要单独配置它们。IP和端口可以通过配置key facility进行更改： ceph config set mgr mgr/restful/$name/server_addr $IP ceph config set mgr mgr/restful/$name/server_port $PORT 其中$name是ceph-mgr守护进程的ID（通常是hostname）。这些设置也可以在集群范围内配置，而不是manager 指定的。例如： ceph config set mgr mgr/restful/server_addr $IP ceph config set mgr mgr/restful/server_port $PORT 如果端口没有配置，restful将默认绑定到8003端口。如果没有配置IP地址，restful将绑定到::，这对应于所有可用的IPv4和IPv6地址。 CREATING AN API USER 要创建API用户，请运行以下命令： 1ceph restful create-key &lt;username&gt; 将&lt;username&gt;替换为所需的用户名。 例如，创建一个名为api的用户： 12$ ceph restful create-key api52dffd92-a103-4a10-bfce-5b60f48f764e 从ceph restful create-key api生成的UUID作为用户的密钥。 要列出所有API密钥，请运行以下命令： 1ceph restful list-keys ceph restful list-keys命令将以JSON输出： 123&#123; &quot;api&quot;: &quot;52dffd92-a103-4a10-bfce-5b60f48f764e&quot;&#125; 可以使用curl来测试您的API用户。 例如： 1curl -k https://api:52dffd92-a103-4a10-bfce-5b60f48f764e@&lt;ceph-mgr&gt;:&lt;port&gt;/server 在上述情况下，我们使用GET从服务器endpoint获取信息。 LOAD BALANCER 请注意，restful只会在当前处于active状态的manager启动。查询Ceph集群状态来查看哪个manager处于active状态（例如，ceph mgr dump）。为了使API可通过一致的URL访问，无论哪个管理器daemon当前处于active状态，您可能需要在前端设置一个负载平衡器，以将流量引导至任何可用的manager endpoint。 AVAILABLE METHODS 您可以导航到/doc endpoint，以获取可用endpoints的完整列表以及为每个endpoints实现的HTTP方法。 例如，使用/osd/&lt;id&gt; endpoint的PATCH method设置OSD id 1，则可以使用以下curl命令： 1echo -En &apos;&#123;&quot;up&quot;: true&#125;&apos; | curl --request PATCH --data @- --silent --insecure --user &lt;user&gt; &apos;https://&lt;ceph-mgr&gt;:&lt;port&gt;/osd/1&apos; 或者您可以使用python这样做： 12345678$ python&gt;&gt; import requests&gt;&gt; result = requests.patch( &apos;https://&lt;ceph-mgr&gt;:&lt;port&gt;/osd/1&apos;, json=&#123;&quot;up&quot;: True&#125;, auth=(&quot;&lt;user&gt;&quot;, &quot;&lt;password&gt;&quot;) )&gt;&gt; print result.json() restful module中实现的其他一些endpoints包括 /config/cluster: GET /config/osd: GET, PATCH /crush/rule: GET /mon: GET /osd: GET /pool: GET, POST /pool/&lt;arg&gt;: DELETE, GET, PATCH /request: DELETE, GET, POST /request/&lt;arg&gt;: DELETE, GET /server: GET THE /REQUEST ENDPOINT 您可以使用/request endpoint DELETE, POST or PATCH method轮询请求状态。这些方法默认情况下是异步的，因为它们可能需要更长的时间才能完成执行。您可以通过将?wait=1附加到请求url来修改此行为。]]></content>
      <tags>
        <tag>CEPH-MGR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CEPH MANAGER DAEMON - Local pool plugin]]></title>
    <url>%2F2017%2F11%2F26%2FCEPH-MANAGER-DAEMON-Local-pool-plugin%2F</url>
    <content type="text"><![CDATA[localpool插件可以自动创建本地化为整个集群子集的RADOS pool。例如，默认情况下，它将为集群中的每个不同rack创建一个pool。对于某些希望在本地以及整个集群内分布一些数据的部署可能非常有用。（个人理解这里的本地有可能是不同RULE下的pool，不同RULE下是否同名pool没有验证是否允许） ENABLING localpool模块启用： ceph mgr module enable localpool CONFIGURING localpool模块可以支持以下选项： subtree（default: rack）: 为哪个CRUSH subtree type模块创建一个池 failure_domain (default: host) : 我们应该在哪个故障域中分隔数据副本 pg_num (default: 128) : 为每个pool创建PG的数量 num_rep (default: 3) : 每个pool的副本数。（目前，pool只支持replicated模式） min_size (default: none) : 设置min_size值（如果没有设置此选项，则不会改变Ceph的默认值） prefix (default: by-$subtreetype-) : pool名的prefix（前缀） 通过config-key接口设置这些选项。例如，设置复制2副本，64PG ceph config-key set mgr/localpool/num_rep 2 ceph config-key set mgr/localpool/pg_num 64]]></content>
      <tags>
        <tag>CEPH-MGR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CEPH MANAGER DAEMON - Dashboard plugin]]></title>
    <url>%2F2017%2F11%2F25%2FCEPH-MANAGER-DAEMON-Dashboard-plugin%2F</url>
    <content type="text"><![CDATA[CEPH MANAGER DAEMON - Dashboard plugin - DASHBOARD PLUGINDashboard 插件使用ceph-mgr托管的Web server，可以可视化集群的统计信息。 ENABLINGdashboard模块启用： ceph mgr module enable dashboard CONFIGURATION 像大多数Web应用程序一样，dashboard 绑定到主机名和端口。默认情况下，ceph-mgr daemon托管的dashboard（即当前active manager）将绑定到主机上任何可用的IPv4或IPv6地址的7000端口。由于每个ceph-mgr都拥有自己的dashboard instance，因此可能需要单独配置它们。hostname 和port 可以通过配置key facility进行更改： ceph config-key set mgr/dashboard/$name/server_addr $IP ceph config-key set mgr/dashboard/$name/server_port $PORT 其中$name是托管此dashboard Web应用程序的ceph-mgr的ID。这些设置也可以在集群范围内配置，而不是管理员特定的。 例如： ceph config-key set mgr/dashboard/server_addr $IP ceph config-key set mgr/dashboard/server_port $PORT如果未配置端口，则Web应用程序将绑定到7000端口。如果未配置地址，则Web应用将绑定到::，其对应于所有可用的IPv4和IPv6地址。 您可以为所有URLs配置prefix： ceph config-key set mgr/dashboard/url_prefix $PREFIX 所以你可以访问dashboard http://$IP:$PORT/$PREFIX/. LOAD BALANCER请注意，dashboard 将只在当时处于active状态的管理器上启动。查询Ceph集群状态以查看哪个管理器处于active状态（例如，ceph mgr dump）。为了使仪表板可通过一致的URL访问，无论哪个管理器daemon当前处于active状态，您可能需要在前端设置一个负载平衡器，以将流量引导至任何可用的管理器endpoint。 如果你使用HTTP反向代理转发到dashboard的subpath，则需要配置url_prefix（请参见上文）。]]></content>
      <tags>
        <tag>CEPH-MGR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CEPH MANAGER DAEMON - Installation and Configuration]]></title>
    <url>%2F2017%2F11%2F22%2FCEPH-MANAGER-DAEMON-Installation-and-Configuration%2F</url>
    <content type="text"><![CDATA[CEPH MANAGER DAEMON - Installation and Configuration - CEPH-MGR ADMINISTRATOR’S GUIDE MANUAL SETUP 通常，你可以使用ceph-ansible这样的工具来建立一个ceph-mgr守护进程。 下面这些说明描述了如何手动设置ceph-mgr守护进程。 首先，为守护进程创建一个身份验证密钥： ceph auth get-or-create mgr.$name mon &apos;allow profile mgr&apos; osd &apos;allow *&apos; mds &apos;allow *&apos; 该密钥放置到mgr数据路径中，对于集群名“ceph”和mgr $name为“foo”，该路径将是/var/lib/ceph/mgr/ceph-foo。 启动ceph-mgr守护进程： ceph-mgr -i $name 通过查看ceph状态的输出来检查mgr是否已经出现，它现在应该包含一个mgr状态行： mgr active: $name CLIENT AUTHENTICATION manager是一个新的守护进程，需要新的CephX功能。 如果您从旧版本的Ceph升级集群，或者使用默认安装/部署工具，则admin client应该自动获得此功能。如果您使用其他地方的工具，则在调用某些ceph集群命令时可能会收到EACCES错误。为了解决这个问题，可以通过修改用户权限来为client的cephx功能添加一个“mgr allow *”。 （http://docs.ceph.com/docs/master/rados/operations/user-management/#modify-user-capabilities） HIGH AVAILABILITY 一般来说，你应该在每台运行ceph-mon守护进程的主机上设置一个ceph-mgr来达到相同的可用性。默认情况下，首先启动的ceph-mgr实例将被monitors设置为active，而其他的则是standbys。ceph-mgr守护进程中不需要仲裁。如果 active daemon在mon_mgr_beacon_grace（default 30s）规定时间内发送beacon到monitors失败，standby会替代当前active。如果要抢先进行故障切换，可以使用命令ceph mgr fail 明确标记ceph-mgr daemon失败。 USING MODULES 使用命令ceph mgr module ls查看哪些模块可用，哪些是当前启用的。使用命令ceph mgr module enable 和ceph mgr module disable 分别启用模块和禁用模块。如果启用了一个模块，则active ceph-mgr daemon将加载并执行它。当模块在加载时发布它的IP地址，如HTTP server。要查看这些模块的地址，请使用命令ceph mgr services。一些模块也可以实现一个特殊的standby模式，它在standby ceph-mgr daemons以及active daemon。如果client尝试连接到standby，这使得提供服务的模块能够将其client重定向到active daemon。有关每个模块提供的功能的更多信息，请参阅个别管理器模块的文档页面。 以下是启用dashboard模块的示例： $ ceph mgr module ls { &quot;enabled_modules&quot;: [ &quot;restful&quot;, &quot;status&quot; ], &quot;disabled_modules&quot;: [ &quot;dashboard&quot; ] } $ ceph mgr module enable dashboard $ ceph mgr module ls { &quot;enabled_modules&quot;: [ &quot;restful&quot;, &quot;status&quot;, &quot;dashboard&quot; ], &quot;disabled_modules&quot;: [ ] } $ ceph mgr services { &quot;dashboard&quot;: &quot;http://myserver.com:7789/&quot;, &quot;restful&quot;: &quot;https://myserver.com:8789/&quot; } CALLING MODULE COMMANDS 在模块实现command line hooks的情况下，这些commands将作为普通的Ceph命令来访问： ceph &lt;command | help&gt; 如果你想查看manager处理的命令列表（标准ceph help将显示所有的mon和mgr命令），你可以直接发送一个命令到manager daemon： ceph tell mgr help 注意，没有必要去访问一个特定的mgr instance，mgr将自动选择当前的active daemon。 CONFIGURATION OPTION(mgr_module_path, OPT_STR, CEPH_PKGLIBDIR “/mgr”) // 从哪里加载python模块 mgr module path Description: Path to load modules from Type: String Default: &quot;&lt;library dir&gt;/mgr&quot; mgr data Description: Path to load daemon data (such as keyring) Type: String Default: &quot;/var/lib/ceph/mgr/$cluster-$id&quot; mgr tick period Description: How many seconds between mgr beacons to monitors, and other periodic checks. Type: Integer Default: 5 mon mgr beacon grace Description: How long after last beacon should a mgr be considered failed Type: Integer Default: 30]]></content>
      <tags>
        <tag>CEPH-MGR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vdbench]]></title>
    <url>%2F2017%2F10%2F31%2Fvdbench%2F</url>
    <content type="text"><![CDATA[[root@centos001 vdbench]# lsb_release -aLSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.3.1611 (Core)Release: 7.3.1611Codename: Core vdbench单机测试1、下载vdbench50406.zip 2、解压vdbench50406.zip 3、安装java，jdk[root@centos001 ~]# yum install java-1.8.0-openjdk unzip net-tools iotop -y[root@yujiangcontrol ~]# java -versionopenjdk version “1.8.0_144”OpenJDK Runtime Environment (build 1.8.0_144-b01)OpenJDK 64-Bit Server VM (build 25.144-b01, mixed mode) 4、配置vdbench[root@centos001 ~]# unzip vdbench50406.zip[root@centos001 ~]# ./vdbench -t 5、配置文件[root@centos001 ~]# vi yujiangsd=sd1,lun=/dev/sdb,openflags=o_directwd=wd1,sd=sd1,seekpct=random,rdpct=0,xfersize=8192rd=rd1,wd=wd*,elapsed=120,interval=10,iorate=maxopenflags=o_direct 直接对块设备进行读写 6、执行配置文件[root@centos001 ~]# sudo ./vdbench -f yujiang vdbench多机测试[root@centos001 vdbench]# cat /etc/hosts192.168.30.100 centos001 centos001.test.com192.168.30.103 plana003 plana003.test.com192.168.30.104 plana004 plana004.test.com192.168.30.105 plana005 plana005.test.com 1、生成sshkey[root@centos001 vdbench]# ssh-keygen与其他slave节点做ssh互信[root@centos001 vdbench]# ssh-copy-id plana003[root@centos001 vdbench]# ssh-copy-id plana004[root@centos001 vdbench]# ssh-copy-id plana005[root@centos001 vdbench]# ssh plana003.test.com[root@centos001 vdbench]# ssh plana004.test.com[root@centos001 vdbench]# ssh plana005.test.com 2、把vdbench拷贝到每台slave节点，包括master节点，路径相同/root/vdbench 3、每台slave机监听，ssh不用监听[root@plana003 vdbench]# ./vdbench rsh[root@plana004 vdbench]# ./vdbench rsh[root@plana005 vdbench]# ./vdbench rsh 备注：Host Definition (HD)These parameters are ONLY needed when running Vdbench in a multi-host environment or if you want to override the number of JVMs used in a single-host environment.See also Host Definition parameter detail. hd=default Sets defaults for all HDs that are entered laterhd=localhost Sets values for the current hosthd=host_label Specify a host label.system=host_name Host IP address or network name, e.g. xyz.customer.comvdbench=vdbench_dir_name Where to find Vdbench on a remote host if different from current.jvms=nnn How many slaves to use. See Multi JVM execution.shell=rsh | ssh | vdbench How to start a Vdbench slave on a remote system.user=xxxx Userid on remote system Required.clients=nn This host will simulate the running of multiple ‘clients’. Very useful if you want to simulate numerous clients for file servers without having all the hardware.mount=”mount xxx …” This mount command is issued on the target host after the possibly needed mount directories have been created.rg=name Unique name for this Replay Group (RG).devices=(xxx,yyy,….) The device numbers from Swat’s flatfile.bin.gz to be replayed. Storage Definition (SD)See also Storage Definition Parameter Detail.This set of parameters identifies each physical or logical volume manager volume or file system file used in the requested workload. Of course, with a file system file, the file system takes the responsibility of all I/O: reads and writes can and will be cached (see also openflags=) and Vdbench will not have control over physical I/O. However, Vdbench can be used to test file system file performance (See also File system testing).Example: sd=sd1,lun=/dev/rdsk/cxt0d0s0,threads=8 sd=default Sets defaults for all SDs that are entered later.sd=name Unique name for this Storage Definition (SD).host=name Name of host where this SD can be found. Default ‘localhost’lun=lun_name Name of raw disk or file system file.align=nnn Generate logical byte address in ‘nnn’ byte boundaries, not using default ‘xfersize’ boundaries.count=(nn,mm) Creates a sequence of SD parameters.hitarea=nn See read hit percentage for an explanation. Default 1m.journal=xxx Directory name for journal file for data validationoffset=nnn At which offset in a lun to start I/O.openflags=(flag,..) Pass specific flags when opening a lun or filerange=(nn,mm) Use only a subset ‘range=nn’: Limit Seek Range of this SD.replay=(group,..) Replay Group(s) using this SD.replay=(nnn,..) Device number(s) to select for Swat Vdbench replayresetbus=nnn Issue ioctl (USCSI_RESET_ALL) every nnn seconds. Solaris onlyresetlun=nnn Issue ioctl (USCSI_RESET) every nnn seconds. Solaris onlysize=nn Size of the raw disk or file to use for workload. Optional unless you want Vdbench to create a disk file for you.streams=(nn,mm) Create independent sequential streams on the same device.threads=nn Maximum number of concurrent outstanding I/O for this SD. Default 8 Workload Definition (WD)See also Workload Definition Parameter Detail.The Workload Definition parameters describe what kind of workload must be executed using the storage definitions entered.Example: wd=wd1,sd=(sd1,sd2),rdpct=100,xfersize=4k wd=default Sets defaults for all WDs that are entered later.wd=name Unique name for this Workload Definition (WD)sd=xx Name(s) of Storage Definition(s) to usehost=host_label Which host to run this workload on. Default localhost.iorate=nn Requested fixed I/O rate for this workload.openflags=(flag,..) Pass specific flags when opening a lun or file.priority=nn I/O priority to be used for this workload.range=(nn,nn) Limit seek range to a defined range within an SD.rdpct=nn Read percentage. Default 100.rhpct=nn Read hit percentage. Default 0.seekpct=nn Percentage of random seeks. Default seekpct=100 or seekpct=random.skew=nn Percentage of skew that this workload receives from the total I/O rate.stride=(min,max) To allow for skip-sequential I/O.threads=nn Only available during SD concatenation.whpct=nn Write hit percentage. Default 0.xfersize=nn Data transfer size. Default 4k.xfersize=(n,m,n,m,..) Specify a distribution list with percentages.xfersize=(min,max,align) Generate xfersize as a random value between min and max. Run Definition (RD)See also Run Definition Parameter Detail.The Run Definition parameters define which of the earlier defined workloads need to be executed, what I/O rates need to be generated, and how long the workload will run. One Run Definition can result in multiple actual workloads, depending on the parameters used.Example: rd=run1,wd=(wd1,wd2),iorate=1000,elapsed=60,interval=5There is a separate list of RD parameters for file system testing. rd=default Sets defaults for all RDs that are entered later.rd=name Unique name for this Run Definition (RD).wd=xx Workload Definitions to use for this run.sd=xxx Which SDs to use for this run (Optional).curve=(nn,nn,..) Data points to generate when creating a performance curve.distribution=(x[,variable] I/O inter arrival time calculations: exponential, uniform, or deterministic. Default exponential.elapsed=nn Elapsed time for this run in seconds. Default 30 seconds.maxdata=nnn Stop the run after nnn bytes have been read or written, e.g.maxdata=200g. Vdbench will stop at the lower of elapsed= and maxdata=.endcmd=cmd Execute command or script at the end of the last run(for)compratio=nn Multiple runs for each compression percentage.(for)hitarea=nn Multiple runs for each hit area size.(for)hpct=nn Multiple runs for each read hit percentage.(for)rdpct=nn Multiple runs for each read percentage.(for)seekpct=nn Multiple runs for each seek percentage.(for)threads=nn Multiple runs for each thread count.(for)whpct=nn Multiple runs for each write hit percentage.(for)xfersize=nn Multiple runs for each data transfer size.Most forxxx parameters may be abbreviated to their regular name, e.g. xfersize=(..,..)interval=nn Reporting interval in seconds. Default ‘min(elapsed/2,60)’iorate=(nn,nn,nn,…) One or more I/O rates.iorate=curve Create a performance curve.iorate=max Run an uncontrolled workload.iorate=(nn,ss,…) nn,ss: pairs of I/O rates and seconds of duration for this I/O rate. See also ‘distribution=variable’.openflags=xxxx Pass specific flags when opening a lun or filepause=nn Sleep ‘nn’ seconds before starting next run.replay=(filename,split=split_dir,repeat=nn)-‘filename’: Replay file name used for Swat Vdbench replay ‘split_dir’: directory used to do the replay file split. ‘nn’: how often to repeat the replay.startcmd=cmd Execute command or script at the beginning of the first runwarmup=nn Override warmup period. 1、多slave host不同块设备For instance if a lun is /dev/rdsk/a on hosta but it is named /dev/rdsk/b on hostb then you’ll have to tell Vdbench about it.sd=sd1,lun=/dev/rdsk/a,host=hosta,lun=/dev/rdsk/b,host=hostbBy default Vdbench assumes that the lun names on each host are identical. 2、[root@centos001 vdbench]# cat example5hd=default,vdbench=/root/vdbench,user=root,shell=sshhd=plana003,system=plana003.test.comhd=plana004,system=plana004.test.comhd=plana005,system=plana005.test.com sd=sd1,host=,lun=/dev/sdb,size=10m,openflags=o_directwd=wd1,sd=sd,rdpct=100,xf=4krd=rd1,wd=wd1,el=3,in=1,io=10,warmup=30解释：hd=default以下hd都使用后面的默认值vdbench到每台slave机的相同目录寻找vdbench程序user以root用户登录远程slave机shell以ssh协议登录hd=plana003远程slave机标签system=plana003.test.com ip地址或者配置hosts文件，用来ssh连接slave机 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101root@node1:/home/simth/config# cat vdbenchcase1.confighd=default,vdbench=/home/simth,user=root,shell=sshhd=hd1,system=node1hd=hd2,system=node2hd=hd3,system=node3fsd=sd1,anchor=/home/node-1/node-1-rbd-1,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd2,anchor=/home/node-1/node-1-rbd-2,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd3,anchor=/home/node-1/node-1-rbd-3,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd4,anchor=/home/node-1/node-1-rbd-4,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd5,anchor=/home/node-1/node-1-rbd-5,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd6,anchor=/home/node-1/node-1-rbd-6,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd7,anchor=/home/node-1/node-1-rbd-7,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd8,anchor=/home/node-1/node-1-rbd-8,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd9,anchor=/home/node-1/node-1-rbd-9,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd10,anchor=/home/node-1/node-1-rbd-10,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfwd=wd1,fsd=sd*,hd=hd1,operation=write,threads=1,fileio=sequential,fileselect=random,xfersize=16kfwd=wd2,fsd=sd*,hd=hd2,operation=write,threads=1,fileio=sequential,fileselect=random,xfersize=16kfwd=wd3,fsd=sd*,hd=hd3,operation=write,threads=1,fileio=sequential,fileselect=random,xfersize=16krd=rd1,fwd=wd*,fwdrate=max,format=clean,interval=1rd=rd2,fwd=wd*,fwdrate=122,format=restart,elapse=3600,maxdata=1800G,interval=5,warmup=30hd=default,vdbench=/home/simth,user=root,shell=sshhd=hd1,system=node1hd=hd2,system=node2hd=hd3,system=node3fsd=sd1,anchor=/home/node-1/node-1-rbd-1,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd2,anchor=/home/node-1/node-1-rbd-2,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd3,anchor=/home/node-1/node-1-rbd-3,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd4,anchor=/home/node-1/node-1-rbd-4,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd5,anchor=/home/node-1/node-1-rbd-5,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd6,anchor=/home/node-1/node-1-rbd-6,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd7,anchor=/home/node-1/node-1-rbd-7,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd8,anchor=/home/node-1/node-1-rbd-8,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd9,anchor=/home/node-1/node-1-rbd-9,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd10,anchor=/home/node-1/node-1-rbd-10,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfwd=wd1,fsd=sd*,hd=hd1,operation=read,threads=1,fileio=sequential,fileselect=random,xfersize=1024kfwd=wd2,fsd=sd*,hd=hd2,operation=read,threads=1,fileio=sequential,fileselect=random,xfersize=1024kfwd=wd3,fsd=sd*,hd=hd3,operation=read,threads=1,fileio=sequential,fileselect=random,xfersize=1024k*rd=rd1,fwd=wd*,fwdrate=max,format=clean,interval=1rd=rd2,fwd=wd*,fwdrate=690,format=restart,elapse=3600,maxdata=1800G,interval=5,warmup=30hd=default,vdbench=/home/simth,user=root,shell=sshhd=hd1,system=node1hd=hd2,system=node2hd=hd3,system=node3fsd=sd1,anchor=/home/node-1/node-1-rbd-1,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd2,anchor=/home/node-1/node-1-rbd-2,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd3,anchor=/home/node-1/node-1-rbd-3,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd4,anchor=/home/node-1/node-1-rbd-4,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd5,anchor=/home/node-1/node-1-rbd-5,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd6,anchor=/home/node-1/node-1-rbd-6,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd7,anchor=/home/node-1/node-1-rbd-7,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd8,anchor=/home/node-1/node-1-rbd-8,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd9,anchor=/home/node-1/node-1-rbd-9,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd10,anchor=/home/node-1/node-1-rbd-10,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfwd=wd1,fsd=sd*,hd=hd1,operation=write,fileio=random,threads=8,fileselect=random,xfersize=8kfwd=wd2,fsd=sd*,hd=hd2,operation=write,fileio=random,threads=8,fileselect=random,xfersize=8kfwd=wd3,fsd=sd*,hd=hd3,operation=write,fileio=random,threads=8,fileselect=random,xfersize=8k*rd=rd1,fwd=wd*,fwdrate=max,format=clean,interval=1rd=rd2,fwd=wd*,fwdrate=1014,format=restart,elapse=3600,maxdata=1800G,interval=5,warmup=30hd=default,vdbench=/home/simth,user=root,shell=sshhd=hd1,system=node1hd=hd2,system=node2hd=hd3,system=node3fsd=sd1,anchor=/home/node-1/node-1-rbd-1,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd2,anchor=/home/node-1/node-1-rbd-2,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd3,anchor=/home/node-1/node-1-rbd-3,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd4,anchor=/home/node-1/node-1-rbd-4,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd5,anchor=/home/node-1/node-1-rbd-5,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd6,anchor=/home/node-1/node-1-rbd-6,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd7,anchor=/home/node-1/node-1-rbd-7,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd8,anchor=/home/node-1/node-1-rbd-8,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd9,anchor=/home/node-1/node-1-rbd-9,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfsd=sd10,anchor=/home/node-1/node-1-rbd-10,openflags=o_direct,depth=3,width=3,files=3300,size=(2M,100),shared=yesfwd=wd1,fsd=sd*,hd=hd1,operation=read,fileio=random,threads=8,fileselect=random,xfersize=8kfwd=wd2,fsd=sd*,hd=hd2,operation=read,fileio=random,threads=8,fileselect=random,xfersize=8kfwd=wd3,fsd=sd*,hd=hd3,operation=read,fileio=random,threads=8,fileselect=random,xfersize=8k*rd=rd1,fwd=wd*,fwdrate=max,format=clean,interval=1rd=rd2,fwd=wd*,fwdrate=11826,format=restart,elapse=3600,maxdata=1800G,interval=5,warmup=30]]></content>
      <tags>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch(1)]]></title>
    <url>%2F2017%2F09%2F17%2FElasticSearch-1%2F</url>
    <content type="text"><![CDATA[ElasticSearch入门笔记来自：http://www.imooc.com/video/15762 ElasticSearch简介1、什么是ElasticSearch？ 基于Apache Lucene构建的开源搜索引擎 采用Java编写，提供简单易用的RESTFul API 轻松的横向扩展，可支持PB级的结构化或非结构化数据处理 2、应用场景 海量数据分析引擎 站内搜索引擎 数据仓库 3、一线公司实际应用场景 英国卫报 - 实时分析公众对文章的回应 维基百科、GitHub - 站内实时搜索 百度 - 实时日志监控平台 等等... 4、前置知识 熟悉用Maven构建项目 了解Spiring Boot的基本使用 5、环境要求 IDE工具 IntelliJ IDEA、Eclipse等常规用IDE即可 Java JDK1.8 其他依赖 Maven、NodeJs（6.0以上） 6、课程简介 安装 基础概念 基本用法 高级查询 实战演练 课程总结 安装1、版本问题 版本历史 1.x-&gt;2.x-&gt;5.x 版本选择 5.x 2、单实例安装（1）官网下载elasticsearch https://www.elastic.co/cn/ yujiang@ubuntu001:~$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.0.tar.gz （2）解压并修改配置文件 yujiang@ubuntu001:~$ tar zxvf elasticsearch-5.6.0.tar.gz yujiang@ubuntu001:~$ cd elasticsearch-5.6.0/ yujiang@ubuntu001:~/elasticsearch-5.6.0$ vi config/elasticsearch.yml 添加 network.host: 192.168.30.134 （3）启动elasticsearch yujiang@ubuntu001:~$ sudo sysctl -w vm.max_map_count=262144 或 yujiang@ubuntu001:~$ sudo vi /etc/sysctl.conf vm.max_map_count=262144 yujiang@ubuntu001:~/elasticsearch-5.6.0$ sh ./bin/elasticsearch （4）打开浏览器 http://192.168.30.134:9200/ 3、实用插件Head安装（1）安装nodejs yujiang@ubuntu001:~$ wget https://nodejs.org/dist/v6.11.3/node-v6.11.3-linux-x64.tar.xz yujiang@ubuntu001:~$ xz -d node-v6.11.3-linux-x64.tar.xz yujiang@ubuntu001:~$ tar -xvf node-v6.11.3-linux-x64.tar yujiang@ubuntu001:~$ sudo ln -s /home/yujiang/node-v6.11.3-linux-x64/bin/node /usr/bin/node yujiang@ubuntu001:~$ sudo ln -s /home/yujiang/node-v6.11.3-linux-x64/bin/npm /usr/bin/npm （2）修改ElasticSearch配置文件，支持跨域，并在后台运行ElasticSearch yujiang@ubuntu001:~/elasticsearch-5.6.0$ vi config/elasticsearch.yml 添加 http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; yujiang@ubuntu001:~/elasticsearch-5.6.0$ sh ./bin/elasticsearch -d （3）运行elasticsearch-head https://github.com/mobz/elasticsearch-head yujiang@ubuntu001:~$ wget https://github.com/mobz/elasticsearch-head/archive/master.zip yujiang@ubuntu001:~$ unzip master.zip yujiang@ubuntu001:~$ cd elasticsearch-head-master/ yujiang@ubuntu001:~/elasticsearch-head-master$ npm install yujiang@ubuntu001:~/elasticsearch-head-master$ npm run start （4）打开浏览器 http://192.168.30.134:9100/ web页面输入 http://192.168.30.134:9200/ 4、分布式安装 （1）master节点修改配置文件 yujiang@ubuntu001:~/elasticsearch-5.6.0$ vi config/elasticsearch.yml 添加 cluster.name: escluster node.name: master node.master: true （2）slave节点修改配置文件 cluster.name: escluster node.name: slave1 network.host: 192.168.30.135 #http.port: 8200 discovery.zen.ping.unicast.hosts: [&quot;192.168.30.134&quot;] yujiang@ubuntu002:~/elasticsearch-5.6.0$ sh ./bin/elasticsearch （3）安装java jdk yujiang@ubuntu002:~$ sudo add-apt-repository ppa:openjdk-r/ppa yujiang@ubuntu002:~$ sudo apt-get update yujiang@ubuntu002:~$ sudo apt-get install -f openjdk-8-jdk yujiang@ubuntu002:~$ java -version openjdk version &quot;1.8.0_131&quot; OpenJDK Runtime Environment (build 1.8.0_131-8u131-b11-2ubuntu1.16.04.3-b11) OpenJDK 64-Bit Server VM (build 25.131-b11, mixed mode) ElasticSearch基础概念集群和节点 master + slave1 + slave2 + slave...n 索引 含有相同属性的文档集合 类型 索引可以定义一个或多个类型，文档必须属于一个类型 文档 文档是可以被索引的基本数据单位 分片 每个索引都有多个分片，每个分片是一个Lucene索引 分片的好处，加入一个索引的数据量很大，就会造成硬盘压力很大，就会出现搜索瓶颈，可以将索引分成多个分片，从而分担压力，还允许用户进行水平的扩展和拆分，及分布式的操作，可以提高搜索和其他操作的效率 备份 拷贝一份分片就完成了分片的备份 当一个主分片失败或者出现问题时，备份分片可以代替工作，从而提高了es的可用性。备份的分片还可以执行搜索的操作，以分摊搜索的压力。es在创建索引时会创建5和分片和1个备份，数量可以修改，分片只有在创建索引的时候指定而不能在后期进行修改，备份是可以动态修改的 ElasticSearch的基本用法RESTFul API （1）API基本格式 http://&lt;ip&gt;:&lt;port&gt;/&lt;索引&gt;/&lt;类型&gt;/&lt;文档id&gt; （2）常用HTTP动词 GET/PUT/POST/DELETE]]></content>
      <tags>
        <tag>ES</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lambdas]]></title>
    <url>%2F2017%2F09%2F11%2FLambdas%2F</url>
    <content type="text"><![CDATA[C++11 introduced lambdas, allowing the definition of inline functionality, which can be used as a parameter or a local object.Lambdas change the way the C++ standard library is used. Syntax of LambdasA lambda is a definition of functionality that can be defined inside statements and expressions. Thus, you can use a lambda as an inline function.The minimal lambda function has no parameters and simply does something. For example:123[] &#123;std::cout &lt;&lt; "hello lambda" &lt;&lt; std::endl;&#125; You can call it directly: …mutable_opt throwSpec_opt-&gt;retType_opt{…}123[] &#123;std::cout &lt;&lt; "hello lambda" &lt;&lt; std::endl;&#125; (); // prints ‘‘hello lambda’’ or pass it to objects to get called:12345auto l = [] &#123;std::cout &lt;&lt; "hello lambda" &lt;&lt; std::endl;&#125;;...l(); // prints ‘‘hello lambda’’ lambda introducer，capture to access nonstatic outside objects inside the lambda.static objects such as std::cout can be used |…mutable_opt throwSpec_opt-&gt;retType_opt | all of them are optional,bug if one of them occurs,the parentheses for the parameters are mandatorymutable关系到[…]中的数据是否可以被改写（objects are passed by value,but inside the function object defined by the lambda, you have write access to the passed value.）,可以取用外部变量(…)函数的参数retType,without any specific definition of the return type, it is deduced from the return value.you can specify a capture to access data of outer scope that is not passed as an argument:[=]means that the outer scope is passed to the lambda by value.[&amp;]means that the outer scope is passed to the lambda by reference.1234Ex:int x=0;int y=42;auto qqq=[x,&amp;y]&#123;...&#125; //[=,&amp;y]to pass y by reference and all other objects by value.y是by reference并且允许使用所有objects by value 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061//The type of a lambda is an anouymous function object (or functor)int id = 0;//objects are passed by value, but inside the functionnn object defined by the lambda, you have write access to the passed value.auto f = [id]()mutable &#123; cout &lt;&lt; "id: " &lt;&lt; id &lt;&lt; endl; ++id; //如果没写mutable,难道不能++id;吗?&#125;;id = 42;f(); //0f(); //1f(); //2cout &lt;&lt; id &lt;&lt; endl; //42//等同于class Functor &#123;private: int id; //copy of outside idpublic: void operator()()&#123; cout &lt;&lt; "id: " &lt;&lt; id &lt;&lt; endl; ++id; //OK &#125;&#125;;Functor f;//[&amp;id]int id = 0;auto f = [&amp;id](int param) &#123; cout &lt;&lt; "id: " &lt;&lt; id &lt;&lt; endl; cout &lt;&lt; "param: " &lt;&lt; param &lt;&lt; endl; ++id; ++param;&#125;;id = 42;f(7); //id:42,param:7f(7); //id:43,param:7f(7); //id:44,param:7cout &lt;&lt; id &lt;&lt; endl; //id:45//无mutable,[Error] increment of read-only variable 'id'int id = 0;auto f = [id]() &#123; cout &lt;&lt; "id: " &lt;&lt; id &lt;&lt; endl; ++id;&#125;;id = 42;f();f();f();cout &lt;&lt; id &lt;&lt; endl;//int id = 0;auto f = [id]()mutable &#123; cout &lt;&lt; "id: " &lt;&lt; id &lt;&lt; endl; ++id; static int x = 5; //可以声明变量 int y = 6; return id; //可以返回数值&#125;;f(); Here is what compiler generates for lambda’s:123456789101112131415161、int tobefound = 5;auto lambda1 = [tobefound](int val)&#123;return val == tobefound;&#125;;2、class UnNamedLocalFunction&#123;private: int localVar;public: UnNamedLocalFunction(int var):localVar(var)&#123;&#125; bool operator()(int val)&#123; return val == localVar; &#125;&#125;;UnNamedLocalFunction lambda2(tobefound);bool b1 = lambda1(5);bool b2 = lambda2(5);]]></content>
      <tags>
        <tag>Cpp2.0</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CppImprovement]]></title>
    <url>%2F2017%2F09%2F07%2FCppImprovement%2F</url>
    <content type="text"><![CDATA[Spaces in Template ExpressionsThe requirement to put a space between two closing template expressions has gone 12vector&lt;list&lt;int&gt; &gt;; // OK in each C++ versionvector&lt;list&lt;int&gt;&gt;; // OK since C++11 nullptr and std::nullptr_tC++11 lets you use nullptr instead of 0 or NULL to specify that a pointer refers to no value (which differs from having an undefined value). This new feature especially helps to avoid mistakes that occurred when a null pointer was interpreted as an integral value. For example:12345void f(int);void f(void*);f(0); // calls f(int)f(NULL); // calls f(int) if NULL is 0, ambiguous otherwisef(nullptr); // calls f(void*) nullptr is a new keyword. It automatically converts into each pointer type but not to any integral type. It has type std::nullptr_t, defined in (see Section 5.8.1, page 161), so you can now even overload operations for the case that a null pointer is passed. Note that std::nullptr_t counts as a fundamental data type (see Section 5.4.2, page 127). Automatic Type Deduction with autoWith C++11, you can declare a variable or an object without specifying its specific type by using auto.For example:123auto i = 42; // i has type intdouble f();auto d = f(); // d has type double The type of a variable declared with auto is deduced from its initializer. Thus, an initialization is required: auto i; // ERROR: can’t dedulce the type of i Additional qualifiers are allowed.For example:1static auto vat = 0.19; Using auto is especially useful where the type is a pretty long and/or complicated expression.For example:123456vector&lt;string&gt; v;...auto pos = v.begin(); // pos has type vector&lt;string&gt;::iteratorauto l = [] (int x) -&gt; bool &#123; // l has the type of a lambda..., // taking an int and returning a bool&#125;; The latter is an object, representing a lambda Uniform Initialization and Initializer ListsBefore C++11, programmers, especially novices, could easily become confused by the question of how to initialize a variable or an object. Initialization could happen with parentheses, braces, and/or assignment operators.For this reason, C++11 introduced the concept of uniform initialization, which means that for any initialization, you can use one common syntax. This syntax uses braces, so the following is possible now:123456int values[] &#123; 1, 2, 3 &#125;;std::vector&lt;int&gt; v &#123; 2, 3, 5, 7, 11, 13, 17 &#125;;std::vector&lt;std::string&gt; cities &#123;"Berlin", "New York", "London", "Braunschweig", "Cairo", "Cologne"&#125;;std::complex&lt;double&gt; c&#123;4.0,3.0&#125;; // equivalent to c(4.0,3.0) 其实是利用一个事实：编译器看到{t1,t2…tn}便做出一个关联至一个initializer_list,它关联至一个array&lt;T,n&gt;。调用函数（例如ctor）时该array内的元素可被编译器分解逐一传给函数。但若函数参数是个initializer_list，调用者却不能给予数个T参数然后以为它们会被自动转为一个initializer_list传入） std::vectorstd::string cities {“Berlin”, “New York”, “London”, “Braunschweig”, “Cairo”, “Cologne”};，这形成一个initializer_list,背后有个array&lt;string,6&gt;。调用vectorctors时编译器找到一个vectorctor接受initializer_list。所有容器皆有类似于此ctor。 std::complex c{4.0,3.0}; // equivalent to c(4.0,3.0)这形成一个initializer_list，背后有个array&lt;double,2&gt;。调用complexctor时该array内的2个元素被分解传给ctor。complex并无任何ctor接受initializer_list An initializer list forces so-called value initialization, which means that even local variables of fundamental data types, which usually have an undefined initial value, are initialized by zero (or nullptr, if it is a pointer):1234int i; // i has undefined valueint j&#123;&#125;; // j is initialized by 0int* p; // p has undefined valueint* q&#123;&#125;; // q is initialized by nullptr Note, however, that narrowing initializations — those that reduce precision or where the supplied value gets modified— are not possible with braces. For example:12345678int x1(5.3); // OK, but OUCH: x1 becomes 5int x2 = 5.3; // OK, but OUCH: x2 becomes 5int x3&#123;5.0&#125;; // ERROR: narrowingint x4 = &#123;5.3&#125;; // ERROR: narrowingchar c1&#123;7&#125;; // OK: even though 7 is an int, this is not narrowingchar c2&#123;99999&#125;; // ERROR: narrowing (if 99999 doesn’t fit into a char)std::vector&lt;int&gt; v1 &#123; 1, 2, 4, 5 &#125;; // OKstd::vector&lt;int&gt; v2 &#123; 1, 2.3, 4, 5.6 &#125;; // ERROR: narrowing doubles to ints To support the concept of initializer lists for user-defined types, C++11 provides the class template std::initializer_list&lt;&gt;. It can be used to support initializations by a list of values or in any other place where you want to process just a list of values. For example:123456789void print (std::initializer_list&lt;int&gt; vals)&#123; for (auto p=vals.begin(); p!=vals.end(); ++p) &#123; // process a list of values std::cout &lt;&lt; *p &lt;&lt; "\n"; &#125;&#125;print (&#123;12,3,5,7,11,13,17&#125;); // pass a list of values to print()传给initializer_list者，一定必须也是个initializer_list(or&#123;&#125;形式) cpp1.0(none explicit one argument ctor 才可以做隐式转换)explicit for ctors taking one argumentcpp2.0explicit for ctors taking more than one argument range-based for statement1234567891011121314151617for ( decl : coll ) &#123; statement&#125;for ( int i : &#123; 2,3,4,5,6&#125; ) &#123; cout &lt;&lt; i &lt;&lt; endl;&#125;vector&lt;double&gt; vec;...for ( auto elem : vec ) &#123; cout &lt;&lt; elem &lt;&lt; endl;&#125;for ( auto&amp; elem : vec ) &#123; elem *= 3;&#125; Note that no explicit type conversions are possible when elements are initialized as decl inside the for loop. Thus, the following does not compile:12345678910class C&#123;public:explicit C(const std::string&amp; s); // explicit(!) type conversion from strings...&#125;;std::vector&lt;std::string&gt; vs;for (const C&amp; elem : vs) &#123; // ERROR, no conversion from string to C definedstd::cout &lt;&lt; elem &lt;&lt; std::endl;&#125; =default,=delete如果你自行定义了一个ctor，那么编译器就不会再给你一个default ctor。如果你强制加上=default，就可以重新获得并使用default ctor。 Alias Template ( template typedef )12345template&lt;typename T&gt;using Vec = std::vector&lt;T,MyAlloc&lt;T&gt;&gt;; //standard vector using own allocatorVec&lt;int&gt; coll;is equivalent to std::vector&lt;int,MyAlloc&lt;int&gt;&gt; coll;It is not possible to partially or explicitly specialize an alias template. Type Alias (similar to typedef)123456//type alias,identical to//typedef void (*func)(int,int);using func = void(*)(int,int);//the name 'func' now denotes a pointer to function: void example(int,int)&#123;&#125;func fn = example; noexcept1void foo() noexcept; ==&gt; void foo() noexcept(true); declares that foo() won’t throw. If an exception is not handled locally inside foo() — thus, if foo() throws — the program is terminated, calling std::terminate(), which by default calls std::abort(). You can even specify a condition under which a function throws no exception. For example, for any type Type, the global swap() usually is defined as follows:1234void swap (Type&amp; x, Type&amp; y) noexcept(noexcept(x.swap(y)))&#123;x.swap(y);&#125; Here, inside noexcept(…), you can specify a Boolean condition under which no exception gets thrown: Specifying noexcept without condition is a short form of specifying noexcept(true). https://stackoverflow.com/questions/8001823/how-to-enforce-move-semantics-when-a-vector-grows override告诉编译器，子类要override父类的成员函数，让编译器帮助检查12345678struce Base&#123; virtual void vfunc(float)&#123;&#125;&#125;;struct Derived2 : Base &#123; virtual void vfunc(int) override&#123;&#125; //[Error] marked override,bug does not override virtual void vfunc(float) override&#123;&#125;&#125;; final123456789struct Base1 final&#123;&#125;;struct Derived1 : Base1&#123;&#125;;//[Error] cannot derive from 'final' base 'Base1' in derived type 'Derived1'struct Base2&#123; virtual void f() final;&#125;;struct Derived2 : Base2&#123; void f();//[Error] overriding final function 'virtual void Base2::f()'&#125;; decltypeBy using the new decltype keyword, you can let the compiler find out the type of an expression. This is the realization of the often requested typeof feature. However, the existing typeof implementations were inconsistent and incomplete, so C++11 introduced a new keyword. For example:GNU C++中的typeof并不是标准库的一部分12345map&lt;string, float&gt; coll;decltype(coll)::value_type elem;这样写（before C++11）map&lt;string, float&gt;::value_type elem; One application of decltype is to declare return types (see below). Another is to use it in metaprogramming (see Section 5.4.1, page 125) or to pass the type of a lambda (see Section 10.3.4, page 504). defines a type equivalent to the type of an expression 应用：1、decltype,used to declare return typesSometimes, the return type of a function depends on an expression processed with the arguments.However, something like12template &lt;typename T1, typename T2&gt;decltype(x+y) add(T1 x, T2 y); was not possible before C++11, because the return expression uses objects not introduced or in scope yet.But with C++11, you can alternatively declare the return type of a function behind the parameter list:12template &lt;typename T1, typename T2&gt;auto add(T1 x, T2 y) -&gt; decltype(x+y); This uses the same syntax as for lambdas to declare return types[…] (…) mutable_opt throwSpec_opt -&gt;retType_opt {…}2、decltype, to used in metaprogramming3、decltype, pass the type of a lambda面对lambda，我们手上往往只有object，没有type。要获得其type就得借助于decltype。1234567auto cmp = [](const Persion&amp; p1, const Person&amp; p2) &#123; return p1.lastname() &lt; p2.lastname() || (p1.lastname() == p2.lastname() &amp;&amp; p1.firstname() &lt; p2.firstname()); &#125;set&lt;Person, decltype(cmp)&gt;coll(cmp); 笔记来自 http://boolan.com/ 侯捷C++新标准C++11/14内容引自《The C++ standard Library A Tutorial and reference》]]></content>
      <tags>
        <tag>Cpp2.0</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CppVariadicTemplates]]></title>
    <url>%2F2017%2F09%2F07%2FCppVariadicTemplates%2F</url>
    <content type="text"><![CDATA[数量不定的模板参数 123456首先确定编译器是否支持C++11#include &lt;iostream&gt;int main(int argc, char** argv) &#123; std::cout &lt;&lt; __cplusplus &lt;&lt; std::endl; return 0;&#125; VariadicTemplates特性最大的特点是可以用在函数递归（函数自己调用自己才叫recursive递归），tuple的做法]]></content>
      <tags>
        <tag>Cpp2.0</tag>
      </tags>
  </entry>
</search>
